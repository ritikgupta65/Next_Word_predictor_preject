{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"\"\"A miracle is taking place as you read these lines: the squiggles on this page are forming into words and concepts and emotions as they navigate their way through your cortex. My thoughts from November 2021 have now successfully invaded your\n",
    " brain. If they manage to catch your attention and survive long enough in this harsh\n",
    " and highly competitive environment, they may have a chance to reproduce again as \n",
    " you share these thoughts with others. Thanks to language, thoughts have become air\n",
    "borne and highly contagious brain germs—and no vaccine is coming.\n",
    " Luckily, most brain germs are harmless,1 and a few are wonderfully useful. In fact,\n",
    " humanity’s brain germs constitute two of our most precious treasures: knowledge and\n",
    " culture. Much as we can’t digest properly without healthy gut bacteria, we cannot\n",
    " think properly without healthy brain germs. Most of your thoughts are not actually\n",
    " yours: they arose and grew and evolved in many other brains before they infected\n",
    " you. So if we want to build intelligent machines, we will need to find a way to infect\n",
    " them too.\n",
    " The good news is that another miracle has been unfolding over the last few years: sev\n",
    "eral breakthroughs in deep learning have given birth to powerful language models.\n",
    " Since you are reading this book, you have probably seen some astonishing demos of\n",
    " these language models, such as GPT-3, which given a short prompt such as “a frog\n",
    " meets a crocodile” can write a whole story. Although it’s not quite Shakespeare yet, it’s\n",
    " sometimes hard to believe that these texts were written by an artificial neural net\n",
    "work. In fact, GitHub’s Copilot system is helping me write these lines: you’ll never\n",
    " know how much I really wrote.\n",
    " The revolution goes far beyond text generation. It encompasses the whole realm of\n",
    " natural language processing (NLP), from text classification to summarization, trans\n",
    "lation, question answering, chatbots, natural language understanding .more. Wherever there’s language, speech or text, there’s an application for NLP. You\n",
    " can already ask your phone for tomorrow’s weather, or chat with a virtual help desk\n",
    " assistant to troubleshoot a problem, or get meaningful results from search engines\n",
    " that seem to truly understand your query. But the technology is so new that the best\n",
    " is probably yet to come.\n",
    " Like most advances in science, this recent revolution in NLP rests upon the hard\n",
    " work of hundreds of unsung heroes. But three key ingredients of its success do stand\n",
    " out:\n",
    " • The transformer is a neural network architecture proposed in 2017 in a ground\n",
    "breaking paper called “Attention Is All You Need”, published by a team of Google\n",
    " researchers. In just a few years it swept across the field, crushing previous archi\n",
    "tectures that were typically based on recurrent neural networks (RNNs). The\n",
    " Transformer architecture is excellent at capturing patterns in long sequences of\n",
    " data and dealing with huge datasets—so much so that its use is now extending\n",
    " well beyond NLP, for example to image processing tasks.\n",
    " • In most projects, you won’t have access to a huge dataset to train a model from\n",
    " scratch. Luckily, it’s often possible to download a model that was pretrained on a\n",
    " generic dataset: all you need to do then is fine-tune it on your own (much\n",
    " smaller) dataset. Pretraining has been mainstream in image processing since the\n",
    " early 2010s, but in NLP it was restricted to contextless word embeddings (i.e.,\n",
    " dense vector representations of individual words). For example, the word “bear”\n",
    " had the same pretrained embedding in “teddy bear” and in “to bear.” Then, in\n",
    " 2018, several papers proposed full-blown language models that could be pre\n",
    "trained and fine-tuned for a variety of NLP tasks; this completely changed the\n",
    " game.\n",
    " • Model hubs like Hugging Face’s have also been a game-changer. In the early days,\n",
    " pretrained models were just posted anywhere, so it wasn’t easy to find what you\n",
    " needed. Murphy’s law guaranteed that PyTorch users would only find Tensor\n",
    "Flow models, and vice versa. And when you did find a model, figuring out how\n",
    " to fine-tune it wasn’t always easy. This is where Hugging Face’s Transformers\n",
    " library comes in: it’s open source, it supports both TensorFlow and PyTorch, and\n",
    " it makes it easy to download a state-of-the-art pretrained model from the Hug\n",
    "ging Face Hub, configure it for your task, fine-tune it on your dataset, and evalu\n",
    "ate it. Use of the library is growing quickly: in Q4 2021 it was used by over five\n",
    " thousand organizations and was installed using pip over four million times per\n",
    " month. Moreover, the library and its ecosystem are expanding beyond NLP:\n",
    " image processing models are available too. You can also download numerous\n",
    " datasets from the Hub to train or evaluate your models.\n",
    " So what more can you ask for? Well, this book! It was written by open source devel\n",
    "opers at Hugging Face—including the creator of the Transformers library!—and it\n",
    " \n",
    "Foreword\n",
    "shows: the breadth and depth of the information you will find in these pages is\n",
    " astounding. It covers everything from the Transformer architecture itself, to the\n",
    " Transformers library and the entire ecosystem around it. I particularly appreciated\n",
    " the hands-on approach: you can follow along in Jupyter notebooks, and all the code\n",
    " examples are straight to the point and simple to understand. The authors have exten\n",
    "sive experience in training very large transformer models, and they provide a wealth\n",
    " of tips and tricks for getting everything to work efficiently. Last but not least, their\n",
    " writing style is direct and lively: it reads like a novel.\n",
    " In short, I thoroughly enjoyed this book, and I’m certain you will too. Anyone inter\n",
    "ested in building products with state-of-the-art language-processing features needs to\n",
    " read it. It’s packed to the brim with all the right brain germs!. Hopefully, by now you are\n",
    "   excited to learn how to start training and integrating these\n",
    " versatile models into your own applications! You’ve seen in this chapter that with just\n",
    " a few lines of code you can use state-of-the-art models for classification, named entity\n",
    " recognition, question answering, translation, and summarization, but this is really\n",
    " just the “tip of the iceberg.”\n",
    " In the following chapters you will learn how to adapt transformers to a wide range of\n",
    " use cases, such as building a text classifier, or a lightweight model for production, or\n",
    " even training a language model from scratch. We’ll be taking a hands-on approach,\n",
    " which means that for every concept covered there will be accompanying code that\n",
    " you can run on Google Colab or your own GPU machine.\n",
    " Now that we’re armed with the basic concepts behind transformers, it’s time to get\n",
    " our hands dirty with our first application: text classification. That’s the topic of the\n",
    " next chapter!. The basic idea behind subword tokenization is to combine the best aspects of charac\n",
    "ter and word tokenization. On the one hand, we want to split rare words into smaller\n",
    " units to allow the model to deal with complex words and misspellings. On the other\n",
    " hand, we want to keep frequent words as unique entities so that we can keep the\n",
    " length of our inputs to a manageable size. The main distinguishing feature of\n",
    " subword tokenization (as well as word tokenization) is that it is learned from the pre\n",
    "training corpus using a mix of statistical rules and algorithms.\n",
    " There are several subword tokenization algorithms that are commonly used in NLP,\n",
    " but let’s start with WordPiece,5 which is used by the BERT and DistilBERT tokenizers.\n",
    " The easiest way to understand how WordPiece works is to see it in action.  Trans\n",
    "formers provides a convenient AutoTokenizer class that allows you to quickly load\n",
    " the tokenizer associated with a pretrained model—we just call its from_pretrained()\n",
    " method, providing the ID of a model on the Hub or a local file path. Let’s start by\n",
    " loading the tokenizer for DistilBERT. We can observe three things here. First, some special [CLS] and [SEP] tokens have\n",
    " been added to the start and end of the sequence. These tokens differ from model to\n",
    " model, but their main role is to indicate the start and end of a sequence. Second, the\n",
    " tokens have each been lowercased, which is a feature of this particular checkpoint.\n",
    " Finally, we can see that “tokenizing” and “NLP” have been split into two tokens,\n",
    " which makes sense since they are not common words. The ## prefix in ##izing and\n",
    " ##p means that the preceding string is not whitespace; any token with this prefix\n",
    " should be merged with the previous token when you convert the tokens back to a\n",
    " string. The AutoTokenizer class has a convert_tokens_to_string() method for\n",
    " doing just that, so let’s apply it to our tokens:. From this plot we can see some clear patterns: the negative feelings such as sadness,\n",
    " anger, and fear all occupy similar regions with slightly varying distributions. On the\n",
    " other hand, joy and love are well separated from the negative emotions and also\n",
    " share a similar space. Finally, surprise is scattered all over the place. Although we\n",
    " may have hoped for some separation, this is in no way guaranteed since the model\n",
    " was not trained to know the difference between these emotions. It only learned them\n",
    " implicitly by guessing the masked words in texts.\n",
    " Now that we’ve gained some insight into the features of our dataset, let’s finally train a\n",
    " model on it!. Training the hidden states that serve as inputs to the classification model will help us\n",
    " avoid the problem of working with data that may not be well suited for the classifica\n",
    "tion task. Instead, the initial hidden states adapt during training to decrease the\n",
    " model loss and thus increase its performance. In Chapter 2, we saw what it takes to fine-tune and evaluate a transformer. Now let’s\n",
    " take a look at how they work under the hood. In this chapter we’ll explore the main\n",
    " building blocks of transformer models and how to implement them using PyTorch.\n",
    " We’ll also provide guidance on how to do the same in TensorFlow. We’ll first focus on\n",
    " building the attention mechanism, and then add the bits and pieces necessary to\n",
    " make a transformer encoder work. We’ll also have a brief look at the architectural dif\n",
    "ferences between the encoder and decoder modules. By the end of this chapter you\n",
    " will be able to implement a simple transformer model yourself!\n",
    " While a deep technical understanding of the Transformer architecture is generally\n",
    " not necessary to use  Transformers and fine-tune models for your use case, it can\n",
    " be helpful for comprehending and navigating the limitations of transformers and\n",
    " using them in new domains.\n",
    " This chapter also introduces a taxonomy of transformers to help you understand the\n",
    " zoo of models that have emerged in recent years. Before diving into the code, let’s\n",
    " start with an overview of the original architecture that kick-started the transformer\n",
    " revolution.gets these two as an input as well as all the encoder’s outputs to predict the next\n",
    " token, “fliegt”. In the next step the decoder gets “fliegt” as an additional input. We\n",
    " repeat the process until the decoder predicts the EOS token or we reached a max\n",
    "imum length.\n",
    " The Transformer architecture was originally designed for sequence-to-sequence tasks\n",
    " like machine translation, but both the encoder and decoder blocks were soon adapted\n",
    " as standalone models. Although there are hundreds of different transformer models,\n",
    " most of them belong to one of three types:\n",
    " Encoder-only\n",
    " These models convert an input sequence of text into a rich numerical representa\n",
    "tion that is well suited for tasks like text classification or named entity recogni\n",
    "tion. BERT and its variants, like RoBERTa and DistilBERT, belong to this class of\n",
    " architectures. The representation computed for a given token in this architecture\n",
    " depends both on the left (before the token) and the right (after the token) con\n",
    "texts. This is often called bidirectional attention.\n",
    " Decoder-only\n",
    " Given a prompt of text like “Thanks for lunch, I had a…” these models will auto\n",
    "complete the sequence by iteratively predicting the most probable next word.\n",
    " The family of GPT models belong to this class. The representation computed for\n",
    " a given token in this architecture depends only on the left context. This is often\n",
    " called causal or autoregressive attention.\n",
    " Encoder-decoder\n",
    " These are used for modeling complex mappings from one sequence of text to\n",
    " another; they’re suitable for machine translation and summarization tasks. In\n",
    " addition to the Transformer architecture, which as we’ve seen combines an\n",
    " encoder and a decoder, the BART and T5 models belong to this class.\n",
    " In reality, the distinction between applications for decoder-only\n",
    " versus encoder-only architectures is a bit blurry. For example,\n",
    " decoder-only models like those in the GPT family can be primed\n",
    " for tasks like translation that are conventionally thought of as\n",
    " sequence-to-sequence tasks. Similarly, encoder-only models like\n",
    " BERT can be applied to summarization tasks that are usually asso\n",
    "ciated with encoder-decoder or decoder-only models.1\n",
    " Now that you have a high-level understanding of the Transformer architecture, let’s.\n",
    "   As we discussed in Chapter 1, attention is a mechanism that allows neural networks\n",
    " to assign a different amount of weight or “attention” to each element in a sequence.\n",
    " For text sequences, the elements are token embeddings like the ones we encountered\n",
    " in Chapter 2, where each token is mapped to a vector of some fixed dimension. For\n",
    " example, in BERT each token is represented as a 768-dimensional vector. The “self”\n",
    " part of self-attention refers to the fact that these weights are computed for all hidden\n",
    " states in the same set—for example, all the hidden states of the encoder. By contrast,\n",
    " the attention mechanism associated with recurrent models involves computing the\n",
    " relevance of each encoder hidden state to the decoder hidden state at a given decod\n",
    "ing timestep.\n",
    " The main idea behind self-attention is that instead of using a fixed embedding for\n",
    " each token, we can use the whole sequence to compute a weighted average of each\n",
    " embedding. . Project each token embedding into three vectors called query, key, and value.\n",
    " 2. Compute attention scores. We determine how much the query and key vectors\n",
    " relate to each other using a similarity function. As the name suggests, the similar\n",
    "ity function for scaled dot-product attention is the dot product, computed effi\n",
    "ciently using matrix multiplication of the embeddings. Queries and keys that are\n",
    " similar will have a large dot product, while those that don’t share much in com\n",
    "mon will have little to no overlap. The notion of query, key, and value vectors may seem a bit cryptic the first time you\n",
    " encounter them. Their names were inspired by information retrieval systems, but we\n",
    " can motivate their meaning with a simple analogy. Imagine that you’re at the super\n",
    "market buying all the ingredients you need for your dinner. You have the dish’s recipe,\n",
    " and each of the required ingredients can be thought of as a query. As you scan the\n",
    " shelves, you look at the labels (keys) and check whether they match an ingredient on\n",
    " your list (similarity function). If you have a match, then you take the item (value)\n",
    " from the shelf.\n",
    " In this analogy, you only get one grocery item for every label that matches the ingre\n",
    "dient. Self-attention is a more abstract and “smooth” version of this: every label in the\n",
    " supermarket matches the ingredient to the extent to which each key matches the\n",
    " query. So if your list includes a dozen eggs, then you might end up grabbing 10 eggs,\n",
    " an omelette, and a chicken wing. This has created a \n",
    "5 ×5\n",
    " matrix of attention scores per sample in the batch. We’ll see\n",
    " later that the query, key, and value vectors are generated by applying independent\n",
    " weight matrices WQ,K,V to the embeddings, but for now we’ve kept them equal for\n",
    " simplicity. In scaled dot-product attention, the dot products are scaled by the size of\n",
    " the embedding vectors so that we don’t get too many large numbers during training\n",
    " that can cause the softmax we will apply next to saturate.\n",
    " The torch.bmm() function performs a batch matrix-matrix product\n",
    " that simplifies the computation of the attention scores where the\n",
    " query and key vectors have the shape [batch_size, seq_len,\n",
    " hidden_dim]. If we ignored the batch dimension we could calculate\n",
    " the dot product between each query and key vector by simply\n",
    " transposing the key tensor to have the shape [hidden_dim,\n",
    " seq_len] and then using the matrix product to collect all the dot\n",
    " products in a [seq_len, seq_len] matrix. Since we want to do\n",
    " this for all sequences in the batch independently, we use\n",
    " torch.bmm(), which takes two batches of matrices and multiplies\n",
    " each matrix from the first batch with the corresponding matrix in\n",
    " the second batch.Our attention mechanism with equal query and key vectors will assign a very large\n",
    " score to identical words in the context, and in particular to the current word itself: the\n",
    " dot product of a query with itself is always 1. But in practice, the meaning of a word\n",
    " will be better informed by complementary words in the context than by identical\n",
    " words—for example, the meaning of “flies” is better defined by incorporating infor\n",
    "mation from “time” and “arrow” than by another mention of “flies”. How can we pro\n",
    "mote this behavior?\n",
    " Let’s allow the model to create a different set of vectors for the query, key, and value of\n",
    " a token by using three different linear projections to project our initial token vector\n",
    " into three different spaces.\n",
    " Multi-headed attention\n",
    " In our simple example, we only used the embeddings “as is” to compute the attention\n",
    " scores and weights, but that’s far from the whole story. In practice, the self-attention\n",
    " layer applies three independent linear transformations to each embedding to generate\n",
    " the query, key, and value vectors. These transformations project the embeddings and\n",
    " each projection carries its own set of learnable parameters, which allows the self\n",
    "attention layer to focus on different semantic aspects of the sequence.\n",
    " It also turns out to be beneficial to have multiple sets of linear projections, each one\n",
    " representing a so-called attention head. The resulting multi-head attention layer is\n",
    " illustrated in Figure 3-5. But why do we need more than one attention head? The rea\n",
    "son is that the softmax of one head tends to focus on mostly one aspect of similarity.\n",
    " Having several heads allows the model to focus on several aspects at once. For\n",
    " instance, one head can focus on subject-verb interaction, whereas another finds\n",
    " nearby adjectives. Obviously we don’t handcraft these relations into the model, and\n",
    " they are fully learned from the data. If you are familiar with computer vision models\n",
    " you might see the resemblance to filters in convolutional neural networks, where one\n",
    " filter can be responsible for detecting faces and another one finds wheels of cars in\n",
    " images.This visualization shows the attention weights as lines connecting the token whose\n",
    " embedding is getting updated (left) with every word that is being attended to (right).\n",
    " The intensity of the lines indicates the strength of the attention weights, with dark\n",
    " lines representing values close to 1, and faint lines representing values close to 0.\n",
    " In this example, the input consists of two sentences and the [CLS] and [SEP] tokens\n",
    " are the special tokens in BERT’s tokenizer that we encountered in Chapter 2. One\n",
    " thing we can see from the visualization is that the attention weights are strongest\n",
    " between words that belong to the same sentence, which suggests BERT can tell that it\n",
    " should attend to words in the same sentence. However, for the word “flies” we can see\n",
    " that BERT has identified “arrow” as important in the first sentence and “fruit” and\n",
    " “banana” in the second. These attention weights allow the model to distinguish the\n",
    " use of “flies” as a verb or noun, depending on the context in which it occurs!\n",
    " Now that we’ve covered attention, let’s take a look at implementing the missing piece\n",
    " of the encoder layer: position-wise feed-forward networks. The feed-forward sublayer in\n",
    "   the encoder and decoder is just a simple two-layer fully\n",
    " connected neural network, but with a twist: instead of processing the whole sequence\n",
    " of embeddings as a single vector, it processes each embedding independently. For this\n",
    " reason, this layer is often referred to as a position-wise feed-forward layer. You may\n",
    " also see it referred to as a one-dimensional convolution with a kernel size of one, typ\n",
    "ically by people with a computer vision background (e.g., the OpenAI GPT codebase\n",
    " uses this nomenclature). A rule of thumb from the literature is for the hidden size of\n",
    " the first layer to be four times the size of the embeddings, and a GELU activation\n",
    " function is most commonly used. This is where most of the capacity and memoriza\n",
    "tion is hypothesized to happen, and it’s the part that is most often scaled when scaling\n",
    " up the models. We can implement this as a simple nn.Module as follows. As mentioned earlier, \n",
    " the Transformer architecture makes use of layer normalization\n",
    " and skip connections. The former normalizes each input in the batch to have zero\n",
    " mean and unity variance. Skip connections pass a tensor to the next layer of the\n",
    " model without processing and add it to the processed tensor. When it comes to plac\n",
    "ing the layer normalization in the encoder or decoder layers of a transformer, there\n",
    " are two main choices adopted in the literature:\n",
    " Post layer normalization\n",
    " This is the arrangement used in the Transformer paper; it places layer normaliza\n",
    "tion in between the skip connections. This arrangement is tricky to train from\n",
    " scratch as the gradients can diverge. For this reason, you will often see a concept\n",
    " known as learning rate warm-up, where the learning rate is gradually increased\n",
    " from a small value to some maximum value during training.\n",
    " Pre layer normalization\n",
    " This is the most common arrangement found in the literature; it places layer nor\n",
    "malization within the span of the skip connections. This tends to be much more\n",
    " stable during training, and it does not usually require any learning rate warm-up. \n",
    " Positional embeddings are based on a simple, yet very effective idea: augment the\n",
    " token embeddings with a position-dependent pattern of values arranged in a vector.\n",
    " If the pattern is characteristic for each position, the attention heads and feed-forward\n",
    " layers in each stack can learn to incorporate positional information into their trans\n",
    "formations.\n",
    " There are several ways to achieve this, and one of the most popular approaches is to\n",
    " use a learnable pattern, especially when the pretraining dataset is sufficiently large.\n",
    " This works exactly the same way as the token embeddings, but using the position\n",
    " index instead of the token ID as input. With that approach, an efficient way of encod\n",
    "ing the positions of tokens is learned during pretraining.\n",
    " Let’s create a custom Embeddings module that combines a token embedding layer that\n",
    " projects the input_ids to a dense hidden state together with the positional embed\n",
    "ding that does the same for position_ids. The resulting embedding is simply the\n",
    " sum of both embeddings: We can see that we get a hidden state for each token in the batch. This output format\n",
    " makes the architecture very flexible, and we can easily adapt it for various applica\n",
    "tions such as predicting missing tokens in masked language modeling or predicting\n",
    " the start and end position of an answer in question answering. In the following sec\n",
    "tion we’ll see how we can build a classifier like the one we used in Chapter 2.\n",
    " Transformer models are usually divided into a task-independent body and a task\n",
    "specific head. We’ll encounter this pattern again in Chapter 4 when we look at the\n",
    " design pattern of  Transformers. What we have built so far is the body, so if we wish\n",
    " to build a text classifier, we will need to attach a classification head to that body. We\n",
    " have a hidden state for each token, but we only need to make one prediction. There\n",
    " are several options to approach this. Traditionally, the first token in such models is\n",
    " used for the prediction and we can attach a dropout and a linear layer to make a clas\n",
    "sification prediction. The following class extends the existing encoder for sequence\n",
    " classification. Let’s see if we can shed some light on the mysteries of encoder-decoder attention.\n",
    " Imagine you (the decoder) are in class taking an exam. Your task is to predict the next\n",
    " word based on the previous words (decoder inputs), which sounds simple but is\n",
    " incredibly hard (try it yourself and predict the next words in a passage of this book).\n",
    " Fortunately, your neighbor (the encoder) has the full text. Unfortunately, they’re a\n",
    " foreign exchange student and the text is in their mother tongue. Cunning students\n",
    " that you are, you figure out a way to cheat anyway. You draw a little cartoon illustrat\n",
    "ing the text you already have (the query) and give it to your neighbor. They try to\n",
    " figure out which passage matches that description (the key), draw a cartoon describ\n",
    "ing the word following that passage (the value), and pass that back to you. With this\n",
    " system in place, you ace the exam.\n",
    " Meet the Transformers\n",
    " As you’ve seen in this chapter, there are three main architectures for transformer\n",
    " models: encoders, decoders, and encoder-decoders. The initial success of the early\n",
    " transformer models triggered a Cambrian explosion in model development as\n",
    " researchers built models on various datasets of different size and nature, used new\n",
    " pretraining objectives, and tweaked the architecture to further improve performance.\n",
    " Although the zoo of models is still growing fast, they can still be divided into these\n",
    " three categories.dataset. Since the dataset only contains data without parallel texts (i.e., transla\n",
    "tions), the TLM objective of XLM was dropped. This approach beats XLM and\n",
    " multilingual BERT variants by a large margin, especially on low-resource\n",
    " languages.\n",
    " ALBERT\n",
    " The ALBERT model introduced three changes to make the encoder architecture\n",
    " more efficient.13 First, it decouples the token embedding dimension from the hid\n",
    "den dimension, thus allowing the embedding dimension to be small and thereby\n",
    " saving parameters, especially when the vocabulary gets large. Second, all layers\n",
    " share the same parameters, which decreases the number of effective parameters\n",
    " even further. Finally, the NSP objective is replaced with a sentence-ordering pre\n",
    "diction: the model needs to predict whether or not the order of two consecutive\n",
    " sentences was swapped rather than predicting if they belong together at all. These\n",
    " changes make it possible to train even larger models with fewer parameters and\n",
    " reach superior performance on NLU tasks.\n",
    " ELECTRA\n",
    " One limitation of the standard MLM pretraining objective is that at each training\n",
    " step only the representations of the masked tokens are updated, while the other\n",
    " input tokens are not. To address this issue, ELECTRA uses a two-model\n",
    " approach:14 the first model (which is typically small) works like a standard\n",
    " masked language model and predicts masked tokens. The second model, called\n",
    " the discriminator, is then tasked to predict which of the tokens in the first model’s\n",
    " output were originally masked. Therefore, the discriminator needs to make a\n",
    " binary classification for every token, which makes training 30 times more effi\n",
    "cient. For downstream tasks the discriminator is fine-tuned like a standard BERT\n",
    " model.\n",
    " DeBERTa\n",
    " The DeBERTa model introduces two architectural changes.15 First, each token is\n",
    " represented as two vectors: one for the content, the other for relative position. By\n",
    " disentangling the tokens’ content from their relative positions, the self-attention\n",
    " layers can better model the dependency of nearby token pairs. On the other\n",
    " hand, the absolute position of a word is also important, especially for decoding.\n",
    " For this reason, an absolute position embedding is added just before the softmax\n",
    " layer of the token decoding head. The progress on transformer decoder models has been spearheaded to a large extent\n",
    " by OpenAI. These models are exceptionally good at predicting the next word in a\n",
    " sequence and are thus mostly used for text generation tasks (see Chapter 5 for more\n",
    " details). Their progress has been fueled by using larger datasets and scaling the lan\n",
    "guage models to larger and larger sizes. Let’s have a look at the evolution of these fas\n",
    "cinating generation models:\n",
    " GPT\n",
    " The introduction of GPT combined two key ideas in NLP:17 the novel and effi\n",
    "cient transformer decoder architecture, and transfer learning. In that setup, the\n",
    " model was pretrained by predicting the next word based on the previous ones.\n",
    " The model was trained on the BookCorpus and achieved great results on down\n",
    "stream tasks such as classification.\n",
    " GPT-2\n",
    " Inspired by the success of the simple and scalable pretraining approach, the origi\n",
    "nal model and training set were upscaled to produce GPT-2.18 This model is able\n",
    " to produce long sequences of coherent text. Due to concerns about possible mis\n",
    "use, the model was released in a staged fashion, with smaller models being pub\n",
    "lished first and the full model later.\n",
    " CTRL\n",
    " Models like GPT-2 can continue an input sequence (also called a prompt). How\n",
    "ever, the user has little control over the style of the generated sequence. The\n",
    " Conditional Transformer Language (CTRL) model addresses this issue by adding\n",
    " “control tokens” at the beginning of the sequence.19 These allow the style of the\n",
    " generated text to be controlled, which allows for diverse generation.\n",
    "   Following the success of scaling GPT up to GPT-2, a thorough analysis on the\n",
    " behavior of language models at different scales revealed that there are simple\n",
    " power laws that govern the relation between compute, dataset size, model size,\n",
    " and the performance of a language model.20 Inspired by these insights, GPT-2\n",
    " was upscaled by a factor of 100 to yield GPT-3,21 with 175 billion parameters.\n",
    " Besides being able to generate impressively realistic text passages, the model also\n",
    " exhibits few-shot learning capabilities: with a few examples of a novel task such\n",
    " as translating text to code, the model is able to accomplish the task on new exam\n",
    "ples. OpenAI has not open-sourced this model, but provides an interface through\n",
    " the OpenAI API.\n",
    " GPT-Neo/GPT-J-6B\n",
    " GPT-Neo and GPT-J-6B are GPT-like models that were trained by EleutherAI, a\n",
    " collective of researchers who aim to re-create and release GPT-3 scale models.22\n",
    " The current models are smaller variants of the full 175-billion-parameter model,\n",
    " with 1.3, 2.7, and 6 billion parameters, and are competitive with the smaller\n",
    " GPT-3 models OpenAI offers.\n",
    " The final branch in the transformers tree of life is the encoder-decoder models. Let’s\n",
    " take a look.\n",
    " The Encoder-Decoder Branch\n",
    " Although it has become common to build models using a single encoder or decoder\n",
    " stack, there are several encoder-decoder variants of the Transformer architecture that\n",
    " have novel applications across both NLU and NLG domains:\n",
    " T5\n",
    " The T5 model unifies all NLU and NLG tasks by converting them into text-to\n",
    "text tasks.23 All tasks are framed as sequence-to-sequence tasks, where adopting\n",
    " an encoder-decoder architecture is natural. For text classification problems, for\n",
    " example, this means that the text is used as the encoder input and the decoder\n",
    " has to generate the label as normal text instead of a class. We will look at this in\n",
    " more detail in Chapter 6. The T5 architecture uses the original Transformer\n",
    " architecture.them to text-to-text tasks. The largest model with 11 billion parameters yielded\n",
    " state-of-the-art results on several benchmarks.\n",
    " BART\n",
    " BART combines the pretraining procedures of BERT and GPT within the\n",
    " encoder-decoder architecture.24 The input sequences undergo one of several pos\n",
    "sible transformations, from simple masking to sentence permutation, token dele\n",
    "tion, and document rotation. These modified inputs are passed through the\n",
    " encoder, and the decoder has to reconstruct the original texts. This makes the\n",
    " model more flexible as it is possible to use it for NLU as well as NLG tasks, and it\n",
    " achieves state-of-the-art-performance on both.\n",
    " M2M-100\n",
    " Conventionally a translation model is built for one language pair and translation\n",
    " direction. Naturally, this does not scale to many languages, and in addition there\n",
    " might be shared knowledge between language pairs that could be leveraged for\n",
    " translation between rare languages. M2M-100 is the first translation model that\n",
    " can translate between any of 100 languages.25 This allows for high-quality transla\n",
    "tions between rare and underrepresented languages. The model uses prefix\n",
    " tokens (similar to the special [CLS] token) to indicate the source and target\n",
    " language.\n",
    " BigBird\n",
    " One main limitation of transformer models is the maximum context size, due to\n",
    " the quadratic memory requirements of the attention mechanism. BigBird\n",
    " addresses this issue by using a sparse form of attention that scales linearly.26 This\n",
    " allows for the drastic scaling of contexts from 512 tokens in most BERT models\n",
    " to 4,096 in BigBird. This is especially useful in cases where long dependencies\n",
    " need to be conserved, such as in text summarization.\n",
    " Pretrained checkpoints of all models that we have seen in this section are available on\n",
    " the Hugging Face Hub and can be fine-tuned to your use case with  Transformers,\n",
    " as described in the previous chapter.So far in this book we have applied transformers to solve NLP tasks on English cor\n",
    "pora—but what do you do when your documents are written in Greek, Swahili, or\n",
    " Klingon? One approach is to search the Hugging Face Hub for a suitable pretrained\n",
    " language model and fine-tune it on the task at hand. However, these pretrained mod\n",
    "els tend to exist only for “high-resource” languages like German, Russian, or Man\n",
    "darin, where plenty of webtext is available for pretraining. Another common\n",
    " challenge arises when your corpus is multilingual: maintaining multiple monolingual\n",
    " models in production will not be any fun for you or your engineering team.\n",
    " Fortunately, there is a class of multilingual transformers that come to the rescue. Like\n",
    " BERT, these models use masked language modeling as a pretraining objective, but\n",
    " they are trained jointly on texts in over one hundred languages. By pretraining on\n",
    " huge corpora across many languages, these multilingual transformers enable zero\n",
    "shot cross-lingual transfer. This means that a model that is fine-tuned on one language\n",
    " can be applied to others without any further training! This also makes these models\n",
    " well suited for “code-switching,” where a speaker alternates between two or more lan\n",
    "guages or dialects in the context of a single conversation.\n",
    " In this chapter we will explore how a single transformer model called XLM-RoBERTa\n",
    " (introduced in Chapter 3)1 can be fine-tuned to perform named entity recognition\n",
    " (NER) across several languages. As we saw in Chapter 1, NER is a common NLP task\n",
    " that identifies entities like people, organizations, or locations in text. These entities\n",
    " can be used for various applications such as gaining insights from company docu\n",
    "ments, augmenting the quality of search engines, or simply building a structured\n",
    " database from a corpus. Multilingual transformers involve similar architectures and training procedures as\n",
    " their monolingual counterparts, except that the corpus used for pretraining consists\n",
    " of documents in many languages. A remarkable feature of this approach is that\n",
    " despite receiving no explicit information to differentiate among the languages, the\n",
    " resulting linguistic representations are able to generalize well across languages for a\n",
    " variety of downstream tasks. In some cases, this ability to perform cross-lingual\n",
    " transfer can produce results that are competitive with those of monolingual models,\n",
    " which circumvents the need to train one model per language!\n",
    " To measure the progress of cross-lingual transfer for NER, the CoNLL-2002 and\n",
    " CoNLL-2003 datasets are often used as a benchmark for English, Dutch, Spanish, and\n",
    " German. This benchmark consists of news articles annotated with the same LOC, PER,\n",
    " and ORG categories as PAN-X, but it contains an additional MISC label for miscellane\n",
    "ous entities that do not belong to the previous three groups. Multilingual transformer\n",
    " models are usually evaluated in three different ways:\n",
    " Fine-tune on the English training data and then evaluate on each language’s testset.each\n",
    " Fine-tune and evaluate on monolingual test data to measure per-language performance.all\n",
    " Fine-tune on all the training data to evaluate on all on each language’s test set.\n",
    " We will adopt a similar evaluation strategy for our NER task, but first we need to\n",
    " select a model to evaluate. One of the first multilingual transformers was mBERT,\n",
    " which uses the same architecture and pretraining objective as BERT but adds Wikipe\n",
    "dia articles from many languages to the pretraining corpus. Since then, mBERT has\n",
    " been superseded by XLM-RoBERTa (or XLM-R for short), so that’s the model we’ll consider in this chapter.\n",
    "Chapter 4: Multilingual Named Entity Recognition\n",
    "As we saw in Chapter 3, XLM-R uses only MLM as a pretraining objective for 100\n",
    " languages, but is distinguished by the huge size of its pretraining corpus compared to\n",
    " its predecessors: Wikipedia dumps for each language and 2.5 terabytes of Common\n",
    " Crawl data from the web. This corpus is several orders of magnitude larger than the\n",
    " ones used in earlier models and provides a significant boost in signal for low-resource\n",
    " languages like Burmese and Swahili, where only a small number of Wikipedia articles\n",
    " exist.\n",
    " The RoBERTa part of the model’s name refers to the fact that the pretraining\n",
    " approach is the same as for the monolingual RoBERTa models. RoBERTa’s developers\n",
    " improved on several aspects of BERT, in particular by removing the next sentence\n",
    " prediction task altogether.3 XLM-R also drops the language embeddings used in XLM\n",
    " and uses SentencePiece to tokenize the raw texts directly.4 Besides its multilingual\n",
    " nature, a notable difference between XLM-R and RoBERTa is the size of the respec\n",
    "tive vocabularies: 250,000 tokens versus 55,000!\n",
    " XLM-R is a great choice for multilingual NLU tasks. In the next section, we’ll explore\n",
    " how it can efficiently tokenize across many language.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Lowecasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.remove HTML tags and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_html(text):\n",
    "    pattern=  re.compile('<.*?>')\n",
    "    return pattern.sub(r' ' , text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern =  re.compile(r'https?://\\S + |www\\.\\S+')\n",
    "    return pattern.sub(r' ' , text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = remove_html(text)\n",
    "text = remove_url(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## removing /n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a miracle is taking place as you read these lines: the squiggles on this page are forming into words and concepts and emotions as they navigate their way through your cortex. my thoughts from november 2021 have now successfully invaded your brain. if they manage to catch your attention and survive long enough in this harsh and highly competitive environment, they may have a chance to reproduce again as  you share these thoughts with others. thanks to language, thoughts have become airborne and highly contagious brain germs—and no vaccine is coming. luckily, most brain germs are harmless,1 and a few are wonderfully useful. in fact, humanity’s brain germs constitute two of our most precious treasures: knowledge and culture. much as we can’t digest properly without healthy gut bacteria, we cannot think properly without healthy brain germs. most of your thoughts are not actually yours: they arose and grew and evolved in many other brains before they infected you. so if we want to build intelligent machines, we will need to find a way to infect them too. the good news is that another miracle has been unfolding over the last few years: several breakthroughs in deep learning have given birth to powerful language models. since you are reading this book, you have probably seen some astonishing demos of these language models, such as gpt-3, which given a short prompt such as “a frog meets a crocodile” can write a whole story. although it’s not quite shakespeare yet, it’s sometimes hard to believe that these texts were written by an artificial neural network. in fact, github’s copilot system is helping me write these lines: you’ll never know how much i really wrote. the revolution goes far beyond text generation. it encompasses the whole realm of natural language processing (nlp), from text classification to summarization, translation, question answering, chatbots, natural language understanding .more. wherever there’s language, speech or text, there’s an application for nlp. you can already ask your phone for tomorrow’s weather, or chat with a virtual help desk assistant to troubleshoot a problem, or get meaningful results from search engines that seem to truly understand your query. but the technology is so new that the best is probably yet to come. like most advances in science, this recent revolution in nlp rests upon the hard work of hundreds of unsung heroes. but three key ingredients of its success do stand out: • the transformer is a neural network architecture proposed in 2017 in a groundbreaking paper called “attention is all you need”, published by a team of google researchers. in just a few years it swept across the field, crushing previous architectures that were typically based on recurrent neural networks (rnns). the transformer architecture is excellent at capturing patterns in long sequences of data and dealing with huge datasets—so much so that its use is now extending well beyond nlp, for example to image processing tasks. • in most projects, you won’t have access to a huge dataset to train a model from scratch. luckily, it’s often possible to download a model that was pretrained on a generic dataset: all you need to do then is fine-tune it on your own (much smaller) dataset. pretraining has been mainstream in image processing since the early 2010s, but in nlp it was restricted to contextless word embeddings (i.e., dense vector representations of individual words). for example, the word “bear” had the same pretrained embedding in “teddy bear” and in “to bear.” then, in 2018, several papers proposed full-blown language models that could be pretrained and fine-tuned for a variety of nlp tasks; this completely changed the game. • model hubs like hugging face’s have also been a game-changer. in the early days, pretrained models were just posted anywhere, so it wasn’t easy to find what you needed. murphy’s law guaranteed that pytorch users would only find tensorflow models, and vice versa. and when you did find a model, figuring out how to fine-tune it wasn’t always easy. this is where hugging face’s transformers library comes in: it’s open source, it supports both tensorflow and pytorch, and it makes it easy to download a state-of-the-art pretrained model from the hugging face hub, configure it for your task, fine-tune it on your dataset, and evaluate it. use of the library is growing quickly: in q4 2021 it was used by over five thousand organizations and was installed using pip over four million times per month. moreover, the library and its ecosystem are expanding beyond nlp: image processing models are available too. you can also download numerous datasets from the hub to train or evaluate your models. so what more can you ask for? well, this book! it was written by open source developers at hugging face—including the creator of the transformers library!—and it forewordshows: the breadth and depth of the information you will find in these pages is astounding. it covers everything from the transformer architecture itself, to the transformers library and the entire ecosystem around it. i particularly appreciated the hands-on approach: you can follow along in jupyter notebooks, and all the code examples are straight to the point and simple to understand. the authors have extensive experience in training very large transformer models, and they provide a wealth of tips and tricks for getting everything to work efficiently. last but not least, their writing style is direct and lively: it reads like a novel. in short, i thoroughly enjoyed this book, and i’m certain you will too. anyone interested in building products with state-of-the-art language-processing features needs to read it. it’s packed to the brim with all the right brain germs!. hopefully, by now you are   excited to learn how to start training and integrating these versatile models into your own applications! you’ve seen in this chapter that with just a few lines of code you can use state-of-the-art models for classification, named entity recognition, question answering, translation, and summarization, but this is really just the “tip of the iceberg.” in the following chapters you will learn how to adapt transformers to a wide range of use cases, such as building a text classifier, or a lightweight model for production, or even training a language model from scratch. we’ll be taking a hands-on approach, which means that for every concept covered there will be accompanying code that you can run on google colab or your own gpu machine. now that we’re armed with the basic concepts behind transformers, it’s time to get our hands dirty with our first application: text classification. that’s the topic of the next chapter!. the basic idea behind subword tokenization is to combine the best aspects of character and word tokenization. on the one hand, we want to split rare words into smaller units to allow the model to deal with complex words and misspellings. on the other hand, we want to keep frequent words as unique entities so that we can keep the length of our inputs to a manageable size. the main distinguishing feature of subword tokenization (as well as word tokenization) is that it is learned from the pretraining corpus using a mix of statistical rules and algorithms. there are several subword tokenization algorithms that are commonly used in nlp, but let’s start with wordpiece,5 which is used by the bert and distilbert tokenizers. the easiest way to understand how wordpiece works is to see it in action.  transformers provides a convenient autotokenizer class that allows you to quickly load the tokenizer associated with a pretrained model—we just call its from_pretrained() method, providing the id of a model on the hub or a local file path. let’s start by loading the tokenizer for distilbert. we can observe three things here. first, some special [cls] and [sep] tokens have been added to the start and end of the sequence. these tokens differ from model to model, but their main role is to indicate the start and end of a sequence. second, the tokens have each been lowercased, which is a feature of this particular checkpoint. finally, we can see that “tokenizing” and “nlp” have been split into two tokens, which makes sense since they are not common words. the ## prefix in ##izing and ##p means that the preceding string is not whitespace; any token with this prefix should be merged with the previous token when you convert the tokens back to a string. the autotokenizer class has a convert_tokens_to_string() method for doing just that, so let’s apply it to our tokens:. from this plot we can see some clear patterns: the negative feelings such as sadness, anger, and fear all occupy similar regions with slightly varying distributions. on the other hand, joy and love are well separated from the negative emotions and also share a similar space. finally, surprise is scattered all over the place. although we may have hoped for some separation, this is in no way guaranteed since the model was not trained to know the difference between these emotions. it only learned them implicitly by guessing the masked words in texts. now that we’ve gained some insight into the features of our dataset, let’s finally train a model on it!. training the hidden states that serve as inputs to the classification model will help us avoid the problem of working with data that may not be well suited for the classification task. instead, the initial hidden states adapt during training to decrease the model loss and thus increase its performance. in chapter 2, we saw what it takes to fine-tune and evaluate a transformer. now let’s take a look at how they work under the hood. in this chapter we’ll explore the main building blocks of transformer models and how to implement them using pytorch. we’ll also provide guidance on how to do the same in tensorflow. we’ll first focus on building the attention mechanism, and then add the bits and pieces necessary to make a transformer encoder work. we’ll also have a brief look at the architectural differences between the encoder and decoder modules. by the end of this chapter you will be able to implement a simple transformer model yourself! while a deep technical understanding of the transformer architecture is generally not necessary to use  transformers and fine-tune models for your use case, it can be helpful for comprehending and navigating the limitations of transformers and using them in new domains. this chapter also introduces a taxonomy of transformers to help you understand the zoo of models that have emerged in recent years. before diving into the code, let’s start with an overview of the original architecture that kick-started the transformer revolution.gets these two as an input as well as all the encoder’s outputs to predict the next token, “fliegt”. in the next step the decoder gets “fliegt” as an additional input. we repeat the process until the decoder predicts the eos token or we reached a maximum length. the transformer architecture was originally designed for sequence-to-sequence tasks like machine translation, but both the encoder and decoder blocks were soon adapted as standalone models. although there are hundreds of different transformer models, most of them belong to one of three types: encoder-only these models convert an input sequence of text into a rich numerical representation that is well suited for tasks like text classification or named entity recognition. bert and its variants, like roberta and distilbert, belong to this class of architectures. the representation computed for a given token in this architecture depends both on the left (before the token) and the right (after the token) contexts. this is often called bidirectional attention. decoder-only given a prompt of text like “thanks for lunch, i had a…” these models will autocomplete the sequence by iteratively predicting the most probable next word. the family of gpt models belong to this class. the representation computed for a given token in this architecture depends only on the left context. this is often called causal or autoregressive attention. encoder-decoder these are used for modeling complex mappings from one sequence of text to another; they’re suitable for machine translation and summarization tasks. in addition to the transformer architecture, which as we’ve seen combines an encoder and a decoder, the bart and t5 models belong to this class. in reality, the distinction between applications for decoder-only versus encoder-only architectures is a bit blurry. for example, decoder-only models like those in the gpt family can be primed for tasks like translation that are conventionally thought of as sequence-to-sequence tasks. similarly, encoder-only models like bert can be applied to summarization tasks that are usually associated with encoder-decoder or decoder-only models.1 now that you have a high-level understanding of the transformer architecture, let’s.   as we discussed in chapter 1, attention is a mechanism that allows neural networks to assign a different amount of weight or “attention” to each element in a sequence. for text sequences, the elements are token embeddings like the ones we encountered in chapter 2, where each token is mapped to a vector of some fixed dimension. for example, in bert each token is represented as a 768-dimensional vector. the “self” part of self-attention refers to the fact that these weights are computed for all hidden states in the same set—for example, all the hidden states of the encoder. by contrast, the attention mechanism associated with recurrent models involves computing the relevance of each encoder hidden state to the decoder hidden state at a given decoding timestep. the main idea behind self-attention is that instead of using a fixed embedding for each token, we can use the whole sequence to compute a weighted average of each embedding. . project each token embedding into three vectors called query, key, and value. 2. compute attention scores. we determine how much the query and key vectors relate to each other using a similarity function. as the name suggests, the similarity function for scaled dot-product attention is the dot product, computed efficiently using matrix multiplication of the embeddings. queries and keys that are similar will have a large dot product, while those that don’t share much in common will have little to no overlap. the notion of query, key, and value vectors may seem a bit cryptic the first time you encounter them. their names were inspired by information retrieval systems, but we can motivate their meaning with a simple analogy. imagine that you’re at the supermarket buying all the ingredients you need for your dinner. you have the dish’s recipe, and each of the required ingredients can be thought of as a query. as you scan the shelves, you look at the labels (keys) and check whether they match an ingredient on your list (similarity function). if you have a match, then you take the item (value) from the shelf. in this analogy, you only get one grocery item for every label that matches the ingredient. self-attention is a more abstract and “smooth” version of this: every label in the supermarket matches the ingredient to the extent to which each key matches the query. so if your list includes a dozen eggs, then you might end up grabbing 10 eggs, an omelette, and a chicken wing. this has created a 5 ×5 matrix of attention scores per sample in the batch. we’ll see later that the query, key, and value vectors are generated by applying independent weight matrices wq,k,v to the embeddings, but for now we’ve kept them equal for simplicity. in scaled dot-product attention, the dot products are scaled by the size of the embedding vectors so that we don’t get too many large numbers during training that can cause the softmax we will apply next to saturate. the torch.bmm() function performs a batch matrix-matrix product that simplifies the computation of the attention scores where the query and key vectors have the shape [batch_size, seq_len, hidden_dim]. if we ignored the batch dimension we could calculate the dot product between each query and key vector by simply transposing the key tensor to have the shape [hidden_dim, seq_len] and then using the matrix product to collect all the dot products in a [seq_len, seq_len] matrix. since we want to do this for all sequences in the batch independently, we use torch.bmm(), which takes two batches of matrices and multiplies each matrix from the first batch with the corresponding matrix in the second batch.our attention mechanism with equal query and key vectors will assign a very large score to identical words in the context, and in particular to the current word itself: the dot product of a query with itself is always 1. but in practice, the meaning of a word will be better informed by complementary words in the context than by identical words—for example, the meaning of “flies” is better defined by incorporating information from “time” and “arrow” than by another mention of “flies”. how can we promote this behavior? let’s allow the model to create a different set of vectors for the query, key, and value of a token by using three different linear projections to project our initial token vector into three different spaces. multi-headed attention in our simple example, we only used the embeddings “as is” to compute the attention scores and weights, but that’s far from the whole story. in practice, the self-attention layer applies three independent linear transformations to each embedding to generate the query, key, and value vectors. these transformations project the embeddings and each projection carries its own set of learnable parameters, which allows the selfattention layer to focus on different semantic aspects of the sequence. it also turns out to be beneficial to have multiple sets of linear projections, each one representing a so-called attention head. the resulting multi-head attention layer is illustrated in figure 3-5. but why do we need more than one attention head? the reason is that the softmax of one head tends to focus on mostly one aspect of similarity. having several heads allows the model to focus on several aspects at once. for instance, one head can focus on subject-verb interaction, whereas another finds nearby adjectives. obviously we don’t handcraft these relations into the model, and they are fully learned from the data. if you are familiar with computer vision models you might see the resemblance to filters in convolutional neural networks, where one filter can be responsible for detecting faces and another one finds wheels of cars in images.this visualization shows the attention weights as lines connecting the token whose embedding is getting updated (left) with every word that is being attended to (right). the intensity of the lines indicates the strength of the attention weights, with dark lines representing values close to 1, and faint lines representing values close to 0. in this example, the input consists of two sentences and the [cls] and [sep] tokens are the special tokens in bert’s tokenizer that we encountered in chapter 2. one thing we can see from the visualization is that the attention weights are strongest between words that belong to the same sentence, which suggests bert can tell that it should attend to words in the same sentence. however, for the word “flies” we can see that bert has identified “arrow” as important in the first sentence and “fruit” and “banana” in the second. these attention weights allow the model to distinguish the use of “flies” as a verb or noun, depending on the context in which it occurs! now that we’ve covered attention, let’s take a look at implementing the missing piece of the encoder layer: position-wise feed-forward networks. the feed-forward sublayer in   the encoder and decoder is just a simple two-layer fully connected neural network, but with a twist: instead of processing the whole sequence of embeddings as a single vector, it processes each embedding independently. for this reason, this layer is often referred to as a position-wise feed-forward layer. you may also see it referred to as a one-dimensional convolution with a kernel size of one, typically by people with a computer vision background (e.g., the openai gpt codebase uses this nomenclature). a rule of thumb from the literature is for the hidden size of the first layer to be four times the size of the embeddings, and a gelu activation function is most commonly used. this is where most of the capacity and memorization is hypothesized to happen, and it’s the part that is most often scaled when scaling up the models. we can implement this as a simple nn.module as follows. as mentioned earlier,  the transformer architecture makes use of layer normalization and skip connections. the former normalizes each input in the batch to have zero mean and unity variance. skip connections pass a tensor to the next layer of the model without processing and add it to the processed tensor. when it comes to placing the layer normalization in the encoder or decoder layers of a transformer, there are two main choices adopted in the literature: post layer normalization this is the arrangement used in the transformer paper; it places layer normalization in between the skip connections. this arrangement is tricky to train from scratch as the gradients can diverge. for this reason, you will often see a concept known as learning rate warm-up, where the learning rate is gradually increased from a small value to some maximum value during training. pre layer normalization this is the most common arrangement found in the literature; it places layer normalization within the span of the skip connections. this tends to be much more stable during training, and it does not usually require any learning rate warm-up.  positional embeddings are based on a simple, yet very effective idea: augment the token embeddings with a position-dependent pattern of values arranged in a vector. if the pattern is characteristic for each position, the attention heads and feed-forward layers in each stack can learn to incorporate positional information into their transformations. there are several ways to achieve this, and one of the most popular approaches is to use a learnable pattern, especially when the pretraining dataset is sufficiently large. this works exactly the same way as the token embeddings, but using the position index instead of the token id as input. with that approach, an efficient way of encoding the positions of tokens is learned during pretraining. let’s create a custom embeddings module that combines a token embedding layer that projects the input_ids to a dense hidden state together with the positional embedding that does the same for position_ids. the resulting embedding is simply the sum of both embeddings: we can see that we get a hidden state for each token in the batch. this output format makes the architecture very flexible, and we can easily adapt it for various applications such as predicting missing tokens in masked language modeling or predicting the start and end position of an answer in question answering. in the following section we’ll see how we can build a classifier like the one we used in chapter 2. transformer models are usually divided into a task-independent body and a taskspecific head. we’ll encounter this pattern again in chapter 4 when we look at the design pattern of  transformers. what we have built so far is the body, so if we wish to build a text classifier, we will need to attach a classification head to that body. we have a hidden state for each token, but we only need to make one prediction. there are several options to approach this. traditionally, the first token in such models is used for the prediction and we can attach a dropout and a linear layer to make a classification prediction. the following class extends the existing encoder for sequence classification. let’s see if we can shed some light on the mysteries of encoder-decoder attention. imagine you (the decoder) are in class taking an exam. your task is to predict the next word based on the previous words (decoder inputs), which sounds simple but is incredibly hard (try it yourself and predict the next words in a passage of this book). fortunately, your neighbor (the encoder) has the full text. unfortunately, they’re a foreign exchange student and the text is in their mother tongue. cunning students that you are, you figure out a way to cheat anyway. you draw a little cartoon illustrating the text you already have (the query) and give it to your neighbor. they try to figure out which passage matches that description (the key), draw a cartoon describing the word following that passage (the value), and pass that back to you. with this system in place, you ace the exam. meet the transformers as you’ve seen in this chapter, there are three main architectures for transformer models: encoders, decoders, and encoder-decoders. the initial success of the early transformer models triggered a cambrian explosion in model development as researchers built models on various datasets of different size and nature, used new pretraining objectives, and tweaked the architecture to further improve performance. although the zoo of models is still growing fast, they can still be divided into these three categories.dataset. since the dataset only contains data without parallel texts (i.e., translations), the tlm objective of xlm was dropped. this approach beats xlm and multilingual bert variants by a large margin, especially on low-resource languages. albert the albert model introduced three changes to make the encoder architecture more efficient.13 first, it decouples the token embedding dimension from the hidden dimension, thus allowing the embedding dimension to be small and thereby saving parameters, especially when the vocabulary gets large. second, all layers share the same parameters, which decreases the number of effective parameters even further. finally, the nsp objective is replaced with a sentence-ordering prediction: the model needs to predict whether or not the order of two consecutive sentences was swapped rather than predicting if they belong together at all. these changes make it possible to train even larger models with fewer parameters and reach superior performance on nlu tasks. electra one limitation of the standard mlm pretraining objective is that at each training step only the representations of the masked tokens are updated, while the other input tokens are not. to address this issue, electra uses a two-model approach:14 the first model (which is typically small) works like a standard masked language model and predicts masked tokens. the second model, called the discriminator, is then tasked to predict which of the tokens in the first model’s output were originally masked. therefore, the discriminator needs to make a binary classification for every token, which makes training 30 times more efficient. for downstream tasks the discriminator is fine-tuned like a standard bert model. deberta the deberta model introduces two architectural changes.15 first, each token is represented as two vectors: one for the content, the other for relative position. by disentangling the tokens’ content from their relative positions, the self-attention layers can better model the dependency of nearby token pairs. on the other hand, the absolute position of a word is also important, especially for decoding. for this reason, an absolute position embedding is added just before the softmax layer of the token decoding head. the progress on transformer decoder models has been spearheaded to a large extent by openai. these models are exceptionally good at predicting the next word in a sequence and are thus mostly used for text generation tasks (see chapter 5 for more details). their progress has been fueled by using larger datasets and scaling the language models to larger and larger sizes. let’s have a look at the evolution of these fascinating generation models: gpt the introduction of gpt combined two key ideas in nlp:17 the novel and efficient transformer decoder architecture, and transfer learning. in that setup, the model was pretrained by predicting the next word based on the previous ones. the model was trained on the bookcorpus and achieved great results on downstream tasks such as classification. gpt-2 inspired by the success of the simple and scalable pretraining approach, the original model and training set were upscaled to produce gpt-2.18 this model is able to produce long sequences of coherent text. due to concerns about possible misuse, the model was released in a staged fashion, with smaller models being published first and the full model later. ctrl models like gpt-2 can continue an input sequence (also called a prompt). however, the user has little control over the style of the generated sequence. the conditional transformer language (ctrl) model addresses this issue by adding “control tokens” at the beginning of the sequence.19 these allow the style of the generated text to be controlled, which allows for diverse generation.   following the success of scaling gpt up to gpt-2, a thorough analysis on the behavior of language models at different scales revealed that there are simple power laws that govern the relation between compute, dataset size, model size, and the performance of a language model.20 inspired by these insights, gpt-2 was upscaled by a factor of 100 to yield gpt-3,21 with 175 billion parameters. besides being able to generate impressively realistic text passages, the model also exhibits few-shot learning capabilities: with a few examples of a novel task such as translating text to code, the model is able to accomplish the task on new examples. openai has not open-sourced this model, but provides an interface through the openai api. gpt-neo/gpt-j-6b gpt-neo and gpt-j-6b are gpt-like models that were trained by eleutherai, a collective of researchers who aim to re-create and release gpt-3 scale models.22 the current models are smaller variants of the full 175-billion-parameter model, with 1.3, 2.7, and 6 billion parameters, and are competitive with the smaller gpt-3 models openai offers. the final branch in the transformers tree of life is the encoder-decoder models. let’s take a look. the encoder-decoder branch although it has become common to build models using a single encoder or decoder stack, there are several encoder-decoder variants of the transformer architecture that have novel applications across both nlu and nlg domains: t5 the t5 model unifies all nlu and nlg tasks by converting them into text-totext tasks.23 all tasks are framed as sequence-to-sequence tasks, where adopting an encoder-decoder architecture is natural. for text classification problems, for example, this means that the text is used as the encoder input and the decoder has to generate the label as normal text instead of a class. we will look at this in more detail in chapter 6. the t5 architecture uses the original transformer architecture.them to text-to-text tasks. the largest model with 11 billion parameters yielded state-of-the-art results on several benchmarks. bart bart combines the pretraining procedures of bert and gpt within the encoder-decoder architecture.24 the input sequences undergo one of several possible transformations, from simple masking to sentence permutation, token deletion, and document rotation. these modified inputs are passed through the encoder, and the decoder has to reconstruct the original texts. this makes the model more flexible as it is possible to use it for nlu as well as nlg tasks, and it achieves state-of-the-art-performance on both. m2m-100 conventionally a translation model is built for one language pair and translation direction. naturally, this does not scale to many languages, and in addition there might be shared knowledge between language pairs that could be leveraged for translation between rare languages. m2m-100 is the first translation model that can translate between any of 100 languages.25 this allows for high-quality translations between rare and underrepresented languages. the model uses prefix tokens (similar to the special [cls] token) to indicate the source and target language. bigbird one main limitation of transformer models is the maximum context size, due to the quadratic memory requirements of the attention mechanism. bigbird addresses this issue by using a sparse form of attention that scales linearly.26 this allows for the drastic scaling of contexts from 512 tokens in most bert models to 4,096 in bigbird. this is especially useful in cases where long dependencies need to be conserved, such as in text summarization. pretrained checkpoints of all models that we have seen in this section are available on the hugging face hub and can be fine-tuned to your use case with  transformers, as described in the previous chapter.so far in this book we have applied transformers to solve nlp tasks on english corpora—but what do you do when your documents are written in greek, swahili, or klingon? one approach is to search the hugging face hub for a suitable pretrained language model and fine-tune it on the task at hand. however, these pretrained models tend to exist only for “high-resource” languages like german, russian, or mandarin, where plenty of webtext is available for pretraining. another common challenge arises when your corpus is multilingual: maintaining multiple monolingual models in production will not be any fun for you or your engineering team. fortunately, there is a class of multilingual transformers that come to the rescue. like bert, these models use masked language modeling as a pretraining objective, but they are trained jointly on texts in over one hundred languages. by pretraining on huge corpora across many languages, these multilingual transformers enable zeroshot cross-lingual transfer. this means that a model that is fine-tuned on one language can be applied to others without any further training! this also makes these models well suited for “code-switching,” where a speaker alternates between two or more languages or dialects in the context of a single conversation. in this chapter we will explore how a single transformer model called xlm-roberta (introduced in chapter 3)1 can be fine-tuned to perform named entity recognition (ner) across several languages. as we saw in chapter 1, ner is a common nlp task that identifies entities like people, organizations, or locations in text. these entities can be used for various applications such as gaining insights from company documents, augmenting the quality of search engines, or simply building a structured database from a corpus. multilingual transformers involve similar architectures and training procedures as their monolingual counterparts, except that the corpus used for pretraining consists of documents in many languages. a remarkable feature of this approach is that despite receiving no explicit information to differentiate among the languages, the resulting linguistic representations are able to generalize well across languages for a variety of downstream tasks. in some cases, this ability to perform cross-lingual transfer can produce results that are competitive with those of monolingual models, which circumvents the need to train one model per language! to measure the progress of cross-lingual transfer for ner, the conll-2002 and conll-2003 datasets are often used as a benchmark for english, dutch, spanish, and german. this benchmark consists of news articles annotated with the same loc, per, and org categories as pan-x, but it contains an additional misc label for miscellaneous entities that do not belong to the previous three groups. multilingual transformer models are usually evaluated in three different ways: fine-tune on the english training data and then evaluate on each language’s testset.each fine-tune and evaluate on monolingual test data to measure per-language performance.all fine-tune on all the training data to evaluate on all on each language’s test set. we will adopt a similar evaluation strategy for our ner task, but first we need to select a model to evaluate. one of the first multilingual transformers was mbert, which uses the same architecture and pretraining objective as bert but adds wikipedia articles from many languages to the pretraining corpus. since then, mbert has been superseded by xlm-roberta (or xlm-r for short), so that’s the model we’ll consider in this chapter.chapter 4: multilingual named entity recognitionas we saw in chapter 3, xlm-r uses only mlm as a pretraining objective for 100 languages, but is distinguished by the huge size of its pretraining corpus compared to its predecessors: wikipedia dumps for each language and 2.5 terabytes of common crawl data from the web. this corpus is several orders of magnitude larger than the ones used in earlier models and provides a significant boost in signal for low-resource languages like burmese and swahili, where only a small number of wikipedia articles exist. the roberta part of the model’s name refers to the fact that the pretraining approach is the same as for the monolingual roberta models. roberta’s developers improved on several aspects of bert, in particular by removing the next sentence prediction task altogether.3 xlm-r also drops the language embeddings used in xlm and uses sentencepiece to tokenize the raw texts directly.4 besides its multilingual nature, a notable difference between xlm-r and roberta is the size of the respective vocabularies: 250,000 tokens versus 55,000! xlm-r is a great choice for multilingual nlu tasks. in the next section, we’ll explore how it can efficiently tokenize across many language.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = re.sub(r'\\n', '', text)\n",
    "print(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emojiNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached emoji-2.13.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7.0 in c:\\users\\gupta\\anaconda3\\envs\\env3\\lib\\site-packages (from emoji) (4.11.0)\n",
      "Using cached emoji-2.13.2-py3-none-any.whl (553 kB)\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.13.2\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = emoji.demojize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = text.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'to': 2, 'a': 3, 'of': 4, 'and': 5, 'in': 6, 'is': 7, 'for': 8, 'that': 9, 'this': 10, 'as': 11, 'models': 12, 'model': 13, 'we': 14, 'are': 15, 'on': 16, 'it': 17, 'you': 18, 'with': 19, 'can': 20, 'by': 21, 'from': 22, 'attention': 23, 'have': 24, 'token': 25, 'these': 26, 'transformer': 27, 'each': 28, 'text': 29, 'one': 30, 'encoder': 31, 'decoder': 32, 'language': 33, 'or': 34, 'be': 35, 'your': 36, 'but': 37, 'gpt': 38, 'architecture': 39, 'tasks': 40, 'chapter': 41, 'which': 42, 'like': 43, 'all': 44, 'sequence': 45, 'will': 46, 'at': 47, 'only': 48, 'transformers': 49, 'tokens': 50, 'layer': 51, 'an': 52, 'pretraining': 53, 'used': 54, 'first': 55, 'languages': 56, 'not': 57, 'training': 58, 'into': 59, 'they': 60, 'so': 61, 'has': 62, 'query': 63, 'key': 64, 'use': 65, 'was': 66, 'fine': 67, 'word': 68, 'embeddings': 69, 'embedding': 70, 'words': 71, 'most': 72, 'two': 73, 'several': 74, 'how': 75, 'also': 76, 'using': 77, 'let’s': 78, 'bert': 79, 'see': 80, 'between': 81, 'hidden': 82, 'classification': 83, 'three': 84, 'next': 85, 'size': 86, '2': 87, 'nlp': 88, 'more': 89, 'pretrained': 90, 'same': 91, 'where': 92, 'simple': 93, 'there': 94, 'input': 95, 'their': 96, 'if': 97, 'our': 98, 'such': 99, 'its': 100, 'well': 101, 'state': 102, 'task': 103, 'approach': 104, 'we’ll': 105, 'vectors': 106, 'position': 107, 'xlm': 108, 'multilingual': 109, 'now': 110, 'need': 111, 'them': 112, 'been': 113, 'some': 114, '3': 115, 'translation': 116, 'example': 117, 'dataset': 118, 'then': 119, 'tune': 120, 'when': 121, 'class': 122, 'different': 123, 'value': 124, 'batch': 125, '1': 126, 'were': 127, 'do': 128, 'called': 129, 'just': 130, 'data': 131, 'large': 132, 'look': 133, 'dot': 134, 'product': 135, 'matrix': 136, 'parameters': 137, 'head': 138, 'lines': 139, 'way': 140, 'much': 141, 'many': 142, 'other': 143, 'since': 144, 'it’s': 145, 'processing': 146, 'often': 147, 'vector': 148, 'makes': 149, 'evaluate': 150, 'start': 151, 'main': 152, 'corpus': 153, 'allows': 154, 'common': 155, 'masked': 156, 'belong': 157, 'uses': 158, 'brain': 159, 'few': 160, 'another': 161, 'over': 162, 'learning': 163, 'given': 164, 'texts': 165, 'neural': 166, 'i': 167, 'across': 168, 'previous': 169, 'train': 170, 'hugging': 171, 'both': 172, 'similar': 173, 'performance': 174, 'make': 175, 'roberta': 176, 'predicting': 177, 'context': 178, 'weights': 179, 'sentence': 180, 'normalization': 181, 'objective': 182, 'may': 183, 'without': 184, 'find': 185, 'book': 186, 'seen': 187, 'whole': 188, 'although': 189, 'summarization': 190, 'get': 191, 'out': 192, 'architectures': 193, 'sequences': 194, 'possible': 195, 'smaller': 196, 'tuned': 197, 'what': 198, 'library': 199, 'art': 200, 'hub': 201, 'per': 202, 'information': 203, 'code': 204, 'building': 205, 'applications': 206, 'following': 207, 'every': 208, 'tokenization': 209, 'hand': 210, '5': 211, 'end': 212, 'second': 213, 'any': 214, 'instead': 215, 'during': 216, 'focus': 217, 'mechanism': 218, 'able': 219, 'predict': 220, 'dimension': 221, 'self': 222, 'function': 223, 'up': 224, 'than': 225, 'openai': 226, 'pattern': 227, 'especially': 228, 'prediction': 229, 'larger': 230, 'nlu': 231, '100': 232, 'monolingual': 233, 'r': 234, 'thoughts': 235, 'long': 236, 'share': 237, 'no': 238, 'germs': 239, 'fact': 240, 'before': 241, 'want': 242, 'build': 243, 'too': 244, 'far': 245, 'generation': 246, 'results': 247, 'understand': 248, 'new': 249, 'work': 250, 'success': 251, 'based': 252, 'networks': 253, 'huge': 254, 'own': 255, 'full': 256, 'datasets': 257, 'very': 258, 'novel': 259, 'named': 260, 'entity': 261, 'means': 262, 'aspects': 263, 'allow': 264, 'entities': 265, 'inputs': 266, 'learned': 267, 'finally': 268, 'trained': 269, 'we’ve': 270, 'states': 271, 'take': 272, 'original': 273, 'variants': 274, 'computed': 275, 't5': 276, 'usually': 277, 'compute': 278, 'scores': 279, 'similarity': 280, 'scaled': 281, 'label': 282, 'matches': 283, 'seq': 284, 'len': 285, '“flies”': 286, 'set': 287, 'linear': 288, 'transformations': 289, 'reason': 290, 'feed': 291, 'forward': 292, 'single': 293, 'scaling': 294, 'skip': 295, 'connections': 296, 'layers': 297, 'small': 298, 'efficient': 299, '4': 300, 'transfer': 301, 'billion': 302, 'ner': 303, 'taking': 304, 'place': 305, 'emotions': 306, 'through': 307, 'competitive': 308, 'years': 309, 'short': 310, 'prompt': 311, 'yet': 312, 'hard': 313, 'written': 314, 'network': 315, 'revolution': 316, 'beyond': 317, 'natural': 318, 'question': 319, 'answering': 320, 'understanding': 321, 'help': 322, 'search': 323, 'ingredients': 324, '•': 325, 'researchers': 326, 'typically': 327, 'image': 328, 'scratch': 329, 'download': 330, 'early': 331, 'e': 332, 'representations': 333, '”': 334, 'could': 335, 'easy': 336, 'pytorch': 337, 'tensorflow': 338, 'open': 339, 'source': 340, 'face': 341, 'times': 342, 'available': 343, 'itself': 344, 'hands': 345, 'examples': 346, 'efficiently': 347, 'style': 348, 'products': 349, 'needs': 350, 'right': 351, 'learn': 352, 'recognition': 353, 'adapt': 354, 'cases': 355, 'classifier': 356, 'even': 357, 'machine': 358, 'behind': 359, 'that’s': 360, 'idea': 361, 'subword': 362, 'rare': 363, 'feature': 364, 'distilbert': 365, 'works': 366, 'provides': 367, 'tokenizer': 368, 'associated': 369, 'special': 370, 'cls': 371, 'particular': 372, 'prefix': 373, 'string': 374, 'convert': 375, 'suited': 376, 'initial': 377, 'thus': 378, 'saw': 379, 'explore': 380, 'implement': 381, 'while': 382, 'gets': 383, 'maximum': 384, 'representation': 385, 'left': 386, 'modeling': 387, 'combines': 388, 'bart': 389, 'those': 390, 'applied': 391, 'ones': 392, 'part': 393, 'decoding': 394, 'project': 395, 'don’t': 396, 'little': 397, 'inspired': 398, 'meaning': 399, 'ingredient': 400, 'might': 401, 'generated': 402, 'independent': 403, 'softmax': 404, 'simply': 405, 'tensor': 406, 'better': 407, 'create': 408, 'generate': 409, 'representing': 410, 'resulting': 411, 'figure': 412, 'being': 413, 'values': 414, 'consists': 415, 'however': 416, 'literature': 417, 'arrangement': 418, 'rate': 419, 'does': 420, 'positional': 421, 'various': 422, 'section': 423, 'body': 424, 'built': 425, 'passage': 426, 'further': 427, 'changes': 428, 'standard': 429, 'issue': 430, 'discriminator': 431, 'downstream': 432, 'progress': 433, 'produce': 434, 'nlg': 435, 'bigbird': 436, 'english': 437, 'documents': 438, 'cross': 439, 'lingual': 440, 'articles': 441, 'wikipedia': 442, 'miracle': 443, 'read': 444, 'concepts': 445, '2021': 446, 'highly': 447, 'again': 448, 'others': 449, 'become': 450, 'luckily': 451, 'useful': 452, 'knowledge': 453, 'properly': 454, 'healthy': 455, 'good': 456, 'news': 457, 'last': 458, 'deep': 459, 'probably': 460, 'write': 461, 'story': 462, 'system': 463, 'know': 464, 'really': 465, 'there’s': 466, 'application': 467, 'already': 468, 'ask': 469, 'problem': 470, 'engines': 471, 'seem': 472, 'best': 473, 'come': 474, 'recent': 475, 'hundreds': 476, 'proposed': 477, 'paper': 478, 'published': 479, 'team': 480, 'google': 481, 'recurrent': 482, 'patterns': 483, 'projects': 484, 'dense': 485, 'had': 486, 'variety': 487, 'game': 488, 'face’s': 489, 'wasn’t': 490, 'guaranteed': 491, 'always': 492, 'comes': 493, 'growing': 494, 'quickly': 495, 'organizations': 496, 'four': 497, 'ecosystem': 498, 'developers': 499, 'everything': 500, 'provide': 501, 'getting': 502, 'features': 503, 'you’ve': 504, 'production': 505, 'concept': 506, 'covered': 507, 'basic': 508, 'time': 509, 'split': 510, 'complex': 511, 'keep': 512, 'length': 513, 'algorithms': 514, 'commonly': 515, 'wordpiece': 516, 'autotokenizer': 517, 'method': 518, 'id': 519, 'sep': 520, 'added': 521, 'indicate': 522, 'should': 523, 'back': 524, 'apply': 525, 'negative': 526, 'difference': 527, 'takes': 528, 'blocks': 529, 'add': 530, 'necessary': 531, 'architectural': 532, 'yourself': 533, 'case': 534, 'domains': 535, 'introduces': 536, 'zoo': 537, '“fliegt”': 538, 'step': 539, 'additional': 540, 'predicts': 541, 'originally': 542, 'depends': 543, 'contexts': 544, 'family': 545, 'they’re': 546, 'suitable': 547, 'addition': 548, 'versus': 549, 'bit': 550, 'conventionally': 551, 'thought': 552, 'high': 553, 'assign': 554, 'weight': 555, 'encountered': 556, 'fixed': 557, 'represented': 558, 'dimensional': 559, 'refers': 560, 'name': 561, 'suggests': 562, 'keys': 563, 'encounter': 564, 'analogy': 565, 'imagine': 566, 'supermarket': 567, 'whether': 568, 'match': 569, 'list': 570, 'item': 571, 'extent': 572, 'eggs': 573, 'later': 574, 'matrices': 575, 'equal': 576, 'torch': 577, 'bmm': 578, 'shape': 579, 'dim': 580, 'independently': 581, 'identical': 582, 'current': 583, 'practice': 584, '“arrow”': 585, 'behavior': 586, 'projections': 587, 'multi': 588, 'learnable': 589, 'multiple': 590, 'tends': 591, 'mostly': 592, 'heads': 593, 'verb': 594, 'finds': 595, 'nearby': 596, 'fully': 597, 'computer': 598, 'vision': 599, 'visualization': 600, 'updated': 601, 'close': 602, 'sentences': 603, 'important': 604, 'missing': 605, 'wise': 606, 'referred': 607, 'people': 608, 'module': 609, 'earlier': 610, 'pass': 611, 'places': 612, 'warm': 613, 'within': 614, 'effective': 615, 'stack': 616, 'ways': 617, 'positions': 618, 'ids': 619, 'together': 620, 'output': 621, 'flexible': 622, 'divided': 623, 'attach': 624, 'exam': 625, 'try': 626, 'fortunately': 627, 'neighbor': 628, 'draw': 629, 'cartoon': 630, 'decoders': 631, 'nature': 632, 'still': 633, 'categories': 634, 'contains': 635, 'translations': 636, 'low': 637, 'resource': 638, 'albert': 639, 'introduced': 640, 'number': 641, 'electra': 642, 'limitation': 643, 'mlm': 644, 'model’s': 645, 'deberta': 646, 'content': 647, 'relative': 648, 'pairs': 649, 'absolute': 650, 'great': 651, 'upscaled': 652, 'due': 653, 'ctrl': 654, 'addresses': 655, 'scales': 656, 'insights': 657, '175': 658, 'besides': 659, 'neo': 660, 'j': 661, '6b': 662, 'scale': 663, '6': 664, 'branch': 665, 'procedures': 666, 'm2m': 667, 'quality': 668, 'swahili': 669, 'exist': 670, 'german': 671, 'perform': 672, 'measure': 673, 'conll': 674, 'benchmark': 675, 'language’s': 676, 'test': 677, 'mbert': 678, 'tokenize': 679, '000': 680, 'squiggles': 681, 'page': 682, 'forming': 683, 'navigate': 684, 'cortex': 685, 'my': 686, 'november': 687, 'successfully': 688, 'invaded': 689, 'manage': 690, 'catch': 691, 'survive': 692, 'enough': 693, 'harsh': 694, 'environment': 695, 'chance': 696, 'reproduce': 697, 'thanks': 698, 'airborne': 699, 'contagious': 700, 'germs—and': 701, 'vaccine': 702, 'coming': 703, 'harmless': 704, 'wonderfully': 705, 'humanity’s': 706, 'constitute': 707, 'precious': 708, 'treasures': 709, 'culture': 710, 'can’t': 711, 'digest': 712, 'gut': 713, 'bacteria': 714, 'cannot': 715, 'think': 716, 'actually': 717, 'yours': 718, 'arose': 719, 'grew': 720, 'evolved': 721, 'brains': 722, 'infected': 723, 'intelligent': 724, 'machines': 725, 'infect': 726, 'unfolding': 727, 'breakthroughs': 728, 'birth': 729, 'powerful': 730, 'reading': 731, 'astonishing': 732, 'demos': 733, '“a': 734, 'frog': 735, 'meets': 736, 'crocodile”': 737, 'quite': 738, 'shakespeare': 739, 'sometimes': 740, 'believe': 741, 'artificial': 742, 'github’s': 743, 'copilot': 744, 'helping': 745, 'me': 746, 'you’ll': 747, 'never': 748, 'wrote': 749, 'goes': 750, 'encompasses': 751, 'realm': 752, 'chatbots': 753, 'wherever': 754, 'speech': 755, 'phone': 756, 'tomorrow’s': 757, 'weather': 758, 'chat': 759, 'virtual': 760, 'desk': 761, 'assistant': 762, 'troubleshoot': 763, 'meaningful': 764, 'truly': 765, 'technology': 766, 'advances': 767, 'science': 768, 'rests': 769, 'upon': 770, 'unsung': 771, 'heroes': 772, 'stand': 773, '2017': 774, 'groundbreaking': 775, '“attention': 776, 'need”': 777, 'swept': 778, 'field': 779, 'crushing': 780, 'rnns': 781, 'excellent': 782, 'capturing': 783, 'dealing': 784, 'datasets—so': 785, 'extending': 786, 'won’t': 787, 'access': 788, 'generic': 789, 'mainstream': 790, '2010s': 791, 'restricted': 792, 'contextless': 793, 'individual': 794, '“bear”': 795, '“teddy': 796, 'bear”': 797, '“to': 798, 'bear': 799, '2018': 800, 'papers': 801, 'blown': 802, 'completely': 803, 'changed': 804, 'hubs': 805, 'changer': 806, 'days': 807, 'posted': 808, 'anywhere': 809, 'needed': 810, 'murphy’s': 811, 'law': 812, 'users': 813, 'would': 814, 'vice': 815, 'versa': 816, 'did': 817, 'figuring': 818, 'supports': 819, 'configure': 820, 'q4': 821, 'five': 822, 'thousand': 823, 'installed': 824, 'pip': 825, 'million': 826, 'month': 827, 'moreover': 828, 'expanding': 829, 'numerous': 830, 'face—including': 831, 'creator': 832, '—and': 833, 'forewordshows': 834, 'breadth': 835, 'depth': 836, 'pages': 837, 'astounding': 838, 'covers': 839, 'entire': 840, 'around': 841, 'particularly': 842, 'appreciated': 843, 'follow': 844, 'along': 845, 'jupyter': 846, 'notebooks': 847, 'straight': 848, 'point': 849, 'authors': 850, 'extensive': 851, 'experience': 852, 'wealth': 853, 'tips': 854, 'tricks': 855, 'least': 856, 'writing': 857, 'direct': 858, 'lively': 859, 'reads': 860, 'thoroughly': 861, 'enjoyed': 862, 'i’m': 863, 'certain': 864, 'anyone': 865, 'interested': 866, 'packed': 867, 'brim': 868, 'hopefully': 869, 'excited': 870, 'integrating': 871, 'versatile': 872, '“tip': 873, 'iceberg': 874, 'chapters': 875, 'wide': 876, 'range': 877, 'lightweight': 878, 'accompanying': 879, 'run': 880, 'colab': 881, 'gpu': 882, 'we’re': 883, 'armed': 884, 'dirty': 885, 'topic': 886, 'combine': 887, 'character': 888, 'units': 889, 'deal': 890, 'misspellings': 891, 'frequent': 892, 'unique': 893, 'manageable': 894, 'distinguishing': 895, 'mix': 896, 'statistical': 897, 'rules': 898, 'tokenizers': 899, 'easiest': 900, 'action': 901, 'convenient': 902, 'load': 903, 'model—we': 904, 'call': 905, 'providing': 906, 'local': 907, 'file': 908, 'path': 909, 'loading': 910, 'observe': 911, 'things': 912, 'here': 913, 'differ': 914, 'role': 915, 'lowercased': 916, 'checkpoint': 917, '“tokenizing”': 918, '“nlp”': 919, 'sense': 920, 'izing': 921, 'p': 922, 'preceding': 923, 'whitespace': 924, 'merged': 925, 'doing': 926, 'plot': 927, 'clear': 928, 'feelings': 929, 'sadness': 930, 'anger': 931, 'fear': 932, 'occupy': 933, 'regions': 934, 'slightly': 935, 'varying': 936, 'distributions': 937, 'joy': 938, 'love': 939, 'separated': 940, 'space': 941, 'surprise': 942, 'scattered': 943, 'hoped': 944, 'separation': 945, 'implicitly': 946, 'guessing': 947, 'gained': 948, 'insight': 949, 'serve': 950, 'us': 951, 'avoid': 952, 'working': 953, 'decrease': 954, 'loss': 955, 'increase': 956, 'under': 957, 'hood': 958, 'guidance': 959, 'bits': 960, 'pieces': 961, 'brief': 962, 'differences': 963, 'modules': 964, 'technical': 965, 'generally': 966, 'helpful': 967, 'comprehending': 968, 'navigating': 969, 'limitations': 970, 'taxonomy': 971, 'emerged': 972, 'diving': 973, 'overview': 974, 'kick': 975, 'started': 976, 'encoder’s': 977, 'outputs': 978, 'repeat': 979, 'process': 980, 'until': 981, 'eos': 982, 'reached': 983, 'designed': 984, 'soon': 985, 'adapted': 986, 'standalone': 987, 'types': 988, 'rich': 989, 'numerical': 990, 'after': 991, 'bidirectional': 992, '“thanks': 993, 'lunch': 994, 'a…”': 995, 'autocomplete': 996, 'iteratively': 997, 'probable': 998, 'causal': 999, 'autoregressive': 1000, 'mappings': 1001, 'reality': 1002, 'distinction': 1003, 'blurry': 1004, 'primed': 1005, 'similarly': 1006, 'level': 1007, 'discussed': 1008, 'amount': 1009, '“attention”': 1010, 'element': 1011, 'elements': 1012, 'mapped': 1013, '768': 1014, '“self”': 1015, 'set—for': 1016, 'contrast': 1017, 'involves': 1018, 'computing': 1019, 'relevance': 1020, 'timestep': 1021, 'weighted': 1022, 'average': 1023, 'determine': 1024, 'relate': 1025, 'multiplication': 1026, 'queries': 1027, 'overlap': 1028, 'notion': 1029, 'cryptic': 1030, 'names': 1031, 'retrieval': 1032, 'systems': 1033, 'motivate': 1034, 'you’re': 1035, 'buying': 1036, 'dinner': 1037, 'dish’s': 1038, 'recipe': 1039, 'required': 1040, 'scan': 1041, 'shelves': 1042, 'labels': 1043, 'check': 1044, 'shelf': 1045, 'grocery': 1046, 'abstract': 1047, '“smooth”': 1048, 'version': 1049, 'includes': 1050, 'dozen': 1051, 'grabbing': 1052, '10': 1053, 'omelette': 1054, 'chicken': 1055, 'wing': 1056, 'created': 1057, '×5': 1058, 'sample': 1059, 'applying': 1060, 'wq': 1061, 'k': 1062, 'v': 1063, 'kept': 1064, 'simplicity': 1065, 'numbers': 1066, 'cause': 1067, 'saturate': 1068, 'performs': 1069, 'simplifies': 1070, 'computation': 1071, 'ignored': 1072, 'calculate': 1073, 'transposing': 1074, 'collect': 1075, 'batches': 1076, 'multiplies': 1077, 'corresponding': 1078, 'score': 1079, 'informed': 1080, 'complementary': 1081, 'words—for': 1082, 'defined': 1083, 'incorporating': 1084, '“time”': 1085, 'mention': 1086, 'promote': 1087, 'spaces': 1088, 'headed': 1089, '“as': 1090, 'is”': 1091, 'applies': 1092, 'projection': 1093, 'carries': 1094, 'selfattention': 1095, 'semantic': 1096, 'turns': 1097, 'beneficial': 1098, 'sets': 1099, 'illustrated': 1100, 'why': 1101, 'aspect': 1102, 'having': 1103, 'once': 1104, 'instance': 1105, 'subject': 1106, 'interaction': 1107, 'whereas': 1108, 'adjectives': 1109, 'obviously': 1110, 'handcraft': 1111, 'relations': 1112, 'familiar': 1113, 'resemblance': 1114, 'filters': 1115, 'convolutional': 1116, 'filter': 1117, 'responsible': 1118, 'detecting': 1119, 'faces': 1120, 'wheels': 1121, 'cars': 1122, 'images': 1123, 'shows': 1124, 'connecting': 1125, 'whose': 1126, 'attended': 1127, 'intensity': 1128, 'indicates': 1129, 'strength': 1130, 'dark': 1131, 'faint': 1132, '0': 1133, 'bert’s': 1134, 'thing': 1135, 'strongest': 1136, 'tell': 1137, 'attend': 1138, 'identified': 1139, '“fruit”': 1140, '“banana”': 1141, 'distinguish': 1142, 'noun': 1143, 'depending': 1144, 'occurs': 1145, 'implementing': 1146, 'piece': 1147, 'sublayer': 1148, 'connected': 1149, 'twist': 1150, 'processes': 1151, 'convolution': 1152, 'kernel': 1153, 'background': 1154, 'g': 1155, 'codebase': 1156, 'nomenclature': 1157, 'rule': 1158, 'thumb': 1159, 'gelu': 1160, 'activation': 1161, 'capacity': 1162, 'memorization': 1163, 'hypothesized': 1164, 'happen': 1165, 'nn': 1166, 'follows': 1167, 'mentioned': 1168, 'former': 1169, 'normalizes': 1170, 'zero': 1171, 'mean': 1172, 'unity': 1173, 'variance': 1174, 'processed': 1175, 'placing': 1176, 'choices': 1177, 'adopted': 1178, 'post': 1179, 'tricky': 1180, 'gradients': 1181, 'diverge': 1182, 'known': 1183, 'gradually': 1184, 'increased': 1185, 'pre': 1186, 'found': 1187, 'span': 1188, 'stable': 1189, 'require': 1190, 'augment': 1191, 'dependent': 1192, 'arranged': 1193, 'characteristic': 1194, 'incorporate': 1195, 'achieve': 1196, 'popular': 1197, 'approaches': 1198, 'sufficiently': 1199, 'exactly': 1200, 'index': 1201, 'encoding': 1202, 'custom': 1203, 'sum': 1204, 'format': 1205, 'easily': 1206, 'answer': 1207, 'taskspecific': 1208, 'design': 1209, 'wish': 1210, 'options': 1211, 'traditionally': 1212, 'dropout': 1213, 'extends': 1214, 'existing': 1215, 'shed': 1216, 'light': 1217, 'mysteries': 1218, 'sounds': 1219, 'incredibly': 1220, 'unfortunately': 1221, 'foreign': 1222, 'exchange': 1223, 'student': 1224, 'mother': 1225, 'tongue': 1226, 'cunning': 1227, 'students': 1228, 'cheat': 1229, 'anyway': 1230, 'illustrating': 1231, 'give': 1232, 'description': 1233, 'describing': 1234, 'ace': 1235, 'meet': 1236, 'encoders': 1237, 'triggered': 1238, 'cambrian': 1239, 'explosion': 1240, 'development': 1241, 'objectives': 1242, 'tweaked': 1243, 'improve': 1244, 'fast': 1245, 'parallel': 1246, 'tlm': 1247, 'dropped': 1248, 'beats': 1249, 'margin': 1250, '13': 1251, 'decouples': 1252, 'allowing': 1253, 'thereby': 1254, 'saving': 1255, 'vocabulary': 1256, 'decreases': 1257, 'nsp': 1258, 'replaced': 1259, 'ordering': 1260, 'order': 1261, 'consecutive': 1262, 'swapped': 1263, 'rather': 1264, 'fewer': 1265, 'reach': 1266, 'superior': 1267, 'address': 1268, '14': 1269, 'tasked': 1270, 'therefore': 1271, 'binary': 1272, '30': 1273, '15': 1274, 'disentangling': 1275, 'tokens’': 1276, 'dependency': 1277, 'spearheaded': 1278, 'exceptionally': 1279, 'details': 1280, 'fueled': 1281, 'sizes': 1282, 'evolution': 1283, 'fascinating': 1284, 'introduction': 1285, 'combined': 1286, 'ideas': 1287, '17': 1288, 'setup': 1289, 'bookcorpus': 1290, 'achieved': 1291, 'scalable': 1292, '18': 1293, 'coherent': 1294, 'concerns': 1295, 'about': 1296, 'misuse': 1297, 'released': 1298, 'staged': 1299, 'fashion': 1300, 'continue': 1301, 'user': 1302, 'control': 1303, 'conditional': 1304, 'adding': 1305, '“control': 1306, 'tokens”': 1307, 'beginning': 1308, '19': 1309, 'controlled': 1310, 'diverse': 1311, 'thorough': 1312, 'analysis': 1313, 'revealed': 1314, 'power': 1315, 'laws': 1316, 'govern': 1317, 'relation': 1318, '20': 1319, 'factor': 1320, 'yield': 1321, '21': 1322, 'impressively': 1323, 'realistic': 1324, 'passages': 1325, 'exhibits': 1326, 'shot': 1327, 'capabilities': 1328, 'translating': 1329, 'accomplish': 1330, 'sourced': 1331, 'interface': 1332, 'api': 1333, 'eleutherai': 1334, 'collective': 1335, 'who': 1336, 'aim': 1337, 're': 1338, 'release': 1339, '22': 1340, 'parameter': 1341, '7': 1342, 'offers': 1343, 'final': 1344, 'tree': 1345, 'life': 1346, 'unifies': 1347, 'converting': 1348, 'totext': 1349, '23': 1350, 'framed': 1351, 'adopting': 1352, 'problems': 1353, 'normal': 1354, 'detail': 1355, 'largest': 1356, '11': 1357, 'yielded': 1358, 'benchmarks': 1359, '24': 1360, 'undergo': 1361, 'masking': 1362, 'permutation': 1363, 'deletion': 1364, 'document': 1365, 'rotation': 1366, 'modified': 1367, 'passed': 1368, 'reconstruct': 1369, 'achieves': 1370, 'pair': 1371, 'direction': 1372, 'naturally': 1373, 'shared': 1374, 'leveraged': 1375, 'translate': 1376, '25': 1377, 'underrepresented': 1378, 'target': 1379, 'quadratic': 1380, 'memory': 1381, 'requirements': 1382, 'sparse': 1383, 'form': 1384, 'linearly': 1385, '26': 1386, 'drastic': 1387, '512': 1388, '096': 1389, 'dependencies': 1390, 'conserved': 1391, 'checkpoints': 1392, 'described': 1393, 'solve': 1394, 'corpora—but': 1395, 'greek': 1396, 'klingon': 1397, 'tend': 1398, '“high': 1399, 'resource”': 1400, 'russian': 1401, 'mandarin': 1402, 'plenty': 1403, 'webtext': 1404, 'challenge': 1405, 'arises': 1406, 'maintaining': 1407, 'fun': 1408, 'engineering': 1409, 'rescue': 1410, 'jointly': 1411, 'hundred': 1412, 'corpora': 1413, 'enable': 1414, 'zeroshot': 1415, '“code': 1416, 'switching': 1417, 'speaker': 1418, 'alternates': 1419, 'dialects': 1420, 'conversation': 1421, 'identifies': 1422, 'locations': 1423, 'gaining': 1424, 'company': 1425, 'augmenting': 1426, 'structured': 1427, 'database': 1428, 'involve': 1429, 'counterparts': 1430, 'except': 1431, 'remarkable': 1432, 'despite': 1433, 'receiving': 1434, 'explicit': 1435, 'differentiate': 1436, 'among': 1437, 'linguistic': 1438, 'generalize': 1439, 'ability': 1440, 'circumvents': 1441, '2002': 1442, '2003': 1443, 'dutch': 1444, 'spanish': 1445, 'annotated': 1446, 'loc': 1447, 'org': 1448, 'pan': 1449, 'x': 1450, 'misc': 1451, 'miscellaneous': 1452, 'groups': 1453, 'evaluated': 1454, 'testset': 1455, 'adopt': 1456, 'evaluation': 1457, 'strategy': 1458, 'select': 1459, 'adds': 1460, 'superseded': 1461, 'consider': 1462, 'recognitionas': 1463, 'distinguished': 1464, 'compared': 1465, 'predecessors': 1466, 'dumps': 1467, 'terabytes': 1468, 'crawl': 1469, 'web': 1470, 'orders': 1471, 'magnitude': 1472, 'significant': 1473, 'boost': 1474, 'signal': 1475, 'burmese': 1476, 'roberta’s': 1477, 'improved': 1478, 'removing': 1479, 'altogether': 1480, 'drops': 1481, 'sentencepiece': 1482, 'raw': 1483, 'directly': 1484, 'notable': 1485, 'respective': 1486, 'vocabularies': 1487, '250': 1488, '55': 1489, 'choice': 1490}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(text1)) :\n",
    "     tokenizer.fit_on_texts([text1[i]])\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1490"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer  = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_text = ''.join(text1)\n",
    "\n",
    "# Use regex to split into sentences, keeping the whitespace intact\n",
    "# sentences = re.split(r'(?<=[.!?])\\s+', combined_text)\n",
    "\n",
    "# Print each sentence\n",
    "# for sentence in sentences:\n",
    "#     print(sentence)\n",
    "  \n",
    "#     for sentence1 in sentence.split('\\n'):\n",
    "#         tokenized_sentence = tokenizer.texts_to_sequences([sentence1])[0]\n",
    "#         print(tokenized_sentence)\n",
    "\n",
    "#         for i in range(1,len(tokenized_sentence)):\n",
    "#             input_sequences.append(tokenized_sentence[:i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for i in range(len(text1)):\n",
    "    tokenized_sentence = tokenizer.texts_to_sequences([text1[i]])[0]\n",
    "    # print(tokenized_sentence\n",
    "    for i in range(1,len(tokenized_sentence)):\n",
    "        input_sequences.append(tokenized_sentence[:i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6011"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 443],\n",
       " [3, 443, 7],\n",
       " [3, 443, 7, 304],\n",
       " [3, 443, 7, 304, 305],\n",
       " [3, 443, 7, 304, 305, 11],\n",
       " [3, 443, 7, 304, 305, 11, 18],\n",
       " [3, 443, 7, 304, 305, 11, 18, 444],\n",
       " [3, 443, 7, 304, 305, 11, 18, 444, 26],\n",
       " [3, 443, 7, 304, 305, 11, 18, 444, 26, 139],\n",
       " [3, 443, 7, 304, 305, 11, 18, 444, 26, 139, 1],\n",
       " [3, 443, 7, 304, 305, 11, 18, 444, 26, 139, 1, 681],\n",
       " [3, 443, 7, 304, 305, 11, 18, 444, 26, 139, 1, 681, 16],\n",
       " [3, 443, 7, 304, 305, 11, 18, 444, 26, 139, 1, 681, 16, 10],\n",
       " [3, 443, 7, 304, 305, 11, 18, 444, 26, 139, 1, 681, 16, 10, 682],\n",
       " [3, 443, 7, 304, 305, 11, 18, 444, 26, 139, 1, 681, 16, 10, 682, 15],\n",
       " [3, 443, 7, 304, 305, 11, 18, 444, 26, 139, 1, 681, 16, 10, 682, 15, 683],\n",
       " [3, 443, 7, 304, 305, 11, 18, 444, 26, 139, 1, 681, 16, 10, 682, 15, 683, 59],\n",
       " [3,\n",
       "  443,\n",
       "  7,\n",
       "  304,\n",
       "  305,\n",
       "  11,\n",
       "  18,\n",
       "  444,\n",
       "  26,\n",
       "  139,\n",
       "  1,\n",
       "  681,\n",
       "  16,\n",
       "  10,\n",
       "  682,\n",
       "  15,\n",
       "  683,\n",
       "  59,\n",
       "  71],\n",
       " [3,\n",
       "  443,\n",
       "  7,\n",
       "  304,\n",
       "  305,\n",
       "  11,\n",
       "  18,\n",
       "  444,\n",
       "  26,\n",
       "  139,\n",
       "  1,\n",
       "  681,\n",
       "  16,\n",
       "  10,\n",
       "  682,\n",
       "  15,\n",
       "  683,\n",
       "  59,\n",
       "  71,\n",
       "  5],\n",
       " [3,\n",
       "  443,\n",
       "  7,\n",
       "  304,\n",
       "  305,\n",
       "  11,\n",
       "  18,\n",
       "  444,\n",
       "  26,\n",
       "  139,\n",
       "  1,\n",
       "  681,\n",
       "  16,\n",
       "  10,\n",
       "  682,\n",
       "  15,\n",
       "  683,\n",
       "  59,\n",
       "  71,\n",
       "  5,\n",
       "  445],\n",
       " [3,\n",
       "  443,\n",
       "  7,\n",
       "  304,\n",
       "  305,\n",
       "  11,\n",
       "  18,\n",
       "  444,\n",
       "  26,\n",
       "  139,\n",
       "  1,\n",
       "  681,\n",
       "  16,\n",
       "  10,\n",
       "  682,\n",
       "  15,\n",
       "  683,\n",
       "  59,\n",
       "  71,\n",
       "  5,\n",
       "  445,\n",
       "  5],\n",
       " [3,\n",
       "  443,\n",
       "  7,\n",
       "  304,\n",
       "  305,\n",
       "  11,\n",
       "  18,\n",
       "  444,\n",
       "  26,\n",
       "  139,\n",
       "  1,\n",
       "  681,\n",
       "  16,\n",
       "  10,\n",
       "  682,\n",
       "  15,\n",
       "  683,\n",
       "  59,\n",
       "  71,\n",
       "  5,\n",
       "  445,\n",
       "  5,\n",
       "  306],\n",
       " [3,\n",
       "  443,\n",
       "  7,\n",
       "  304,\n",
       "  305,\n",
       "  11,\n",
       "  18,\n",
       "  444,\n",
       "  26,\n",
       "  139,\n",
       "  1,\n",
       "  681,\n",
       "  16,\n",
       "  10,\n",
       "  682,\n",
       "  15,\n",
       "  683,\n",
       "  59,\n",
       "  71,\n",
       "  5,\n",
       "  445,\n",
       "  5,\n",
       "  306,\n",
       "  11],\n",
       " [3,\n",
       "  443,\n",
       "  7,\n",
       "  304,\n",
       "  305,\n",
       "  11,\n",
       "  18,\n",
       "  444,\n",
       "  26,\n",
       "  139,\n",
       "  1,\n",
       "  681,\n",
       "  16,\n",
       "  10,\n",
       "  682,\n",
       "  15,\n",
       "  683,\n",
       "  59,\n",
       "  71,\n",
       "  5,\n",
       "  445,\n",
       "  5,\n",
       "  306,\n",
       "  11,\n",
       "  60],\n",
       " [3,\n",
       "  443,\n",
       "  7,\n",
       "  304,\n",
       "  305,\n",
       "  11,\n",
       "  18,\n",
       "  444,\n",
       "  26,\n",
       "  139,\n",
       "  1,\n",
       "  681,\n",
       "  16,\n",
       "  10,\n",
       "  682,\n",
       "  15,\n",
       "  683,\n",
       "  59,\n",
       "  71,\n",
       "  5,\n",
       "  445,\n",
       "  5,\n",
       "  306,\n",
       "  11,\n",
       "  60,\n",
       "  684],\n",
       " [3,\n",
       "  443,\n",
       "  7,\n",
       "  304,\n",
       "  305,\n",
       "  11,\n",
       "  18,\n",
       "  444,\n",
       "  26,\n",
       "  139,\n",
       "  1,\n",
       "  681,\n",
       "  16,\n",
       "  10,\n",
       "  682,\n",
       "  15,\n",
       "  683,\n",
       "  59,\n",
       "  71,\n",
       "  5,\n",
       "  445,\n",
       "  5,\n",
       "  306,\n",
       "  11,\n",
       "  60,\n",
       "  684,\n",
       "  96],\n",
       " [3,\n",
       "  443,\n",
       "  7,\n",
       "  304,\n",
       "  305,\n",
       "  11,\n",
       "  18,\n",
       "  444,\n",
       "  26,\n",
       "  139,\n",
       "  1,\n",
       "  681,\n",
       "  16,\n",
       "  10,\n",
       "  682,\n",
       "  15,\n",
       "  683,\n",
       "  59,\n",
       "  71,\n",
       "  5,\n",
       "  445,\n",
       "  5,\n",
       "  306,\n",
       "  11,\n",
       "  60,\n",
       "  684,\n",
       "  96,\n",
       "  140],\n",
       " [3,\n",
       "  443,\n",
       "  7,\n",
       "  304,\n",
       "  305,\n",
       "  11,\n",
       "  18,\n",
       "  444,\n",
       "  26,\n",
       "  139,\n",
       "  1,\n",
       "  681,\n",
       "  16,\n",
       "  10,\n",
       "  682,\n",
       "  15,\n",
       "  683,\n",
       "  59,\n",
       "  71,\n",
       "  5,\n",
       "  445,\n",
       "  5,\n",
       "  306,\n",
       "  11,\n",
       "  60,\n",
       "  684,\n",
       "  96,\n",
       "  140,\n",
       "  307],\n",
       " [3,\n",
       "  443,\n",
       "  7,\n",
       "  304,\n",
       "  305,\n",
       "  11,\n",
       "  18,\n",
       "  444,\n",
       "  26,\n",
       "  139,\n",
       "  1,\n",
       "  681,\n",
       "  16,\n",
       "  10,\n",
       "  682,\n",
       "  15,\n",
       "  683,\n",
       "  59,\n",
       "  71,\n",
       "  5,\n",
       "  445,\n",
       "  5,\n",
       "  306,\n",
       "  11,\n",
       "  60,\n",
       "  684,\n",
       "  96,\n",
       "  140,\n",
       "  307,\n",
       "  36],\n",
       " [3,\n",
       "  443,\n",
       "  7,\n",
       "  304,\n",
       "  305,\n",
       "  11,\n",
       "  18,\n",
       "  444,\n",
       "  26,\n",
       "  139,\n",
       "  1,\n",
       "  681,\n",
       "  16,\n",
       "  10,\n",
       "  682,\n",
       "  15,\n",
       "  683,\n",
       "  59,\n",
       "  71,\n",
       "  5,\n",
       "  445,\n",
       "  5,\n",
       "  306,\n",
       "  11,\n",
       "  60,\n",
       "  684,\n",
       "  96,\n",
       "  140,\n",
       "  307,\n",
       "  36,\n",
       "  685],\n",
       " [686, 235],\n",
       " [686, 235, 22],\n",
       " [686, 235, 22, 687],\n",
       " [686, 235, 22, 687, 446],\n",
       " [686, 235, 22, 687, 446, 24],\n",
       " [686, 235, 22, 687, 446, 24, 110],\n",
       " [686, 235, 22, 687, 446, 24, 110, 688],\n",
       " [686, 235, 22, 687, 446, 24, 110, 688, 689],\n",
       " [686, 235, 22, 687, 446, 24, 110, 688, 689, 36],\n",
       " [686, 235, 22, 687, 446, 24, 110, 688, 689, 36, 159],\n",
       " [97, 60],\n",
       " [97, 60, 690],\n",
       " [97, 60, 690, 2],\n",
       " [97, 60, 690, 2, 691],\n",
       " [97, 60, 690, 2, 691, 36],\n",
       " [97, 60, 690, 2, 691, 36, 23],\n",
       " [97, 60, 690, 2, 691, 36, 23, 5],\n",
       " [97, 60, 690, 2, 691, 36, 23, 5, 692],\n",
       " [97, 60, 690, 2, 691, 36, 23, 5, 692, 236],\n",
       " [97, 60, 690, 2, 691, 36, 23, 5, 692, 236, 693],\n",
       " [97, 60, 690, 2, 691, 36, 23, 5, 692, 236, 693, 6],\n",
       " [97, 60, 690, 2, 691, 36, 23, 5, 692, 236, 693, 6, 10],\n",
       " [97, 60, 690, 2, 691, 36, 23, 5, 692, 236, 693, 6, 10, 694],\n",
       " [97, 60, 690, 2, 691, 36, 23, 5, 692, 236, 693, 6, 10, 694, 5],\n",
       " [97, 60, 690, 2, 691, 36, 23, 5, 692, 236, 693, 6, 10, 694, 5, 447],\n",
       " [97, 60, 690, 2, 691, 36, 23, 5, 692, 236, 693, 6, 10, 694, 5, 447, 308],\n",
       " [97, 60, 690, 2, 691, 36, 23, 5, 692, 236, 693, 6, 10, 694, 5, 447, 308, 695],\n",
       " [97,\n",
       "  60,\n",
       "  690,\n",
       "  2,\n",
       "  691,\n",
       "  36,\n",
       "  23,\n",
       "  5,\n",
       "  692,\n",
       "  236,\n",
       "  693,\n",
       "  6,\n",
       "  10,\n",
       "  694,\n",
       "  5,\n",
       "  447,\n",
       "  308,\n",
       "  695,\n",
       "  60],\n",
       " [97,\n",
       "  60,\n",
       "  690,\n",
       "  2,\n",
       "  691,\n",
       "  36,\n",
       "  23,\n",
       "  5,\n",
       "  692,\n",
       "  236,\n",
       "  693,\n",
       "  6,\n",
       "  10,\n",
       "  694,\n",
       "  5,\n",
       "  447,\n",
       "  308,\n",
       "  695,\n",
       "  60,\n",
       "  183],\n",
       " [97,\n",
       "  60,\n",
       "  690,\n",
       "  2,\n",
       "  691,\n",
       "  36,\n",
       "  23,\n",
       "  5,\n",
       "  692,\n",
       "  236,\n",
       "  693,\n",
       "  6,\n",
       "  10,\n",
       "  694,\n",
       "  5,\n",
       "  447,\n",
       "  308,\n",
       "  695,\n",
       "  60,\n",
       "  183,\n",
       "  24],\n",
       " [97,\n",
       "  60,\n",
       "  690,\n",
       "  2,\n",
       "  691,\n",
       "  36,\n",
       "  23,\n",
       "  5,\n",
       "  692,\n",
       "  236,\n",
       "  693,\n",
       "  6,\n",
       "  10,\n",
       "  694,\n",
       "  5,\n",
       "  447,\n",
       "  308,\n",
       "  695,\n",
       "  60,\n",
       "  183,\n",
       "  24,\n",
       "  3],\n",
       " [97,\n",
       "  60,\n",
       "  690,\n",
       "  2,\n",
       "  691,\n",
       "  36,\n",
       "  23,\n",
       "  5,\n",
       "  692,\n",
       "  236,\n",
       "  693,\n",
       "  6,\n",
       "  10,\n",
       "  694,\n",
       "  5,\n",
       "  447,\n",
       "  308,\n",
       "  695,\n",
       "  60,\n",
       "  183,\n",
       "  24,\n",
       "  3,\n",
       "  696],\n",
       " [97,\n",
       "  60,\n",
       "  690,\n",
       "  2,\n",
       "  691,\n",
       "  36,\n",
       "  23,\n",
       "  5,\n",
       "  692,\n",
       "  236,\n",
       "  693,\n",
       "  6,\n",
       "  10,\n",
       "  694,\n",
       "  5,\n",
       "  447,\n",
       "  308,\n",
       "  695,\n",
       "  60,\n",
       "  183,\n",
       "  24,\n",
       "  3,\n",
       "  696,\n",
       "  2],\n",
       " [97,\n",
       "  60,\n",
       "  690,\n",
       "  2,\n",
       "  691,\n",
       "  36,\n",
       "  23,\n",
       "  5,\n",
       "  692,\n",
       "  236,\n",
       "  693,\n",
       "  6,\n",
       "  10,\n",
       "  694,\n",
       "  5,\n",
       "  447,\n",
       "  308,\n",
       "  695,\n",
       "  60,\n",
       "  183,\n",
       "  24,\n",
       "  3,\n",
       "  696,\n",
       "  2,\n",
       "  697],\n",
       " [97,\n",
       "  60,\n",
       "  690,\n",
       "  2,\n",
       "  691,\n",
       "  36,\n",
       "  23,\n",
       "  5,\n",
       "  692,\n",
       "  236,\n",
       "  693,\n",
       "  6,\n",
       "  10,\n",
       "  694,\n",
       "  5,\n",
       "  447,\n",
       "  308,\n",
       "  695,\n",
       "  60,\n",
       "  183,\n",
       "  24,\n",
       "  3,\n",
       "  696,\n",
       "  2,\n",
       "  697,\n",
       "  448],\n",
       " [97,\n",
       "  60,\n",
       "  690,\n",
       "  2,\n",
       "  691,\n",
       "  36,\n",
       "  23,\n",
       "  5,\n",
       "  692,\n",
       "  236,\n",
       "  693,\n",
       "  6,\n",
       "  10,\n",
       "  694,\n",
       "  5,\n",
       "  447,\n",
       "  308,\n",
       "  695,\n",
       "  60,\n",
       "  183,\n",
       "  24,\n",
       "  3,\n",
       "  696,\n",
       "  2,\n",
       "  697,\n",
       "  448,\n",
       "  11],\n",
       " [97,\n",
       "  60,\n",
       "  690,\n",
       "  2,\n",
       "  691,\n",
       "  36,\n",
       "  23,\n",
       "  5,\n",
       "  692,\n",
       "  236,\n",
       "  693,\n",
       "  6,\n",
       "  10,\n",
       "  694,\n",
       "  5,\n",
       "  447,\n",
       "  308,\n",
       "  695,\n",
       "  60,\n",
       "  183,\n",
       "  24,\n",
       "  3,\n",
       "  696,\n",
       "  2,\n",
       "  697,\n",
       "  448,\n",
       "  11,\n",
       "  18],\n",
       " [97,\n",
       "  60,\n",
       "  690,\n",
       "  2,\n",
       "  691,\n",
       "  36,\n",
       "  23,\n",
       "  5,\n",
       "  692,\n",
       "  236,\n",
       "  693,\n",
       "  6,\n",
       "  10,\n",
       "  694,\n",
       "  5,\n",
       "  447,\n",
       "  308,\n",
       "  695,\n",
       "  60,\n",
       "  183,\n",
       "  24,\n",
       "  3,\n",
       "  696,\n",
       "  2,\n",
       "  697,\n",
       "  448,\n",
       "  11,\n",
       "  18,\n",
       "  237],\n",
       " [97,\n",
       "  60,\n",
       "  690,\n",
       "  2,\n",
       "  691,\n",
       "  36,\n",
       "  23,\n",
       "  5,\n",
       "  692,\n",
       "  236,\n",
       "  693,\n",
       "  6,\n",
       "  10,\n",
       "  694,\n",
       "  5,\n",
       "  447,\n",
       "  308,\n",
       "  695,\n",
       "  60,\n",
       "  183,\n",
       "  24,\n",
       "  3,\n",
       "  696,\n",
       "  2,\n",
       "  697,\n",
       "  448,\n",
       "  11,\n",
       "  18,\n",
       "  237,\n",
       "  26],\n",
       " [97,\n",
       "  60,\n",
       "  690,\n",
       "  2,\n",
       "  691,\n",
       "  36,\n",
       "  23,\n",
       "  5,\n",
       "  692,\n",
       "  236,\n",
       "  693,\n",
       "  6,\n",
       "  10,\n",
       "  694,\n",
       "  5,\n",
       "  447,\n",
       "  308,\n",
       "  695,\n",
       "  60,\n",
       "  183,\n",
       "  24,\n",
       "  3,\n",
       "  696,\n",
       "  2,\n",
       "  697,\n",
       "  448,\n",
       "  11,\n",
       "  18,\n",
       "  237,\n",
       "  26,\n",
       "  235],\n",
       " [97,\n",
       "  60,\n",
       "  690,\n",
       "  2,\n",
       "  691,\n",
       "  36,\n",
       "  23,\n",
       "  5,\n",
       "  692,\n",
       "  236,\n",
       "  693,\n",
       "  6,\n",
       "  10,\n",
       "  694,\n",
       "  5,\n",
       "  447,\n",
       "  308,\n",
       "  695,\n",
       "  60,\n",
       "  183,\n",
       "  24,\n",
       "  3,\n",
       "  696,\n",
       "  2,\n",
       "  697,\n",
       "  448,\n",
       "  11,\n",
       "  18,\n",
       "  237,\n",
       "  26,\n",
       "  235,\n",
       "  19],\n",
       " [97,\n",
       "  60,\n",
       "  690,\n",
       "  2,\n",
       "  691,\n",
       "  36,\n",
       "  23,\n",
       "  5,\n",
       "  692,\n",
       "  236,\n",
       "  693,\n",
       "  6,\n",
       "  10,\n",
       "  694,\n",
       "  5,\n",
       "  447,\n",
       "  308,\n",
       "  695,\n",
       "  60,\n",
       "  183,\n",
       "  24,\n",
       "  3,\n",
       "  696,\n",
       "  2,\n",
       "  697,\n",
       "  448,\n",
       "  11,\n",
       "  18,\n",
       "  237,\n",
       "  26,\n",
       "  235,\n",
       "  19,\n",
       "  449],\n",
       " [698, 2],\n",
       " [698, 2, 33],\n",
       " [698, 2, 33, 235],\n",
       " [698, 2, 33, 235, 24],\n",
       " [698, 2, 33, 235, 24, 450],\n",
       " [698, 2, 33, 235, 24, 450, 699],\n",
       " [698, 2, 33, 235, 24, 450, 699, 5],\n",
       " [698, 2, 33, 235, 24, 450, 699, 5, 447],\n",
       " [698, 2, 33, 235, 24, 450, 699, 5, 447, 700],\n",
       " [698, 2, 33, 235, 24, 450, 699, 5, 447, 700, 159],\n",
       " [698, 2, 33, 235, 24, 450, 699, 5, 447, 700, 159, 701],\n",
       " [698, 2, 33, 235, 24, 450, 699, 5, 447, 700, 159, 701, 238],\n",
       " [698, 2, 33, 235, 24, 450, 699, 5, 447, 700, 159, 701, 238, 702],\n",
       " [698, 2, 33, 235, 24, 450, 699, 5, 447, 700, 159, 701, 238, 702, 7],\n",
       " [698, 2, 33, 235, 24, 450, 699, 5, 447, 700, 159, 701, 238, 702, 7, 703],\n",
       " [451, 72],\n",
       " [451, 72, 159],\n",
       " [451, 72, 159, 239],\n",
       " [451, 72, 159, 239, 15],\n",
       " [451, 72, 159, 239, 15, 704],\n",
       " [451, 72, 159, 239, 15, 704, 126],\n",
       " [451, 72, 159, 239, 15, 704, 126, 5],\n",
       " [451, 72, 159, 239, 15, 704, 126, 5, 3],\n",
       " [451, 72, 159, 239, 15, 704, 126, 5, 3, 160],\n",
       " [451, 72, 159, 239, 15, 704, 126, 5, 3, 160, 15],\n",
       " [451, 72, 159, 239, 15, 704, 126, 5, 3, 160, 15, 705],\n",
       " [451, 72, 159, 239, 15, 704, 126, 5, 3, 160, 15, 705, 452],\n",
       " [6, 240],\n",
       " [6, 240, 706],\n",
       " [6, 240, 706, 159],\n",
       " [6, 240, 706, 159, 239],\n",
       " [6, 240, 706, 159, 239, 707],\n",
       " [6, 240, 706, 159, 239, 707, 73],\n",
       " [6, 240, 706, 159, 239, 707, 73, 4],\n",
       " [6, 240, 706, 159, 239, 707, 73, 4, 98],\n",
       " [6, 240, 706, 159, 239, 707, 73, 4, 98, 72],\n",
       " [6, 240, 706, 159, 239, 707, 73, 4, 98, 72, 708],\n",
       " [6, 240, 706, 159, 239, 707, 73, 4, 98, 72, 708, 709],\n",
       " [6, 240, 706, 159, 239, 707, 73, 4, 98, 72, 708, 709, 453],\n",
       " [6, 240, 706, 159, 239, 707, 73, 4, 98, 72, 708, 709, 453, 5],\n",
       " [6, 240, 706, 159, 239, 707, 73, 4, 98, 72, 708, 709, 453, 5, 710],\n",
       " [141, 11],\n",
       " [141, 11, 14],\n",
       " [141, 11, 14, 711],\n",
       " [141, 11, 14, 711, 712],\n",
       " [141, 11, 14, 711, 712, 454],\n",
       " [141, 11, 14, 711, 712, 454, 184],\n",
       " [141, 11, 14, 711, 712, 454, 184, 455],\n",
       " [141, 11, 14, 711, 712, 454, 184, 455, 713],\n",
       " [141, 11, 14, 711, 712, 454, 184, 455, 713, 714],\n",
       " [141, 11, 14, 711, 712, 454, 184, 455, 713, 714, 14],\n",
       " [141, 11, 14, 711, 712, 454, 184, 455, 713, 714, 14, 715],\n",
       " [141, 11, 14, 711, 712, 454, 184, 455, 713, 714, 14, 715, 716],\n",
       " [141, 11, 14, 711, 712, 454, 184, 455, 713, 714, 14, 715, 716, 454],\n",
       " [141, 11, 14, 711, 712, 454, 184, 455, 713, 714, 14, 715, 716, 454, 184],\n",
       " [141, 11, 14, 711, 712, 454, 184, 455, 713, 714, 14, 715, 716, 454, 184, 455],\n",
       " [141,\n",
       "  11,\n",
       "  14,\n",
       "  711,\n",
       "  712,\n",
       "  454,\n",
       "  184,\n",
       "  455,\n",
       "  713,\n",
       "  714,\n",
       "  14,\n",
       "  715,\n",
       "  716,\n",
       "  454,\n",
       "  184,\n",
       "  455,\n",
       "  159],\n",
       " [141,\n",
       "  11,\n",
       "  14,\n",
       "  711,\n",
       "  712,\n",
       "  454,\n",
       "  184,\n",
       "  455,\n",
       "  713,\n",
       "  714,\n",
       "  14,\n",
       "  715,\n",
       "  716,\n",
       "  454,\n",
       "  184,\n",
       "  455,\n",
       "  159,\n",
       "  239],\n",
       " [72, 4],\n",
       " [72, 4, 36],\n",
       " [72, 4, 36, 235],\n",
       " [72, 4, 36, 235, 15],\n",
       " [72, 4, 36, 235, 15, 57],\n",
       " [72, 4, 36, 235, 15, 57, 717],\n",
       " [72, 4, 36, 235, 15, 57, 717, 718],\n",
       " [72, 4, 36, 235, 15, 57, 717, 718, 60],\n",
       " [72, 4, 36, 235, 15, 57, 717, 718, 60, 719],\n",
       " [72, 4, 36, 235, 15, 57, 717, 718, 60, 719, 5],\n",
       " [72, 4, 36, 235, 15, 57, 717, 718, 60, 719, 5, 720],\n",
       " [72, 4, 36, 235, 15, 57, 717, 718, 60, 719, 5, 720, 5],\n",
       " [72, 4, 36, 235, 15, 57, 717, 718, 60, 719, 5, 720, 5, 721],\n",
       " [72, 4, 36, 235, 15, 57, 717, 718, 60, 719, 5, 720, 5, 721, 6],\n",
       " [72, 4, 36, 235, 15, 57, 717, 718, 60, 719, 5, 720, 5, 721, 6, 142],\n",
       " [72, 4, 36, 235, 15, 57, 717, 718, 60, 719, 5, 720, 5, 721, 6, 142, 143],\n",
       " [72, 4, 36, 235, 15, 57, 717, 718, 60, 719, 5, 720, 5, 721, 6, 142, 143, 722],\n",
       " [72,\n",
       "  4,\n",
       "  36,\n",
       "  235,\n",
       "  15,\n",
       "  57,\n",
       "  717,\n",
       "  718,\n",
       "  60,\n",
       "  719,\n",
       "  5,\n",
       "  720,\n",
       "  5,\n",
       "  721,\n",
       "  6,\n",
       "  142,\n",
       "  143,\n",
       "  722,\n",
       "  241],\n",
       " [72,\n",
       "  4,\n",
       "  36,\n",
       "  235,\n",
       "  15,\n",
       "  57,\n",
       "  717,\n",
       "  718,\n",
       "  60,\n",
       "  719,\n",
       "  5,\n",
       "  720,\n",
       "  5,\n",
       "  721,\n",
       "  6,\n",
       "  142,\n",
       "  143,\n",
       "  722,\n",
       "  241,\n",
       "  60],\n",
       " [72,\n",
       "  4,\n",
       "  36,\n",
       "  235,\n",
       "  15,\n",
       "  57,\n",
       "  717,\n",
       "  718,\n",
       "  60,\n",
       "  719,\n",
       "  5,\n",
       "  720,\n",
       "  5,\n",
       "  721,\n",
       "  6,\n",
       "  142,\n",
       "  143,\n",
       "  722,\n",
       "  241,\n",
       "  60,\n",
       "  723],\n",
       " [72,\n",
       "  4,\n",
       "  36,\n",
       "  235,\n",
       "  15,\n",
       "  57,\n",
       "  717,\n",
       "  718,\n",
       "  60,\n",
       "  719,\n",
       "  5,\n",
       "  720,\n",
       "  5,\n",
       "  721,\n",
       "  6,\n",
       "  142,\n",
       "  143,\n",
       "  722,\n",
       "  241,\n",
       "  60,\n",
       "  723,\n",
       "  18],\n",
       " [61, 97],\n",
       " [61, 97, 14],\n",
       " [61, 97, 14, 242],\n",
       " [61, 97, 14, 242, 2],\n",
       " [61, 97, 14, 242, 2, 243],\n",
       " [61, 97, 14, 242, 2, 243, 724],\n",
       " [61, 97, 14, 242, 2, 243, 724, 725],\n",
       " [61, 97, 14, 242, 2, 243, 724, 725, 14],\n",
       " [61, 97, 14, 242, 2, 243, 724, 725, 14, 46],\n",
       " [61, 97, 14, 242, 2, 243, 724, 725, 14, 46, 111],\n",
       " [61, 97, 14, 242, 2, 243, 724, 725, 14, 46, 111, 2],\n",
       " [61, 97, 14, 242, 2, 243, 724, 725, 14, 46, 111, 2, 185],\n",
       " [61, 97, 14, 242, 2, 243, 724, 725, 14, 46, 111, 2, 185, 3],\n",
       " [61, 97, 14, 242, 2, 243, 724, 725, 14, 46, 111, 2, 185, 3, 140],\n",
       " [61, 97, 14, 242, 2, 243, 724, 725, 14, 46, 111, 2, 185, 3, 140, 2],\n",
       " [61, 97, 14, 242, 2, 243, 724, 725, 14, 46, 111, 2, 185, 3, 140, 2, 726],\n",
       " [61, 97, 14, 242, 2, 243, 724, 725, 14, 46, 111, 2, 185, 3, 140, 2, 726, 112],\n",
       " [61,\n",
       "  97,\n",
       "  14,\n",
       "  242,\n",
       "  2,\n",
       "  243,\n",
       "  724,\n",
       "  725,\n",
       "  14,\n",
       "  46,\n",
       "  111,\n",
       "  2,\n",
       "  185,\n",
       "  3,\n",
       "  140,\n",
       "  2,\n",
       "  726,\n",
       "  112,\n",
       "  244],\n",
       " [1, 456],\n",
       " [1, 456, 457],\n",
       " [1, 456, 457, 7],\n",
       " [1, 456, 457, 7, 9],\n",
       " [1, 456, 457, 7, 9, 161],\n",
       " [1, 456, 457, 7, 9, 161, 443],\n",
       " [1, 456, 457, 7, 9, 161, 443, 62],\n",
       " [1, 456, 457, 7, 9, 161, 443, 62, 113],\n",
       " [1, 456, 457, 7, 9, 161, 443, 62, 113, 727],\n",
       " [1, 456, 457, 7, 9, 161, 443, 62, 113, 727, 162],\n",
       " [1, 456, 457, 7, 9, 161, 443, 62, 113, 727, 162, 1],\n",
       " [1, 456, 457, 7, 9, 161, 443, 62, 113, 727, 162, 1, 458],\n",
       " [1, 456, 457, 7, 9, 161, 443, 62, 113, 727, 162, 1, 458, 160],\n",
       " [1, 456, 457, 7, 9, 161, 443, 62, 113, 727, 162, 1, 458, 160, 309],\n",
       " [1, 456, 457, 7, 9, 161, 443, 62, 113, 727, 162, 1, 458, 160, 309, 74],\n",
       " [1, 456, 457, 7, 9, 161, 443, 62, 113, 727, 162, 1, 458, 160, 309, 74, 728],\n",
       " [1,\n",
       "  456,\n",
       "  457,\n",
       "  7,\n",
       "  9,\n",
       "  161,\n",
       "  443,\n",
       "  62,\n",
       "  113,\n",
       "  727,\n",
       "  162,\n",
       "  1,\n",
       "  458,\n",
       "  160,\n",
       "  309,\n",
       "  74,\n",
       "  728,\n",
       "  6],\n",
       " [1,\n",
       "  456,\n",
       "  457,\n",
       "  7,\n",
       "  9,\n",
       "  161,\n",
       "  443,\n",
       "  62,\n",
       "  113,\n",
       "  727,\n",
       "  162,\n",
       "  1,\n",
       "  458,\n",
       "  160,\n",
       "  309,\n",
       "  74,\n",
       "  728,\n",
       "  6,\n",
       "  459],\n",
       " [1,\n",
       "  456,\n",
       "  457,\n",
       "  7,\n",
       "  9,\n",
       "  161,\n",
       "  443,\n",
       "  62,\n",
       "  113,\n",
       "  727,\n",
       "  162,\n",
       "  1,\n",
       "  458,\n",
       "  160,\n",
       "  309,\n",
       "  74,\n",
       "  728,\n",
       "  6,\n",
       "  459,\n",
       "  163],\n",
       " [1,\n",
       "  456,\n",
       "  457,\n",
       "  7,\n",
       "  9,\n",
       "  161,\n",
       "  443,\n",
       "  62,\n",
       "  113,\n",
       "  727,\n",
       "  162,\n",
       "  1,\n",
       "  458,\n",
       "  160,\n",
       "  309,\n",
       "  74,\n",
       "  728,\n",
       "  6,\n",
       "  459,\n",
       "  163,\n",
       "  24],\n",
       " [1,\n",
       "  456,\n",
       "  457,\n",
       "  7,\n",
       "  9,\n",
       "  161,\n",
       "  443,\n",
       "  62,\n",
       "  113,\n",
       "  727,\n",
       "  162,\n",
       "  1,\n",
       "  458,\n",
       "  160,\n",
       "  309,\n",
       "  74,\n",
       "  728,\n",
       "  6,\n",
       "  459,\n",
       "  163,\n",
       "  24,\n",
       "  164],\n",
       " [1,\n",
       "  456,\n",
       "  457,\n",
       "  7,\n",
       "  9,\n",
       "  161,\n",
       "  443,\n",
       "  62,\n",
       "  113,\n",
       "  727,\n",
       "  162,\n",
       "  1,\n",
       "  458,\n",
       "  160,\n",
       "  309,\n",
       "  74,\n",
       "  728,\n",
       "  6,\n",
       "  459,\n",
       "  163,\n",
       "  24,\n",
       "  164,\n",
       "  729],\n",
       " [1,\n",
       "  456,\n",
       "  457,\n",
       "  7,\n",
       "  9,\n",
       "  161,\n",
       "  443,\n",
       "  62,\n",
       "  113,\n",
       "  727,\n",
       "  162,\n",
       "  1,\n",
       "  458,\n",
       "  160,\n",
       "  309,\n",
       "  74,\n",
       "  728,\n",
       "  6,\n",
       "  459,\n",
       "  163,\n",
       "  24,\n",
       "  164,\n",
       "  729,\n",
       "  2],\n",
       " [1,\n",
       "  456,\n",
       "  457,\n",
       "  7,\n",
       "  9,\n",
       "  161,\n",
       "  443,\n",
       "  62,\n",
       "  113,\n",
       "  727,\n",
       "  162,\n",
       "  1,\n",
       "  458,\n",
       "  160,\n",
       "  309,\n",
       "  74,\n",
       "  728,\n",
       "  6,\n",
       "  459,\n",
       "  163,\n",
       "  24,\n",
       "  164,\n",
       "  729,\n",
       "  2,\n",
       "  730],\n",
       " [1,\n",
       "  456,\n",
       "  457,\n",
       "  7,\n",
       "  9,\n",
       "  161,\n",
       "  443,\n",
       "  62,\n",
       "  113,\n",
       "  727,\n",
       "  162,\n",
       "  1,\n",
       "  458,\n",
       "  160,\n",
       "  309,\n",
       "  74,\n",
       "  728,\n",
       "  6,\n",
       "  459,\n",
       "  163,\n",
       "  24,\n",
       "  164,\n",
       "  729,\n",
       "  2,\n",
       "  730,\n",
       "  33],\n",
       " [1,\n",
       "  456,\n",
       "  457,\n",
       "  7,\n",
       "  9,\n",
       "  161,\n",
       "  443,\n",
       "  62,\n",
       "  113,\n",
       "  727,\n",
       "  162,\n",
       "  1,\n",
       "  458,\n",
       "  160,\n",
       "  309,\n",
       "  74,\n",
       "  728,\n",
       "  6,\n",
       "  459,\n",
       "  163,\n",
       "  24,\n",
       "  164,\n",
       "  729,\n",
       "  2,\n",
       "  730,\n",
       "  33,\n",
       "  12],\n",
       " [144, 18],\n",
       " [144, 18, 15],\n",
       " [144, 18, 15, 731],\n",
       " [144, 18, 15, 731, 10],\n",
       " [144, 18, 15, 731, 10, 186],\n",
       " [144, 18, 15, 731, 10, 186, 18],\n",
       " [144, 18, 15, 731, 10, 186, 18, 24],\n",
       " [144, 18, 15, 731, 10, 186, 18, 24, 460],\n",
       " [144, 18, 15, 731, 10, 186, 18, 24, 460, 187],\n",
       " [144, 18, 15, 731, 10, 186, 18, 24, 460, 187, 114],\n",
       " [144, 18, 15, 731, 10, 186, 18, 24, 460, 187, 114, 732],\n",
       " [144, 18, 15, 731, 10, 186, 18, 24, 460, 187, 114, 732, 733],\n",
       " [144, 18, 15, 731, 10, 186, 18, 24, 460, 187, 114, 732, 733, 4],\n",
       " [144, 18, 15, 731, 10, 186, 18, 24, 460, 187, 114, 732, 733, 4, 26],\n",
       " [144, 18, 15, 731, 10, 186, 18, 24, 460, 187, 114, 732, 733, 4, 26, 33],\n",
       " [144, 18, 15, 731, 10, 186, 18, 24, 460, 187, 114, 732, 733, 4, 26, 33, 12],\n",
       " [144,\n",
       "  18,\n",
       "  15,\n",
       "  731,\n",
       "  10,\n",
       "  186,\n",
       "  18,\n",
       "  24,\n",
       "  460,\n",
       "  187,\n",
       "  114,\n",
       "  732,\n",
       "  733,\n",
       "  4,\n",
       "  26,\n",
       "  33,\n",
       "  12,\n",
       "  99],\n",
       " [144,\n",
       "  18,\n",
       "  15,\n",
       "  731,\n",
       "  10,\n",
       "  186,\n",
       "  18,\n",
       "  24,\n",
       "  460,\n",
       "  187,\n",
       "  114,\n",
       "  732,\n",
       "  733,\n",
       "  4,\n",
       "  26,\n",
       "  33,\n",
       "  12,\n",
       "  99,\n",
       "  11],\n",
       " [144,\n",
       "  18,\n",
       "  15,\n",
       "  731,\n",
       "  10,\n",
       "  186,\n",
       "  18,\n",
       "  24,\n",
       "  460,\n",
       "  187,\n",
       "  114,\n",
       "  732,\n",
       "  733,\n",
       "  4,\n",
       "  26,\n",
       "  33,\n",
       "  12,\n",
       "  99,\n",
       "  11,\n",
       "  38],\n",
       " [144,\n",
       "  18,\n",
       "  15,\n",
       "  731,\n",
       "  10,\n",
       "  186,\n",
       "  18,\n",
       "  24,\n",
       "  460,\n",
       "  187,\n",
       "  114,\n",
       "  732,\n",
       "  733,\n",
       "  4,\n",
       "  26,\n",
       "  33,\n",
       "  12,\n",
       "  99,\n",
       "  11,\n",
       "  38,\n",
       "  115],\n",
       " [144,\n",
       "  18,\n",
       "  15,\n",
       "  731,\n",
       "  10,\n",
       "  186,\n",
       "  18,\n",
       "  24,\n",
       "  460,\n",
       "  187,\n",
       "  114,\n",
       "  732,\n",
       "  733,\n",
       "  4,\n",
       "  26,\n",
       "  33,\n",
       "  12,\n",
       "  99,\n",
       "  11,\n",
       "  38,\n",
       "  115,\n",
       "  42],\n",
       " [144,\n",
       "  18,\n",
       "  15,\n",
       "  731,\n",
       "  10,\n",
       "  186,\n",
       "  18,\n",
       "  24,\n",
       "  460,\n",
       "  187,\n",
       "  114,\n",
       "  732,\n",
       "  733,\n",
       "  4,\n",
       "  26,\n",
       "  33,\n",
       "  12,\n",
       "  99,\n",
       "  11,\n",
       "  38,\n",
       "  115,\n",
       "  42,\n",
       "  164],\n",
       " [144,\n",
       "  18,\n",
       "  15,\n",
       "  731,\n",
       "  10,\n",
       "  186,\n",
       "  18,\n",
       "  24,\n",
       "  460,\n",
       "  187,\n",
       "  114,\n",
       "  732,\n",
       "  733,\n",
       "  4,\n",
       "  26,\n",
       "  33,\n",
       "  12,\n",
       "  99,\n",
       "  11,\n",
       "  38,\n",
       "  115,\n",
       "  42,\n",
       "  164,\n",
       "  3],\n",
       " [144,\n",
       "  18,\n",
       "  15,\n",
       "  731,\n",
       "  10,\n",
       "  186,\n",
       "  18,\n",
       "  24,\n",
       "  460,\n",
       "  187,\n",
       "  114,\n",
       "  732,\n",
       "  733,\n",
       "  4,\n",
       "  26,\n",
       "  33,\n",
       "  12,\n",
       "  99,\n",
       "  11,\n",
       "  38,\n",
       "  115,\n",
       "  42,\n",
       "  164,\n",
       "  3,\n",
       "  310],\n",
       " [144,\n",
       "  18,\n",
       "  15,\n",
       "  731,\n",
       "  10,\n",
       "  186,\n",
       "  18,\n",
       "  24,\n",
       "  460,\n",
       "  187,\n",
       "  114,\n",
       "  732,\n",
       "  733,\n",
       "  4,\n",
       "  26,\n",
       "  33,\n",
       "  12,\n",
       "  99,\n",
       "  11,\n",
       "  38,\n",
       "  115,\n",
       "  42,\n",
       "  164,\n",
       "  3,\n",
       "  310,\n",
       "  311],\n",
       " [144,\n",
       "  18,\n",
       "  15,\n",
       "  731,\n",
       "  10,\n",
       "  186,\n",
       "  18,\n",
       "  24,\n",
       "  460,\n",
       "  187,\n",
       "  114,\n",
       "  732,\n",
       "  733,\n",
       "  4,\n",
       "  26,\n",
       "  33,\n",
       "  12,\n",
       "  99,\n",
       "  11,\n",
       "  38,\n",
       "  115,\n",
       "  42,\n",
       "  164,\n",
       "  3,\n",
       "  310,\n",
       "  311,\n",
       "  99],\n",
       " [144,\n",
       "  18,\n",
       "  15,\n",
       "  731,\n",
       "  10,\n",
       "  186,\n",
       "  18,\n",
       "  24,\n",
       "  460,\n",
       "  187,\n",
       "  114,\n",
       "  732,\n",
       "  733,\n",
       "  4,\n",
       "  26,\n",
       "  33,\n",
       "  12,\n",
       "  99,\n",
       "  11,\n",
       "  38,\n",
       "  115,\n",
       "  42,\n",
       "  164,\n",
       "  3,\n",
       "  310,\n",
       "  311,\n",
       "  99,\n",
       "  11],\n",
       " [144,\n",
       "  18,\n",
       "  15,\n",
       "  731,\n",
       "  10,\n",
       "  186,\n",
       "  18,\n",
       "  24,\n",
       "  460,\n",
       "  187,\n",
       "  114,\n",
       "  732,\n",
       "  733,\n",
       "  4,\n",
       "  26,\n",
       "  33,\n",
       "  12,\n",
       "  99,\n",
       "  11,\n",
       "  38,\n",
       "  115,\n",
       "  42,\n",
       "  164,\n",
       "  3,\n",
       "  310,\n",
       "  311,\n",
       "  99,\n",
       "  11,\n",
       "  734],\n",
       " [144,\n",
       "  18,\n",
       "  15,\n",
       "  731,\n",
       "  10,\n",
       "  186,\n",
       "  18,\n",
       "  24,\n",
       "  460,\n",
       "  187,\n",
       "  114,\n",
       "  732,\n",
       "  733,\n",
       "  4,\n",
       "  26,\n",
       "  33,\n",
       "  12,\n",
       "  99,\n",
       "  11,\n",
       "  38,\n",
       "  115,\n",
       "  42,\n",
       "  164,\n",
       "  3,\n",
       "  310,\n",
       "  311,\n",
       "  99,\n",
       "  11,\n",
       "  734,\n",
       "  735],\n",
       " [144,\n",
       "  18,\n",
       "  15,\n",
       "  731,\n",
       "  10,\n",
       "  186,\n",
       "  18,\n",
       "  24,\n",
       "  460,\n",
       "  187,\n",
       "  114,\n",
       "  732,\n",
       "  733,\n",
       "  4,\n",
       "  26,\n",
       "  33,\n",
       "  12,\n",
       "  99,\n",
       "  11,\n",
       "  38,\n",
       "  115,\n",
       "  42,\n",
       "  164,\n",
       "  3,\n",
       "  310,\n",
       "  311,\n",
       "  99,\n",
       "  11,\n",
       "  734,\n",
       "  735,\n",
       "  736],\n",
       " [144,\n",
       "  18,\n",
       "  15,\n",
       "  731,\n",
       "  10,\n",
       "  186,\n",
       "  18,\n",
       "  24,\n",
       "  460,\n",
       "  187,\n",
       "  114,\n",
       "  732,\n",
       "  733,\n",
       "  4,\n",
       "  26,\n",
       "  33,\n",
       "  12,\n",
       "  99,\n",
       "  11,\n",
       "  38,\n",
       "  115,\n",
       "  42,\n",
       "  164,\n",
       "  3,\n",
       "  310,\n",
       "  311,\n",
       "  99,\n",
       "  11,\n",
       "  734,\n",
       "  735,\n",
       "  736,\n",
       "  3],\n",
       " [144,\n",
       "  18,\n",
       "  15,\n",
       "  731,\n",
       "  10,\n",
       "  186,\n",
       "  18,\n",
       "  24,\n",
       "  460,\n",
       "  187,\n",
       "  114,\n",
       "  732,\n",
       "  733,\n",
       "  4,\n",
       "  26,\n",
       "  33,\n",
       "  12,\n",
       "  99,\n",
       "  11,\n",
       "  38,\n",
       "  115,\n",
       "  42,\n",
       "  164,\n",
       "  3,\n",
       "  310,\n",
       "  311,\n",
       "  99,\n",
       "  11,\n",
       "  734,\n",
       "  735,\n",
       "  736,\n",
       "  3,\n",
       "  737],\n",
       " [144,\n",
       "  18,\n",
       "  15,\n",
       "  731,\n",
       "  10,\n",
       "  186,\n",
       "  18,\n",
       "  24,\n",
       "  460,\n",
       "  187,\n",
       "  114,\n",
       "  732,\n",
       "  733,\n",
       "  4,\n",
       "  26,\n",
       "  33,\n",
       "  12,\n",
       "  99,\n",
       "  11,\n",
       "  38,\n",
       "  115,\n",
       "  42,\n",
       "  164,\n",
       "  3,\n",
       "  310,\n",
       "  311,\n",
       "  99,\n",
       "  11,\n",
       "  734,\n",
       "  735,\n",
       "  736,\n",
       "  3,\n",
       "  737,\n",
       "  20],\n",
       " [144,\n",
       "  18,\n",
       "  15,\n",
       "  731,\n",
       "  10,\n",
       "  186,\n",
       "  18,\n",
       "  24,\n",
       "  460,\n",
       "  187,\n",
       "  114,\n",
       "  732,\n",
       "  733,\n",
       "  4,\n",
       "  26,\n",
       "  33,\n",
       "  12,\n",
       "  99,\n",
       "  11,\n",
       "  38,\n",
       "  115,\n",
       "  42,\n",
       "  164,\n",
       "  3,\n",
       "  310,\n",
       "  311,\n",
       "  99,\n",
       "  11,\n",
       "  734,\n",
       "  735,\n",
       "  736,\n",
       "  3,\n",
       "  737,\n",
       "  20,\n",
       "  461],\n",
       " [144,\n",
       "  18,\n",
       "  15,\n",
       "  731,\n",
       "  10,\n",
       "  186,\n",
       "  18,\n",
       "  24,\n",
       "  460,\n",
       "  187,\n",
       "  114,\n",
       "  732,\n",
       "  733,\n",
       "  4,\n",
       "  26,\n",
       "  33,\n",
       "  12,\n",
       "  99,\n",
       "  11,\n",
       "  38,\n",
       "  115,\n",
       "  42,\n",
       "  164,\n",
       "  3,\n",
       "  310,\n",
       "  311,\n",
       "  99,\n",
       "  11,\n",
       "  734,\n",
       "  735,\n",
       "  736,\n",
       "  3,\n",
       "  737,\n",
       "  20,\n",
       "  461,\n",
       "  3],\n",
       " [144,\n",
       "  18,\n",
       "  15,\n",
       "  731,\n",
       "  10,\n",
       "  186,\n",
       "  18,\n",
       "  24,\n",
       "  460,\n",
       "  187,\n",
       "  114,\n",
       "  732,\n",
       "  733,\n",
       "  4,\n",
       "  26,\n",
       "  33,\n",
       "  12,\n",
       "  99,\n",
       "  11,\n",
       "  38,\n",
       "  115,\n",
       "  42,\n",
       "  164,\n",
       "  3,\n",
       "  310,\n",
       "  311,\n",
       "  99,\n",
       "  11,\n",
       "  734,\n",
       "  735,\n",
       "  736,\n",
       "  3,\n",
       "  737,\n",
       "  20,\n",
       "  461,\n",
       "  3,\n",
       "  188],\n",
       " [144,\n",
       "  18,\n",
       "  15,\n",
       "  731,\n",
       "  10,\n",
       "  186,\n",
       "  18,\n",
       "  24,\n",
       "  460,\n",
       "  187,\n",
       "  114,\n",
       "  732,\n",
       "  733,\n",
       "  4,\n",
       "  26,\n",
       "  33,\n",
       "  12,\n",
       "  99,\n",
       "  11,\n",
       "  38,\n",
       "  115,\n",
       "  42,\n",
       "  164,\n",
       "  3,\n",
       "  310,\n",
       "  311,\n",
       "  99,\n",
       "  11,\n",
       "  734,\n",
       "  735,\n",
       "  736,\n",
       "  3,\n",
       "  737,\n",
       "  20,\n",
       "  461,\n",
       "  3,\n",
       "  188,\n",
       "  462],\n",
       " [189, 145],\n",
       " [189, 145, 57],\n",
       " [189, 145, 57, 738],\n",
       " [189, 145, 57, 738, 739],\n",
       " [189, 145, 57, 738, 739, 312],\n",
       " [189, 145, 57, 738, 739, 312, 145],\n",
       " [189, 145, 57, 738, 739, 312, 145, 740],\n",
       " [189, 145, 57, 738, 739, 312, 145, 740, 313],\n",
       " [189, 145, 57, 738, 739, 312, 145, 740, 313, 2],\n",
       " [189, 145, 57, 738, 739, 312, 145, 740, 313, 2, 741],\n",
       " [189, 145, 57, 738, 739, 312, 145, 740, 313, 2, 741, 9],\n",
       " [189, 145, 57, 738, 739, 312, 145, 740, 313, 2, 741, 9, 26],\n",
       " [189, 145, 57, 738, 739, 312, 145, 740, 313, 2, 741, 9, 26, 165],\n",
       " [189, 145, 57, 738, 739, 312, 145, 740, 313, 2, 741, 9, 26, 165, 127],\n",
       " [189, 145, 57, 738, 739, 312, 145, 740, 313, 2, 741, 9, 26, 165, 127, 314],\n",
       " [189,\n",
       "  145,\n",
       "  57,\n",
       "  738,\n",
       "  739,\n",
       "  312,\n",
       "  145,\n",
       "  740,\n",
       "  313,\n",
       "  2,\n",
       "  741,\n",
       "  9,\n",
       "  26,\n",
       "  165,\n",
       "  127,\n",
       "  314,\n",
       "  21],\n",
       " [189,\n",
       "  145,\n",
       "  57,\n",
       "  738,\n",
       "  739,\n",
       "  312,\n",
       "  145,\n",
       "  740,\n",
       "  313,\n",
       "  2,\n",
       "  741,\n",
       "  9,\n",
       "  26,\n",
       "  165,\n",
       "  127,\n",
       "  314,\n",
       "  21,\n",
       "  52],\n",
       " [189,\n",
       "  145,\n",
       "  57,\n",
       "  738,\n",
       "  739,\n",
       "  312,\n",
       "  145,\n",
       "  740,\n",
       "  313,\n",
       "  2,\n",
       "  741,\n",
       "  9,\n",
       "  26,\n",
       "  165,\n",
       "  127,\n",
       "  314,\n",
       "  21,\n",
       "  52,\n",
       "  742],\n",
       " [189,\n",
       "  145,\n",
       "  57,\n",
       "  738,\n",
       "  739,\n",
       "  312,\n",
       "  145,\n",
       "  740,\n",
       "  313,\n",
       "  2,\n",
       "  741,\n",
       "  9,\n",
       "  26,\n",
       "  165,\n",
       "  127,\n",
       "  314,\n",
       "  21,\n",
       "  52,\n",
       "  742,\n",
       "  166],\n",
       " [189,\n",
       "  145,\n",
       "  57,\n",
       "  738,\n",
       "  739,\n",
       "  312,\n",
       "  145,\n",
       "  740,\n",
       "  313,\n",
       "  2,\n",
       "  741,\n",
       "  9,\n",
       "  26,\n",
       "  165,\n",
       "  127,\n",
       "  314,\n",
       "  21,\n",
       "  52,\n",
       "  742,\n",
       "  166,\n",
       "  315],\n",
       " [6, 240],\n",
       " [6, 240, 743],\n",
       " [6, 240, 743, 744],\n",
       " [6, 240, 743, 744, 463],\n",
       " [6, 240, 743, 744, 463, 7],\n",
       " [6, 240, 743, 744, 463, 7, 745],\n",
       " [6, 240, 743, 744, 463, 7, 745, 746],\n",
       " [6, 240, 743, 744, 463, 7, 745, 746, 461],\n",
       " [6, 240, 743, 744, 463, 7, 745, 746, 461, 26],\n",
       " [6, 240, 743, 744, 463, 7, 745, 746, 461, 26, 139],\n",
       " [6, 240, 743, 744, 463, 7, 745, 746, 461, 26, 139, 747],\n",
       " [6, 240, 743, 744, 463, 7, 745, 746, 461, 26, 139, 747, 748],\n",
       " [6, 240, 743, 744, 463, 7, 745, 746, 461, 26, 139, 747, 748, 464],\n",
       " [6, 240, 743, 744, 463, 7, 745, 746, 461, 26, 139, 747, 748, 464, 75],\n",
       " [6, 240, 743, 744, 463, 7, 745, 746, 461, 26, 139, 747, 748, 464, 75, 141],\n",
       " [6,\n",
       "  240,\n",
       "  743,\n",
       "  744,\n",
       "  463,\n",
       "  7,\n",
       "  745,\n",
       "  746,\n",
       "  461,\n",
       "  26,\n",
       "  139,\n",
       "  747,\n",
       "  748,\n",
       "  464,\n",
       "  75,\n",
       "  141,\n",
       "  167],\n",
       " [6,\n",
       "  240,\n",
       "  743,\n",
       "  744,\n",
       "  463,\n",
       "  7,\n",
       "  745,\n",
       "  746,\n",
       "  461,\n",
       "  26,\n",
       "  139,\n",
       "  747,\n",
       "  748,\n",
       "  464,\n",
       "  75,\n",
       "  141,\n",
       "  167,\n",
       "  465],\n",
       " [6,\n",
       "  240,\n",
       "  743,\n",
       "  744,\n",
       "  463,\n",
       "  7,\n",
       "  745,\n",
       "  746,\n",
       "  461,\n",
       "  26,\n",
       "  139,\n",
       "  747,\n",
       "  748,\n",
       "  464,\n",
       "  75,\n",
       "  141,\n",
       "  167,\n",
       "  465,\n",
       "  749],\n",
       " [1, 316],\n",
       " [1, 316, 750],\n",
       " [1, 316, 750, 245],\n",
       " [1, 316, 750, 245, 317],\n",
       " [1, 316, 750, 245, 317, 29],\n",
       " [1, 316, 750, 245, 317, 29, 246],\n",
       " [17, 751],\n",
       " [17, 751, 1],\n",
       " [17, 751, 1, 188],\n",
       " [17, 751, 1, 188, 752],\n",
       " [17, 751, 1, 188, 752, 4],\n",
       " [17, 751, 1, 188, 752, 4, 318],\n",
       " [17, 751, 1, 188, 752, 4, 318, 33],\n",
       " [17, 751, 1, 188, 752, 4, 318, 33, 146],\n",
       " [17, 751, 1, 188, 752, 4, 318, 33, 146, 88],\n",
       " [17, 751, 1, 188, 752, 4, 318, 33, 146, 88, 22],\n",
       " [17, 751, 1, 188, 752, 4, 318, 33, 146, 88, 22, 29],\n",
       " [17, 751, 1, 188, 752, 4, 318, 33, 146, 88, 22, 29, 83],\n",
       " [17, 751, 1, 188, 752, 4, 318, 33, 146, 88, 22, 29, 83, 2],\n",
       " [17, 751, 1, 188, 752, 4, 318, 33, 146, 88, 22, 29, 83, 2, 190],\n",
       " [17, 751, 1, 188, 752, 4, 318, 33, 146, 88, 22, 29, 83, 2, 190, 116],\n",
       " [17, 751, 1, 188, 752, 4, 318, 33, 146, 88, 22, 29, 83, 2, 190, 116, 319],\n",
       " [17,\n",
       "  751,\n",
       "  1,\n",
       "  188,\n",
       "  752,\n",
       "  4,\n",
       "  318,\n",
       "  33,\n",
       "  146,\n",
       "  88,\n",
       "  22,\n",
       "  29,\n",
       "  83,\n",
       "  2,\n",
       "  190,\n",
       "  116,\n",
       "  319,\n",
       "  320],\n",
       " [17,\n",
       "  751,\n",
       "  1,\n",
       "  188,\n",
       "  752,\n",
       "  4,\n",
       "  318,\n",
       "  33,\n",
       "  146,\n",
       "  88,\n",
       "  22,\n",
       "  29,\n",
       "  83,\n",
       "  2,\n",
       "  190,\n",
       "  116,\n",
       "  319,\n",
       "  320,\n",
       "  753],\n",
       " [17,\n",
       "  751,\n",
       "  1,\n",
       "  188,\n",
       "  752,\n",
       "  4,\n",
       "  318,\n",
       "  33,\n",
       "  146,\n",
       "  88,\n",
       "  22,\n",
       "  29,\n",
       "  83,\n",
       "  2,\n",
       "  190,\n",
       "  116,\n",
       "  319,\n",
       "  320,\n",
       "  753,\n",
       "  318],\n",
       " [17,\n",
       "  751,\n",
       "  1,\n",
       "  188,\n",
       "  752,\n",
       "  4,\n",
       "  318,\n",
       "  33,\n",
       "  146,\n",
       "  88,\n",
       "  22,\n",
       "  29,\n",
       "  83,\n",
       "  2,\n",
       "  190,\n",
       "  116,\n",
       "  319,\n",
       "  320,\n",
       "  753,\n",
       "  318,\n",
       "  33],\n",
       " [17,\n",
       "  751,\n",
       "  1,\n",
       "  188,\n",
       "  752,\n",
       "  4,\n",
       "  318,\n",
       "  33,\n",
       "  146,\n",
       "  88,\n",
       "  22,\n",
       "  29,\n",
       "  83,\n",
       "  2,\n",
       "  190,\n",
       "  116,\n",
       "  319,\n",
       "  320,\n",
       "  753,\n",
       "  318,\n",
       "  33,\n",
       "  321],\n",
       " [754, 466],\n",
       " [754, 466, 33],\n",
       " [754, 466, 33, 755],\n",
       " [754, 466, 33, 755, 34],\n",
       " [754, 466, 33, 755, 34, 29],\n",
       " [754, 466, 33, 755, 34, 29, 466],\n",
       " [754, 466, 33, 755, 34, 29, 466, 52],\n",
       " [754, 466, 33, 755, 34, 29, 466, 52, 467],\n",
       " [754, 466, 33, 755, 34, 29, 466, 52, 467, 8],\n",
       " [754, 466, 33, 755, 34, 29, 466, 52, 467, 8, 88],\n",
       " [18, 20],\n",
       " [18, 20, 468],\n",
       " [18, 20, 468, 469],\n",
       " [18, 20, 468, 469, 36],\n",
       " [18, 20, 468, 469, 36, 756],\n",
       " [18, 20, 468, 469, 36, 756, 8],\n",
       " [18, 20, 468, 469, 36, 756, 8, 757],\n",
       " [18, 20, 468, 469, 36, 756, 8, 757, 758],\n",
       " [18, 20, 468, 469, 36, 756, 8, 757, 758, 34],\n",
       " [18, 20, 468, 469, 36, 756, 8, 757, 758, 34, 759],\n",
       " [18, 20, 468, 469, 36, 756, 8, 757, 758, 34, 759, 19],\n",
       " [18, 20, 468, 469, 36, 756, 8, 757, 758, 34, 759, 19, 3],\n",
       " [18, 20, 468, 469, 36, 756, 8, 757, 758, 34, 759, 19, 3, 760],\n",
       " [18, 20, 468, 469, 36, 756, 8, 757, 758, 34, 759, 19, 3, 760, 322],\n",
       " [18, 20, 468, 469, 36, 756, 8, 757, 758, 34, 759, 19, 3, 760, 322, 761],\n",
       " [18, 20, 468, 469, 36, 756, 8, 757, 758, 34, 759, 19, 3, 760, 322, 761, 762],\n",
       " [18,\n",
       "  20,\n",
       "  468,\n",
       "  469,\n",
       "  36,\n",
       "  756,\n",
       "  8,\n",
       "  757,\n",
       "  758,\n",
       "  34,\n",
       "  759,\n",
       "  19,\n",
       "  3,\n",
       "  760,\n",
       "  322,\n",
       "  761,\n",
       "  762,\n",
       "  2],\n",
       " [18,\n",
       "  20,\n",
       "  468,\n",
       "  469,\n",
       "  36,\n",
       "  756,\n",
       "  8,\n",
       "  757,\n",
       "  758,\n",
       "  34,\n",
       "  759,\n",
       "  19,\n",
       "  3,\n",
       "  760,\n",
       "  322,\n",
       "  761,\n",
       "  762,\n",
       "  2,\n",
       "  763],\n",
       " [18,\n",
       "  20,\n",
       "  468,\n",
       "  469,\n",
       "  36,\n",
       "  756,\n",
       "  8,\n",
       "  757,\n",
       "  758,\n",
       "  34,\n",
       "  759,\n",
       "  19,\n",
       "  3,\n",
       "  760,\n",
       "  322,\n",
       "  761,\n",
       "  762,\n",
       "  2,\n",
       "  763,\n",
       "  3],\n",
       " [18,\n",
       "  20,\n",
       "  468,\n",
       "  469,\n",
       "  36,\n",
       "  756,\n",
       "  8,\n",
       "  757,\n",
       "  758,\n",
       "  34,\n",
       "  759,\n",
       "  19,\n",
       "  3,\n",
       "  760,\n",
       "  322,\n",
       "  761,\n",
       "  762,\n",
       "  2,\n",
       "  763,\n",
       "  3,\n",
       "  470],\n",
       " [18,\n",
       "  20,\n",
       "  468,\n",
       "  469,\n",
       "  36,\n",
       "  756,\n",
       "  8,\n",
       "  757,\n",
       "  758,\n",
       "  34,\n",
       "  759,\n",
       "  19,\n",
       "  3,\n",
       "  760,\n",
       "  322,\n",
       "  761,\n",
       "  762,\n",
       "  2,\n",
       "  763,\n",
       "  3,\n",
       "  470,\n",
       "  34],\n",
       " [18,\n",
       "  20,\n",
       "  468,\n",
       "  469,\n",
       "  36,\n",
       "  756,\n",
       "  8,\n",
       "  757,\n",
       "  758,\n",
       "  34,\n",
       "  759,\n",
       "  19,\n",
       "  3,\n",
       "  760,\n",
       "  322,\n",
       "  761,\n",
       "  762,\n",
       "  2,\n",
       "  763,\n",
       "  3,\n",
       "  470,\n",
       "  34,\n",
       "  191],\n",
       " [18,\n",
       "  20,\n",
       "  468,\n",
       "  469,\n",
       "  36,\n",
       "  756,\n",
       "  8,\n",
       "  757,\n",
       "  758,\n",
       "  34,\n",
       "  759,\n",
       "  19,\n",
       "  3,\n",
       "  760,\n",
       "  322,\n",
       "  761,\n",
       "  762,\n",
       "  2,\n",
       "  763,\n",
       "  3,\n",
       "  470,\n",
       "  34,\n",
       "  191,\n",
       "  764],\n",
       " [18,\n",
       "  20,\n",
       "  468,\n",
       "  469,\n",
       "  36,\n",
       "  756,\n",
       "  8,\n",
       "  757,\n",
       "  758,\n",
       "  34,\n",
       "  759,\n",
       "  19,\n",
       "  3,\n",
       "  760,\n",
       "  322,\n",
       "  761,\n",
       "  762,\n",
       "  2,\n",
       "  763,\n",
       "  3,\n",
       "  470,\n",
       "  34,\n",
       "  191,\n",
       "  764,\n",
       "  247],\n",
       " [18,\n",
       "  20,\n",
       "  468,\n",
       "  469,\n",
       "  36,\n",
       "  756,\n",
       "  8,\n",
       "  757,\n",
       "  758,\n",
       "  34,\n",
       "  759,\n",
       "  19,\n",
       "  3,\n",
       "  760,\n",
       "  322,\n",
       "  761,\n",
       "  762,\n",
       "  2,\n",
       "  763,\n",
       "  3,\n",
       "  470,\n",
       "  34,\n",
       "  191,\n",
       "  764,\n",
       "  247,\n",
       "  22],\n",
       " [18,\n",
       "  20,\n",
       "  468,\n",
       "  469,\n",
       "  36,\n",
       "  756,\n",
       "  8,\n",
       "  757,\n",
       "  758,\n",
       "  34,\n",
       "  759,\n",
       "  19,\n",
       "  3,\n",
       "  760,\n",
       "  322,\n",
       "  761,\n",
       "  762,\n",
       "  2,\n",
       "  763,\n",
       "  3,\n",
       "  470,\n",
       "  34,\n",
       "  191,\n",
       "  764,\n",
       "  247,\n",
       "  22,\n",
       "  323],\n",
       " [18,\n",
       "  20,\n",
       "  468,\n",
       "  469,\n",
       "  36,\n",
       "  756,\n",
       "  8,\n",
       "  757,\n",
       "  758,\n",
       "  34,\n",
       "  759,\n",
       "  19,\n",
       "  3,\n",
       "  760,\n",
       "  322,\n",
       "  761,\n",
       "  762,\n",
       "  2,\n",
       "  763,\n",
       "  3,\n",
       "  470,\n",
       "  34,\n",
       "  191,\n",
       "  764,\n",
       "  247,\n",
       "  22,\n",
       "  323,\n",
       "  471],\n",
       " [18,\n",
       "  20,\n",
       "  468,\n",
       "  469,\n",
       "  36,\n",
       "  756,\n",
       "  8,\n",
       "  757,\n",
       "  758,\n",
       "  34,\n",
       "  759,\n",
       "  19,\n",
       "  3,\n",
       "  760,\n",
       "  322,\n",
       "  761,\n",
       "  762,\n",
       "  2,\n",
       "  763,\n",
       "  3,\n",
       "  470,\n",
       "  34,\n",
       "  191,\n",
       "  764,\n",
       "  247,\n",
       "  22,\n",
       "  323,\n",
       "  471,\n",
       "  9],\n",
       " [18,\n",
       "  20,\n",
       "  468,\n",
       "  469,\n",
       "  36,\n",
       "  756,\n",
       "  8,\n",
       "  757,\n",
       "  758,\n",
       "  34,\n",
       "  759,\n",
       "  19,\n",
       "  3,\n",
       "  760,\n",
       "  322,\n",
       "  761,\n",
       "  762,\n",
       "  2,\n",
       "  763,\n",
       "  3,\n",
       "  470,\n",
       "  34,\n",
       "  191,\n",
       "  764,\n",
       "  247,\n",
       "  22,\n",
       "  323,\n",
       "  471,\n",
       "  9,\n",
       "  472],\n",
       " [18,\n",
       "  20,\n",
       "  468,\n",
       "  469,\n",
       "  36,\n",
       "  756,\n",
       "  8,\n",
       "  757,\n",
       "  758,\n",
       "  34,\n",
       "  759,\n",
       "  19,\n",
       "  3,\n",
       "  760,\n",
       "  322,\n",
       "  761,\n",
       "  762,\n",
       "  2,\n",
       "  763,\n",
       "  3,\n",
       "  470,\n",
       "  34,\n",
       "  191,\n",
       "  764,\n",
       "  247,\n",
       "  22,\n",
       "  323,\n",
       "  471,\n",
       "  9,\n",
       "  472,\n",
       "  2],\n",
       " [18,\n",
       "  20,\n",
       "  468,\n",
       "  469,\n",
       "  36,\n",
       "  756,\n",
       "  8,\n",
       "  757,\n",
       "  758,\n",
       "  34,\n",
       "  759,\n",
       "  19,\n",
       "  3,\n",
       "  760,\n",
       "  322,\n",
       "  761,\n",
       "  762,\n",
       "  2,\n",
       "  763,\n",
       "  3,\n",
       "  470,\n",
       "  34,\n",
       "  191,\n",
       "  764,\n",
       "  247,\n",
       "  22,\n",
       "  323,\n",
       "  471,\n",
       "  9,\n",
       "  472,\n",
       "  2,\n",
       "  765],\n",
       " [18,\n",
       "  20,\n",
       "  468,\n",
       "  469,\n",
       "  36,\n",
       "  756,\n",
       "  8,\n",
       "  757,\n",
       "  758,\n",
       "  34,\n",
       "  759,\n",
       "  19,\n",
       "  3,\n",
       "  760,\n",
       "  322,\n",
       "  761,\n",
       "  762,\n",
       "  2,\n",
       "  763,\n",
       "  3,\n",
       "  470,\n",
       "  34,\n",
       "  191,\n",
       "  764,\n",
       "  247,\n",
       "  22,\n",
       "  323,\n",
       "  471,\n",
       "  9,\n",
       "  472,\n",
       "  2,\n",
       "  765,\n",
       "  248],\n",
       " [18,\n",
       "  20,\n",
       "  468,\n",
       "  469,\n",
       "  36,\n",
       "  756,\n",
       "  8,\n",
       "  757,\n",
       "  758,\n",
       "  34,\n",
       "  759,\n",
       "  19,\n",
       "  3,\n",
       "  760,\n",
       "  322,\n",
       "  761,\n",
       "  762,\n",
       "  2,\n",
       "  763,\n",
       "  3,\n",
       "  470,\n",
       "  34,\n",
       "  191,\n",
       "  764,\n",
       "  247,\n",
       "  22,\n",
       "  323,\n",
       "  471,\n",
       "  9,\n",
       "  472,\n",
       "  2,\n",
       "  765,\n",
       "  248,\n",
       "  36],\n",
       " [18,\n",
       "  20,\n",
       "  468,\n",
       "  469,\n",
       "  36,\n",
       "  756,\n",
       "  8,\n",
       "  757,\n",
       "  758,\n",
       "  34,\n",
       "  759,\n",
       "  19,\n",
       "  3,\n",
       "  760,\n",
       "  322,\n",
       "  761,\n",
       "  762,\n",
       "  2,\n",
       "  763,\n",
       "  3,\n",
       "  470,\n",
       "  34,\n",
       "  191,\n",
       "  764,\n",
       "  247,\n",
       "  22,\n",
       "  323,\n",
       "  471,\n",
       "  9,\n",
       "  472,\n",
       "  2,\n",
       "  765,\n",
       "  248,\n",
       "  36,\n",
       "  63],\n",
       " [37, 1],\n",
       " [37, 1, 766],\n",
       " [37, 1, 766, 7],\n",
       " [37, 1, 766, 7, 61],\n",
       " [37, 1, 766, 7, 61, 249],\n",
       " [37, 1, 766, 7, 61, 249, 9],\n",
       " [37, 1, 766, 7, 61, 249, 9, 1],\n",
       " [37, 1, 766, 7, 61, 249, 9, 1, 473],\n",
       " [37, 1, 766, 7, 61, 249, 9, 1, 473, 7],\n",
       " [37, 1, 766, 7, 61, 249, 9, 1, 473, 7, 460],\n",
       " [37, 1, 766, 7, 61, 249, 9, 1, 473, 7, 460, 312],\n",
       " [37, 1, 766, 7, 61, 249, 9, 1, 473, 7, 460, 312, 2],\n",
       " [37, 1, 766, 7, 61, 249, 9, 1, 473, 7, 460, 312, 2, 474],\n",
       " [43, 72],\n",
       " [43, 72, 767],\n",
       " [43, 72, 767, 6],\n",
       " [43, 72, 767, 6, 768],\n",
       " [43, 72, 767, 6, 768, 10],\n",
       " [43, 72, 767, 6, 768, 10, 475],\n",
       " [43, 72, 767, 6, 768, 10, 475, 316],\n",
       " [43, 72, 767, 6, 768, 10, 475, 316, 6],\n",
       " [43, 72, 767, 6, 768, 10, 475, 316, 6, 88],\n",
       " [43, 72, 767, 6, 768, 10, 475, 316, 6, 88, 769],\n",
       " [43, 72, 767, 6, 768, 10, 475, 316, 6, 88, 769, 770],\n",
       " [43, 72, 767, 6, 768, 10, 475, 316, 6, 88, 769, 770, 1],\n",
       " [43, 72, 767, 6, 768, 10, 475, 316, 6, 88, 769, 770, 1, 313],\n",
       " [43, 72, 767, 6, 768, 10, 475, 316, 6, 88, 769, 770, 1, 313, 250],\n",
       " [43, 72, 767, 6, 768, 10, 475, 316, 6, 88, 769, 770, 1, 313, 250, 4],\n",
       " [43, 72, 767, 6, 768, 10, 475, 316, 6, 88, 769, 770, 1, 313, 250, 4, 476],\n",
       " [43, 72, 767, 6, 768, 10, 475, 316, 6, 88, 769, 770, 1, 313, 250, 4, 476, 4],\n",
       " [43,\n",
       "  72,\n",
       "  767,\n",
       "  6,\n",
       "  768,\n",
       "  10,\n",
       "  475,\n",
       "  316,\n",
       "  6,\n",
       "  88,\n",
       "  769,\n",
       "  770,\n",
       "  1,\n",
       "  313,\n",
       "  250,\n",
       "  4,\n",
       "  476,\n",
       "  4,\n",
       "  771],\n",
       " [43,\n",
       "  72,\n",
       "  767,\n",
       "  6,\n",
       "  768,\n",
       "  10,\n",
       "  475,\n",
       "  316,\n",
       "  6,\n",
       "  88,\n",
       "  769,\n",
       "  770,\n",
       "  1,\n",
       "  313,\n",
       "  250,\n",
       "  4,\n",
       "  476,\n",
       "  4,\n",
       "  771,\n",
       "  772],\n",
       " [37, 84],\n",
       " [37, 84, 64],\n",
       " [37, 84, 64, 324],\n",
       " [37, 84, 64, 324, 4],\n",
       " [37, 84, 64, 324, 4, 100],\n",
       " [37, 84, 64, 324, 4, 100, 251],\n",
       " [37, 84, 64, 324, 4, 100, 251, 128],\n",
       " [37, 84, 64, 324, 4, 100, 251, 128, 773],\n",
       " [37, 84, 64, 324, 4, 100, 251, 128, 773, 192],\n",
       " [37, 84, 64, 324, 4, 100, 251, 128, 773, 192, 325],\n",
       " [37, 84, 64, 324, 4, 100, 251, 128, 773, 192, 325, 1],\n",
       " [37, 84, 64, 324, 4, 100, 251, 128, 773, 192, 325, 1, 27],\n",
       " [37, 84, 64, 324, 4, 100, 251, 128, 773, 192, 325, 1, 27, 7],\n",
       " [37, 84, 64, 324, 4, 100, 251, 128, 773, 192, 325, 1, 27, 7, 3],\n",
       " [37, 84, 64, 324, 4, 100, 251, 128, 773, 192, 325, 1, 27, 7, 3, 166],\n",
       " [37, 84, 64, 324, 4, 100, 251, 128, 773, 192, 325, 1, 27, 7, 3, 166, 315],\n",
       " [37, 84, 64, 324, 4, 100, 251, 128, 773, 192, 325, 1, 27, 7, 3, 166, 315, 39],\n",
       " [37,\n",
       "  84,\n",
       "  64,\n",
       "  324,\n",
       "  4,\n",
       "  100,\n",
       "  251,\n",
       "  128,\n",
       "  773,\n",
       "  192,\n",
       "  325,\n",
       "  1,\n",
       "  27,\n",
       "  7,\n",
       "  3,\n",
       "  166,\n",
       "  315,\n",
       "  39,\n",
       "  477],\n",
       " [37,\n",
       "  84,\n",
       "  64,\n",
       "  324,\n",
       "  4,\n",
       "  100,\n",
       "  251,\n",
       "  128,\n",
       "  773,\n",
       "  192,\n",
       "  325,\n",
       "  1,\n",
       "  27,\n",
       "  7,\n",
       "  3,\n",
       "  166,\n",
       "  315,\n",
       "  39,\n",
       "  477,\n",
       "  6],\n",
       " [37,\n",
       "  84,\n",
       "  64,\n",
       "  324,\n",
       "  4,\n",
       "  100,\n",
       "  251,\n",
       "  128,\n",
       "  773,\n",
       "  192,\n",
       "  325,\n",
       "  1,\n",
       "  27,\n",
       "  7,\n",
       "  3,\n",
       "  166,\n",
       "  315,\n",
       "  39,\n",
       "  477,\n",
       "  6,\n",
       "  774],\n",
       " [37,\n",
       "  84,\n",
       "  64,\n",
       "  324,\n",
       "  4,\n",
       "  100,\n",
       "  251,\n",
       "  128,\n",
       "  773,\n",
       "  192,\n",
       "  325,\n",
       "  1,\n",
       "  27,\n",
       "  7,\n",
       "  3,\n",
       "  166,\n",
       "  315,\n",
       "  39,\n",
       "  477,\n",
       "  6,\n",
       "  774,\n",
       "  6],\n",
       " [37,\n",
       "  84,\n",
       "  64,\n",
       "  324,\n",
       "  4,\n",
       "  100,\n",
       "  251,\n",
       "  128,\n",
       "  773,\n",
       "  192,\n",
       "  325,\n",
       "  1,\n",
       "  27,\n",
       "  7,\n",
       "  3,\n",
       "  166,\n",
       "  315,\n",
       "  39,\n",
       "  477,\n",
       "  6,\n",
       "  774,\n",
       "  6,\n",
       "  3],\n",
       " [37,\n",
       "  84,\n",
       "  64,\n",
       "  324,\n",
       "  4,\n",
       "  100,\n",
       "  251,\n",
       "  128,\n",
       "  773,\n",
       "  192,\n",
       "  325,\n",
       "  1,\n",
       "  27,\n",
       "  7,\n",
       "  3,\n",
       "  166,\n",
       "  315,\n",
       "  39,\n",
       "  477,\n",
       "  6,\n",
       "  774,\n",
       "  6,\n",
       "  3,\n",
       "  775],\n",
       " [37,\n",
       "  84,\n",
       "  64,\n",
       "  324,\n",
       "  4,\n",
       "  100,\n",
       "  251,\n",
       "  128,\n",
       "  773,\n",
       "  192,\n",
       "  325,\n",
       "  1,\n",
       "  27,\n",
       "  7,\n",
       "  3,\n",
       "  166,\n",
       "  315,\n",
       "  39,\n",
       "  477,\n",
       "  6,\n",
       "  774,\n",
       "  6,\n",
       "  3,\n",
       "  775,\n",
       "  478],\n",
       " [37,\n",
       "  84,\n",
       "  64,\n",
       "  324,\n",
       "  4,\n",
       "  100,\n",
       "  251,\n",
       "  128,\n",
       "  773,\n",
       "  192,\n",
       "  325,\n",
       "  1,\n",
       "  27,\n",
       "  7,\n",
       "  3,\n",
       "  166,\n",
       "  315,\n",
       "  39,\n",
       "  477,\n",
       "  6,\n",
       "  774,\n",
       "  6,\n",
       "  3,\n",
       "  775,\n",
       "  478,\n",
       "  129],\n",
       " [37,\n",
       "  84,\n",
       "  64,\n",
       "  324,\n",
       "  4,\n",
       "  100,\n",
       "  251,\n",
       "  128,\n",
       "  773,\n",
       "  192,\n",
       "  325,\n",
       "  1,\n",
       "  27,\n",
       "  7,\n",
       "  3,\n",
       "  166,\n",
       "  315,\n",
       "  39,\n",
       "  477,\n",
       "  6,\n",
       "  774,\n",
       "  6,\n",
       "  3,\n",
       "  775,\n",
       "  478,\n",
       "  129,\n",
       "  776],\n",
       " [37,\n",
       "  84,\n",
       "  64,\n",
       "  324,\n",
       "  4,\n",
       "  100,\n",
       "  251,\n",
       "  128,\n",
       "  773,\n",
       "  192,\n",
       "  325,\n",
       "  1,\n",
       "  27,\n",
       "  7,\n",
       "  3,\n",
       "  166,\n",
       "  315,\n",
       "  39,\n",
       "  477,\n",
       "  6,\n",
       "  774,\n",
       "  6,\n",
       "  3,\n",
       "  775,\n",
       "  478,\n",
       "  129,\n",
       "  776,\n",
       "  7],\n",
       " [37,\n",
       "  84,\n",
       "  64,\n",
       "  324,\n",
       "  4,\n",
       "  100,\n",
       "  251,\n",
       "  128,\n",
       "  773,\n",
       "  192,\n",
       "  325,\n",
       "  1,\n",
       "  27,\n",
       "  7,\n",
       "  3,\n",
       "  166,\n",
       "  315,\n",
       "  39,\n",
       "  477,\n",
       "  6,\n",
       "  774,\n",
       "  6,\n",
       "  3,\n",
       "  775,\n",
       "  478,\n",
       "  129,\n",
       "  776,\n",
       "  7,\n",
       "  44],\n",
       " [37,\n",
       "  84,\n",
       "  64,\n",
       "  324,\n",
       "  4,\n",
       "  100,\n",
       "  251,\n",
       "  128,\n",
       "  773,\n",
       "  192,\n",
       "  325,\n",
       "  1,\n",
       "  27,\n",
       "  7,\n",
       "  3,\n",
       "  166,\n",
       "  315,\n",
       "  39,\n",
       "  477,\n",
       "  6,\n",
       "  774,\n",
       "  6,\n",
       "  3,\n",
       "  775,\n",
       "  478,\n",
       "  129,\n",
       "  776,\n",
       "  7,\n",
       "  44,\n",
       "  18],\n",
       " [37,\n",
       "  84,\n",
       "  64,\n",
       "  324,\n",
       "  4,\n",
       "  100,\n",
       "  251,\n",
       "  128,\n",
       "  773,\n",
       "  192,\n",
       "  325,\n",
       "  1,\n",
       "  27,\n",
       "  7,\n",
       "  3,\n",
       "  166,\n",
       "  315,\n",
       "  39,\n",
       "  477,\n",
       "  6,\n",
       "  774,\n",
       "  6,\n",
       "  3,\n",
       "  775,\n",
       "  478,\n",
       "  129,\n",
       "  776,\n",
       "  7,\n",
       "  44,\n",
       "  18,\n",
       "  777],\n",
       " [37,\n",
       "  84,\n",
       "  64,\n",
       "  324,\n",
       "  4,\n",
       "  100,\n",
       "  251,\n",
       "  128,\n",
       "  773,\n",
       "  192,\n",
       "  325,\n",
       "  1,\n",
       "  27,\n",
       "  7,\n",
       "  3,\n",
       "  166,\n",
       "  315,\n",
       "  39,\n",
       "  477,\n",
       "  6,\n",
       "  774,\n",
       "  6,\n",
       "  3,\n",
       "  775,\n",
       "  478,\n",
       "  129,\n",
       "  776,\n",
       "  7,\n",
       "  44,\n",
       "  18,\n",
       "  777,\n",
       "  479],\n",
       " [37,\n",
       "  84,\n",
       "  64,\n",
       "  324,\n",
       "  4,\n",
       "  100,\n",
       "  251,\n",
       "  128,\n",
       "  773,\n",
       "  192,\n",
       "  325,\n",
       "  1,\n",
       "  27,\n",
       "  7,\n",
       "  3,\n",
       "  166,\n",
       "  315,\n",
       "  39,\n",
       "  477,\n",
       "  6,\n",
       "  774,\n",
       "  6,\n",
       "  3,\n",
       "  775,\n",
       "  478,\n",
       "  129,\n",
       "  776,\n",
       "  7,\n",
       "  44,\n",
       "  18,\n",
       "  777,\n",
       "  479,\n",
       "  21],\n",
       " [37,\n",
       "  84,\n",
       "  64,\n",
       "  324,\n",
       "  4,\n",
       "  100,\n",
       "  251,\n",
       "  128,\n",
       "  773,\n",
       "  192,\n",
       "  325,\n",
       "  1,\n",
       "  27,\n",
       "  7,\n",
       "  3,\n",
       "  166,\n",
       "  315,\n",
       "  39,\n",
       "  477,\n",
       "  6,\n",
       "  774,\n",
       "  6,\n",
       "  3,\n",
       "  775,\n",
       "  478,\n",
       "  129,\n",
       "  776,\n",
       "  7,\n",
       "  44,\n",
       "  18,\n",
       "  777,\n",
       "  479,\n",
       "  21,\n",
       "  3],\n",
       " [37,\n",
       "  84,\n",
       "  64,\n",
       "  324,\n",
       "  4,\n",
       "  100,\n",
       "  251,\n",
       "  128,\n",
       "  773,\n",
       "  192,\n",
       "  325,\n",
       "  1,\n",
       "  27,\n",
       "  7,\n",
       "  3,\n",
       "  166,\n",
       "  315,\n",
       "  39,\n",
       "  477,\n",
       "  6,\n",
       "  774,\n",
       "  6,\n",
       "  3,\n",
       "  775,\n",
       "  478,\n",
       "  129,\n",
       "  776,\n",
       "  7,\n",
       "  44,\n",
       "  18,\n",
       "  777,\n",
       "  479,\n",
       "  21,\n",
       "  3,\n",
       "  480],\n",
       " [37,\n",
       "  84,\n",
       "  64,\n",
       "  324,\n",
       "  4,\n",
       "  100,\n",
       "  251,\n",
       "  128,\n",
       "  773,\n",
       "  192,\n",
       "  325,\n",
       "  1,\n",
       "  27,\n",
       "  7,\n",
       "  3,\n",
       "  166,\n",
       "  315,\n",
       "  39,\n",
       "  477,\n",
       "  6,\n",
       "  774,\n",
       "  6,\n",
       "  3,\n",
       "  775,\n",
       "  478,\n",
       "  129,\n",
       "  776,\n",
       "  7,\n",
       "  44,\n",
       "  18,\n",
       "  777,\n",
       "  479,\n",
       "  21,\n",
       "  3,\n",
       "  480,\n",
       "  4],\n",
       " [37,\n",
       "  84,\n",
       "  64,\n",
       "  324,\n",
       "  4,\n",
       "  100,\n",
       "  251,\n",
       "  128,\n",
       "  773,\n",
       "  192,\n",
       "  325,\n",
       "  1,\n",
       "  27,\n",
       "  7,\n",
       "  3,\n",
       "  166,\n",
       "  315,\n",
       "  39,\n",
       "  477,\n",
       "  6,\n",
       "  774,\n",
       "  6,\n",
       "  3,\n",
       "  775,\n",
       "  478,\n",
       "  129,\n",
       "  776,\n",
       "  7,\n",
       "  44,\n",
       "  18,\n",
       "  777,\n",
       "  479,\n",
       "  21,\n",
       "  3,\n",
       "  480,\n",
       "  4,\n",
       "  481],\n",
       " [37,\n",
       "  84,\n",
       "  64,\n",
       "  324,\n",
       "  4,\n",
       "  100,\n",
       "  251,\n",
       "  128,\n",
       "  773,\n",
       "  192,\n",
       "  325,\n",
       "  1,\n",
       "  27,\n",
       "  7,\n",
       "  3,\n",
       "  166,\n",
       "  315,\n",
       "  39,\n",
       "  477,\n",
       "  6,\n",
       "  774,\n",
       "  6,\n",
       "  3,\n",
       "  775,\n",
       "  478,\n",
       "  129,\n",
       "  776,\n",
       "  7,\n",
       "  44,\n",
       "  18,\n",
       "  777,\n",
       "  479,\n",
       "  21,\n",
       "  3,\n",
       "  480,\n",
       "  4,\n",
       "  481,\n",
       "  326],\n",
       " [6, 130],\n",
       " [6, 130, 3],\n",
       " [6, 130, 3, 160],\n",
       " [6, 130, 3, 160, 309],\n",
       " [6, 130, 3, 160, 309, 17],\n",
       " [6, 130, 3, 160, 309, 17, 778],\n",
       " [6, 130, 3, 160, 309, 17, 778, 168],\n",
       " [6, 130, 3, 160, 309, 17, 778, 168, 1],\n",
       " [6, 130, 3, 160, 309, 17, 778, 168, 1, 779],\n",
       " [6, 130, 3, 160, 309, 17, 778, 168, 1, 779, 780],\n",
       " [6, 130, 3, 160, 309, 17, 778, 168, 1, 779, 780, 169],\n",
       " [6, 130, 3, 160, 309, 17, 778, 168, 1, 779, 780, 169, 193],\n",
       " [6, 130, 3, 160, 309, 17, 778, 168, 1, 779, 780, 169, 193, 9],\n",
       " [6, 130, 3, 160, 309, 17, 778, 168, 1, 779, 780, 169, 193, 9, 127],\n",
       " [6, 130, 3, 160, 309, 17, 778, 168, 1, 779, 780, 169, 193, 9, 127, 327],\n",
       " [6, 130, 3, 160, 309, 17, 778, 168, 1, 779, 780, 169, 193, 9, 127, 327, 252],\n",
       " [6,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  309,\n",
       "  17,\n",
       "  778,\n",
       "  168,\n",
       "  1,\n",
       "  779,\n",
       "  780,\n",
       "  169,\n",
       "  193,\n",
       "  9,\n",
       "  127,\n",
       "  327,\n",
       "  252,\n",
       "  16],\n",
       " [6,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  309,\n",
       "  17,\n",
       "  778,\n",
       "  168,\n",
       "  1,\n",
       "  779,\n",
       "  780,\n",
       "  169,\n",
       "  193,\n",
       "  9,\n",
       "  127,\n",
       "  327,\n",
       "  252,\n",
       "  16,\n",
       "  482],\n",
       " [6,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  309,\n",
       "  17,\n",
       "  778,\n",
       "  168,\n",
       "  1,\n",
       "  779,\n",
       "  780,\n",
       "  169,\n",
       "  193,\n",
       "  9,\n",
       "  127,\n",
       "  327,\n",
       "  252,\n",
       "  16,\n",
       "  482,\n",
       "  166],\n",
       " [6,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  309,\n",
       "  17,\n",
       "  778,\n",
       "  168,\n",
       "  1,\n",
       "  779,\n",
       "  780,\n",
       "  169,\n",
       "  193,\n",
       "  9,\n",
       "  127,\n",
       "  327,\n",
       "  252,\n",
       "  16,\n",
       "  482,\n",
       "  166,\n",
       "  253],\n",
       " [6,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  309,\n",
       "  17,\n",
       "  778,\n",
       "  168,\n",
       "  1,\n",
       "  779,\n",
       "  780,\n",
       "  169,\n",
       "  193,\n",
       "  9,\n",
       "  127,\n",
       "  327,\n",
       "  252,\n",
       "  16,\n",
       "  482,\n",
       "  166,\n",
       "  253,\n",
       "  781],\n",
       " [1, 27],\n",
       " [1, 27, 39],\n",
       " [1, 27, 39, 7],\n",
       " [1, 27, 39, 7, 782],\n",
       " [1, 27, 39, 7, 782, 47],\n",
       " [1, 27, 39, 7, 782, 47, 783],\n",
       " [1, 27, 39, 7, 782, 47, 783, 483],\n",
       " [1, 27, 39, 7, 782, 47, 783, 483, 6],\n",
       " [1, 27, 39, 7, 782, 47, 783, 483, 6, 236],\n",
       " [1, 27, 39, 7, 782, 47, 783, 483, 6, 236, 194],\n",
       " [1, 27, 39, 7, 782, 47, 783, 483, 6, 236, 194, 4],\n",
       " [1, 27, 39, 7, 782, 47, 783, 483, 6, 236, 194, 4, 131],\n",
       " [1, 27, 39, 7, 782, 47, 783, 483, 6, 236, 194, 4, 131, 5],\n",
       " [1, 27, 39, 7, 782, 47, 783, 483, 6, 236, 194, 4, 131, 5, 784],\n",
       " [1, 27, 39, 7, 782, 47, 783, 483, 6, 236, 194, 4, 131, 5, 784, 19],\n",
       " [1, 27, 39, 7, 782, 47, 783, 483, 6, 236, 194, 4, 131, 5, 784, 19, 254],\n",
       " [1, 27, 39, 7, 782, 47, 783, 483, 6, 236, 194, 4, 131, 5, 784, 19, 254, 785],\n",
       " [1,\n",
       "  27,\n",
       "  39,\n",
       "  7,\n",
       "  782,\n",
       "  47,\n",
       "  783,\n",
       "  483,\n",
       "  6,\n",
       "  236,\n",
       "  194,\n",
       "  4,\n",
       "  131,\n",
       "  5,\n",
       "  784,\n",
       "  19,\n",
       "  254,\n",
       "  785,\n",
       "  141],\n",
       " [1,\n",
       "  27,\n",
       "  39,\n",
       "  7,\n",
       "  782,\n",
       "  47,\n",
       "  783,\n",
       "  483,\n",
       "  6,\n",
       "  236,\n",
       "  194,\n",
       "  4,\n",
       "  131,\n",
       "  5,\n",
       "  784,\n",
       "  19,\n",
       "  254,\n",
       "  785,\n",
       "  141,\n",
       "  61],\n",
       " [1,\n",
       "  27,\n",
       "  39,\n",
       "  7,\n",
       "  782,\n",
       "  47,\n",
       "  783,\n",
       "  483,\n",
       "  6,\n",
       "  236,\n",
       "  194,\n",
       "  4,\n",
       "  131,\n",
       "  5,\n",
       "  784,\n",
       "  19,\n",
       "  254,\n",
       "  785,\n",
       "  141,\n",
       "  61,\n",
       "  9],\n",
       " [1,\n",
       "  27,\n",
       "  39,\n",
       "  7,\n",
       "  782,\n",
       "  47,\n",
       "  783,\n",
       "  483,\n",
       "  6,\n",
       "  236,\n",
       "  194,\n",
       "  4,\n",
       "  131,\n",
       "  5,\n",
       "  784,\n",
       "  19,\n",
       "  254,\n",
       "  785,\n",
       "  141,\n",
       "  61,\n",
       "  9,\n",
       "  100],\n",
       " [1,\n",
       "  27,\n",
       "  39,\n",
       "  7,\n",
       "  782,\n",
       "  47,\n",
       "  783,\n",
       "  483,\n",
       "  6,\n",
       "  236,\n",
       "  194,\n",
       "  4,\n",
       "  131,\n",
       "  5,\n",
       "  784,\n",
       "  19,\n",
       "  254,\n",
       "  785,\n",
       "  141,\n",
       "  61,\n",
       "  9,\n",
       "  100,\n",
       "  65],\n",
       " [1,\n",
       "  27,\n",
       "  39,\n",
       "  7,\n",
       "  782,\n",
       "  47,\n",
       "  783,\n",
       "  483,\n",
       "  6,\n",
       "  236,\n",
       "  194,\n",
       "  4,\n",
       "  131,\n",
       "  5,\n",
       "  784,\n",
       "  19,\n",
       "  254,\n",
       "  785,\n",
       "  141,\n",
       "  61,\n",
       "  9,\n",
       "  100,\n",
       "  65,\n",
       "  7],\n",
       " [1,\n",
       "  27,\n",
       "  39,\n",
       "  7,\n",
       "  782,\n",
       "  47,\n",
       "  783,\n",
       "  483,\n",
       "  6,\n",
       "  236,\n",
       "  194,\n",
       "  4,\n",
       "  131,\n",
       "  5,\n",
       "  784,\n",
       "  19,\n",
       "  254,\n",
       "  785,\n",
       "  141,\n",
       "  61,\n",
       "  9,\n",
       "  100,\n",
       "  65,\n",
       "  7,\n",
       "  110],\n",
       " [1,\n",
       "  27,\n",
       "  39,\n",
       "  7,\n",
       "  782,\n",
       "  47,\n",
       "  783,\n",
       "  483,\n",
       "  6,\n",
       "  236,\n",
       "  194,\n",
       "  4,\n",
       "  131,\n",
       "  5,\n",
       "  784,\n",
       "  19,\n",
       "  254,\n",
       "  785,\n",
       "  141,\n",
       "  61,\n",
       "  9,\n",
       "  100,\n",
       "  65,\n",
       "  7,\n",
       "  110,\n",
       "  786],\n",
       " [1,\n",
       "  27,\n",
       "  39,\n",
       "  7,\n",
       "  782,\n",
       "  47,\n",
       "  783,\n",
       "  483,\n",
       "  6,\n",
       "  236,\n",
       "  194,\n",
       "  4,\n",
       "  131,\n",
       "  5,\n",
       "  784,\n",
       "  19,\n",
       "  254,\n",
       "  785,\n",
       "  141,\n",
       "  61,\n",
       "  9,\n",
       "  100,\n",
       "  65,\n",
       "  7,\n",
       "  110,\n",
       "  786,\n",
       "  101],\n",
       " [1,\n",
       "  27,\n",
       "  39,\n",
       "  7,\n",
       "  782,\n",
       "  47,\n",
       "  783,\n",
       "  483,\n",
       "  6,\n",
       "  236,\n",
       "  194,\n",
       "  4,\n",
       "  131,\n",
       "  5,\n",
       "  784,\n",
       "  19,\n",
       "  254,\n",
       "  785,\n",
       "  141,\n",
       "  61,\n",
       "  9,\n",
       "  100,\n",
       "  65,\n",
       "  7,\n",
       "  110,\n",
       "  786,\n",
       "  101,\n",
       "  317],\n",
       " [1,\n",
       "  27,\n",
       "  39,\n",
       "  7,\n",
       "  782,\n",
       "  47,\n",
       "  783,\n",
       "  483,\n",
       "  6,\n",
       "  236,\n",
       "  194,\n",
       "  4,\n",
       "  131,\n",
       "  5,\n",
       "  784,\n",
       "  19,\n",
       "  254,\n",
       "  785,\n",
       "  141,\n",
       "  61,\n",
       "  9,\n",
       "  100,\n",
       "  65,\n",
       "  7,\n",
       "  110,\n",
       "  786,\n",
       "  101,\n",
       "  317,\n",
       "  88],\n",
       " [1,\n",
       "  27,\n",
       "  39,\n",
       "  7,\n",
       "  782,\n",
       "  47,\n",
       "  783,\n",
       "  483,\n",
       "  6,\n",
       "  236,\n",
       "  194,\n",
       "  4,\n",
       "  131,\n",
       "  5,\n",
       "  784,\n",
       "  19,\n",
       "  254,\n",
       "  785,\n",
       "  141,\n",
       "  61,\n",
       "  9,\n",
       "  100,\n",
       "  65,\n",
       "  7,\n",
       "  110,\n",
       "  786,\n",
       "  101,\n",
       "  317,\n",
       "  88,\n",
       "  8],\n",
       " [1,\n",
       "  27,\n",
       "  39,\n",
       "  7,\n",
       "  782,\n",
       "  47,\n",
       "  783,\n",
       "  483,\n",
       "  6,\n",
       "  236,\n",
       "  194,\n",
       "  4,\n",
       "  131,\n",
       "  5,\n",
       "  784,\n",
       "  19,\n",
       "  254,\n",
       "  785,\n",
       "  141,\n",
       "  61,\n",
       "  9,\n",
       "  100,\n",
       "  65,\n",
       "  7,\n",
       "  110,\n",
       "  786,\n",
       "  101,\n",
       "  317,\n",
       "  88,\n",
       "  8,\n",
       "  117],\n",
       " [1,\n",
       "  27,\n",
       "  39,\n",
       "  7,\n",
       "  782,\n",
       "  47,\n",
       "  783,\n",
       "  483,\n",
       "  6,\n",
       "  236,\n",
       "  194,\n",
       "  4,\n",
       "  131,\n",
       "  5,\n",
       "  784,\n",
       "  19,\n",
       "  254,\n",
       "  785,\n",
       "  141,\n",
       "  61,\n",
       "  9,\n",
       "  100,\n",
       "  65,\n",
       "  7,\n",
       "  110,\n",
       "  786,\n",
       "  101,\n",
       "  317,\n",
       "  88,\n",
       "  8,\n",
       "  117,\n",
       "  2],\n",
       " [1,\n",
       "  27,\n",
       "  39,\n",
       "  7,\n",
       "  782,\n",
       "  47,\n",
       "  783,\n",
       "  483,\n",
       "  6,\n",
       "  236,\n",
       "  194,\n",
       "  4,\n",
       "  131,\n",
       "  5,\n",
       "  784,\n",
       "  19,\n",
       "  254,\n",
       "  785,\n",
       "  141,\n",
       "  61,\n",
       "  9,\n",
       "  100,\n",
       "  65,\n",
       "  7,\n",
       "  110,\n",
       "  786,\n",
       "  101,\n",
       "  317,\n",
       "  88,\n",
       "  8,\n",
       "  117,\n",
       "  2,\n",
       "  328],\n",
       " [1,\n",
       "  27,\n",
       "  39,\n",
       "  7,\n",
       "  782,\n",
       "  47,\n",
       "  783,\n",
       "  483,\n",
       "  6,\n",
       "  236,\n",
       "  194,\n",
       "  4,\n",
       "  131,\n",
       "  5,\n",
       "  784,\n",
       "  19,\n",
       "  254,\n",
       "  785,\n",
       "  141,\n",
       "  61,\n",
       "  9,\n",
       "  100,\n",
       "  65,\n",
       "  7,\n",
       "  110,\n",
       "  786,\n",
       "  101,\n",
       "  317,\n",
       "  88,\n",
       "  8,\n",
       "  117,\n",
       "  2,\n",
       "  328,\n",
       "  146],\n",
       " [1,\n",
       "  27,\n",
       "  39,\n",
       "  7,\n",
       "  782,\n",
       "  47,\n",
       "  783,\n",
       "  483,\n",
       "  6,\n",
       "  236,\n",
       "  194,\n",
       "  4,\n",
       "  131,\n",
       "  5,\n",
       "  784,\n",
       "  19,\n",
       "  254,\n",
       "  785,\n",
       "  141,\n",
       "  61,\n",
       "  9,\n",
       "  100,\n",
       "  65,\n",
       "  7,\n",
       "  110,\n",
       "  786,\n",
       "  101,\n",
       "  317,\n",
       "  88,\n",
       "  8,\n",
       "  117,\n",
       "  2,\n",
       "  328,\n",
       "  146,\n",
       "  40],\n",
       " [325, 6],\n",
       " [325, 6, 72],\n",
       " [325, 6, 72, 484],\n",
       " [325, 6, 72, 484, 18],\n",
       " [325, 6, 72, 484, 18, 787],\n",
       " [325, 6, 72, 484, 18, 787, 24],\n",
       " [325, 6, 72, 484, 18, 787, 24, 788],\n",
       " [325, 6, 72, 484, 18, 787, 24, 788, 2],\n",
       " [325, 6, 72, 484, 18, 787, 24, 788, 2, 3],\n",
       " [325, 6, 72, 484, 18, 787, 24, 788, 2, 3, 254],\n",
       " [325, 6, 72, 484, 18, 787, 24, 788, 2, 3, 254, 118],\n",
       " [325, 6, 72, 484, 18, 787, 24, 788, 2, 3, 254, 118, 2],\n",
       " [325, 6, 72, 484, 18, 787, 24, 788, 2, 3, 254, 118, 2, 170],\n",
       " [325, 6, 72, 484, 18, 787, 24, 788, 2, 3, 254, 118, 2, 170, 3],\n",
       " [325, 6, 72, 484, 18, 787, 24, 788, 2, 3, 254, 118, 2, 170, 3, 13],\n",
       " [325, 6, 72, 484, 18, 787, 24, 788, 2, 3, 254, 118, 2, 170, 3, 13, 22],\n",
       " [325, 6, 72, 484, 18, 787, 24, 788, 2, 3, 254, 118, 2, 170, 3, 13, 22, 329],\n",
       " [451, 145],\n",
       " [451, 145, 147],\n",
       " [451, 145, 147, 195],\n",
       " [451, 145, 147, 195, 2],\n",
       " [451, 145, 147, 195, 2, 330],\n",
       " [451, 145, 147, 195, 2, 330, 3],\n",
       " [451, 145, 147, 195, 2, 330, 3, 13],\n",
       " [451, 145, 147, 195, 2, 330, 3, 13, 9],\n",
       " [451, 145, 147, 195, 2, 330, 3, 13, 9, 66],\n",
       " [451, 145, 147, 195, 2, 330, 3, 13, 9, 66, 90],\n",
       " [451, 145, 147, 195, 2, 330, 3, 13, 9, 66, 90, 16],\n",
       " [451, 145, 147, 195, 2, 330, 3, 13, 9, 66, 90, 16, 3],\n",
       " [451, 145, 147, 195, 2, 330, 3, 13, 9, 66, 90, 16, 3, 789],\n",
       " [451, 145, 147, 195, 2, 330, 3, 13, 9, 66, 90, 16, 3, 789, 118],\n",
       " [451, 145, 147, 195, 2, 330, 3, 13, 9, 66, 90, 16, 3, 789, 118, 44],\n",
       " [451, 145, 147, 195, 2, 330, 3, 13, 9, 66, 90, 16, 3, 789, 118, 44, 18],\n",
       " [451, 145, 147, 195, 2, 330, 3, 13, 9, 66, 90, 16, 3, 789, 118, 44, 18, 111],\n",
       " [451,\n",
       "  145,\n",
       "  147,\n",
       "  195,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  13,\n",
       "  9,\n",
       "  66,\n",
       "  90,\n",
       "  16,\n",
       "  3,\n",
       "  789,\n",
       "  118,\n",
       "  44,\n",
       "  18,\n",
       "  111,\n",
       "  2],\n",
       " [451,\n",
       "  145,\n",
       "  147,\n",
       "  195,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  13,\n",
       "  9,\n",
       "  66,\n",
       "  90,\n",
       "  16,\n",
       "  3,\n",
       "  789,\n",
       "  118,\n",
       "  44,\n",
       "  18,\n",
       "  111,\n",
       "  2,\n",
       "  128],\n",
       " [451,\n",
       "  145,\n",
       "  147,\n",
       "  195,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  13,\n",
       "  9,\n",
       "  66,\n",
       "  90,\n",
       "  16,\n",
       "  3,\n",
       "  789,\n",
       "  118,\n",
       "  44,\n",
       "  18,\n",
       "  111,\n",
       "  2,\n",
       "  128,\n",
       "  119],\n",
       " [451,\n",
       "  145,\n",
       "  147,\n",
       "  195,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  13,\n",
       "  9,\n",
       "  66,\n",
       "  90,\n",
       "  16,\n",
       "  3,\n",
       "  789,\n",
       "  118,\n",
       "  44,\n",
       "  18,\n",
       "  111,\n",
       "  2,\n",
       "  128,\n",
       "  119,\n",
       "  7],\n",
       " [451,\n",
       "  145,\n",
       "  147,\n",
       "  195,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  13,\n",
       "  9,\n",
       "  66,\n",
       "  90,\n",
       "  16,\n",
       "  3,\n",
       "  789,\n",
       "  118,\n",
       "  44,\n",
       "  18,\n",
       "  111,\n",
       "  2,\n",
       "  128,\n",
       "  119,\n",
       "  7,\n",
       "  67],\n",
       " [451,\n",
       "  145,\n",
       "  147,\n",
       "  195,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  13,\n",
       "  9,\n",
       "  66,\n",
       "  90,\n",
       "  16,\n",
       "  3,\n",
       "  789,\n",
       "  118,\n",
       "  44,\n",
       "  18,\n",
       "  111,\n",
       "  2,\n",
       "  128,\n",
       "  119,\n",
       "  7,\n",
       "  67,\n",
       "  120],\n",
       " [451,\n",
       "  145,\n",
       "  147,\n",
       "  195,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  13,\n",
       "  9,\n",
       "  66,\n",
       "  90,\n",
       "  16,\n",
       "  3,\n",
       "  789,\n",
       "  118,\n",
       "  44,\n",
       "  18,\n",
       "  111,\n",
       "  2,\n",
       "  128,\n",
       "  119,\n",
       "  7,\n",
       "  67,\n",
       "  120,\n",
       "  17],\n",
       " [451,\n",
       "  145,\n",
       "  147,\n",
       "  195,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  13,\n",
       "  9,\n",
       "  66,\n",
       "  90,\n",
       "  16,\n",
       "  3,\n",
       "  789,\n",
       "  118,\n",
       "  44,\n",
       "  18,\n",
       "  111,\n",
       "  2,\n",
       "  128,\n",
       "  119,\n",
       "  7,\n",
       "  67,\n",
       "  120,\n",
       "  17,\n",
       "  16],\n",
       " [451,\n",
       "  145,\n",
       "  147,\n",
       "  195,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  13,\n",
       "  9,\n",
       "  66,\n",
       "  90,\n",
       "  16,\n",
       "  3,\n",
       "  789,\n",
       "  118,\n",
       "  44,\n",
       "  18,\n",
       "  111,\n",
       "  2,\n",
       "  128,\n",
       "  119,\n",
       "  7,\n",
       "  67,\n",
       "  120,\n",
       "  17,\n",
       "  16,\n",
       "  36],\n",
       " [451,\n",
       "  145,\n",
       "  147,\n",
       "  195,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  13,\n",
       "  9,\n",
       "  66,\n",
       "  90,\n",
       "  16,\n",
       "  3,\n",
       "  789,\n",
       "  118,\n",
       "  44,\n",
       "  18,\n",
       "  111,\n",
       "  2,\n",
       "  128,\n",
       "  119,\n",
       "  7,\n",
       "  67,\n",
       "  120,\n",
       "  17,\n",
       "  16,\n",
       "  36,\n",
       "  255],\n",
       " [451,\n",
       "  145,\n",
       "  147,\n",
       "  195,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  13,\n",
       "  9,\n",
       "  66,\n",
       "  90,\n",
       "  16,\n",
       "  3,\n",
       "  789,\n",
       "  118,\n",
       "  44,\n",
       "  18,\n",
       "  111,\n",
       "  2,\n",
       "  128,\n",
       "  119,\n",
       "  7,\n",
       "  67,\n",
       "  120,\n",
       "  17,\n",
       "  16,\n",
       "  36,\n",
       "  255,\n",
       "  141],\n",
       " [451,\n",
       "  145,\n",
       "  147,\n",
       "  195,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  13,\n",
       "  9,\n",
       "  66,\n",
       "  90,\n",
       "  16,\n",
       "  3,\n",
       "  789,\n",
       "  118,\n",
       "  44,\n",
       "  18,\n",
       "  111,\n",
       "  2,\n",
       "  128,\n",
       "  119,\n",
       "  7,\n",
       "  67,\n",
       "  120,\n",
       "  17,\n",
       "  16,\n",
       "  36,\n",
       "  255,\n",
       "  141,\n",
       "  196],\n",
       " [451,\n",
       "  145,\n",
       "  147,\n",
       "  195,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  13,\n",
       "  9,\n",
       "  66,\n",
       "  90,\n",
       "  16,\n",
       "  3,\n",
       "  789,\n",
       "  118,\n",
       "  44,\n",
       "  18,\n",
       "  111,\n",
       "  2,\n",
       "  128,\n",
       "  119,\n",
       "  7,\n",
       "  67,\n",
       "  120,\n",
       "  17,\n",
       "  16,\n",
       "  36,\n",
       "  255,\n",
       "  141,\n",
       "  196,\n",
       "  118],\n",
       " [53, 62],\n",
       " [53, 62, 113],\n",
       " [53, 62, 113, 790],\n",
       " [53, 62, 113, 790, 6],\n",
       " [53, 62, 113, 790, 6, 328],\n",
       " [53, 62, 113, 790, 6, 328, 146],\n",
       " [53, 62, 113, 790, 6, 328, 146, 144],\n",
       " [53, 62, 113, 790, 6, 328, 146, 144, 1],\n",
       " [53, 62, 113, 790, 6, 328, 146, 144, 1, 331],\n",
       " [53, 62, 113, 790, 6, 328, 146, 144, 1, 331, 791],\n",
       " [53, 62, 113, 790, 6, 328, 146, 144, 1, 331, 791, 37],\n",
       " [53, 62, 113, 790, 6, 328, 146, 144, 1, 331, 791, 37, 6],\n",
       " [53, 62, 113, 790, 6, 328, 146, 144, 1, 331, 791, 37, 6, 88],\n",
       " [53, 62, 113, 790, 6, 328, 146, 144, 1, 331, 791, 37, 6, 88, 17],\n",
       " [53, 62, 113, 790, 6, 328, 146, 144, 1, 331, 791, 37, 6, 88, 17, 66],\n",
       " [53, 62, 113, 790, 6, 328, 146, 144, 1, 331, 791, 37, 6, 88, 17, 66, 792],\n",
       " [53, 62, 113, 790, 6, 328, 146, 144, 1, 331, 791, 37, 6, 88, 17, 66, 792, 2],\n",
       " [53,\n",
       "  62,\n",
       "  113,\n",
       "  790,\n",
       "  6,\n",
       "  328,\n",
       "  146,\n",
       "  144,\n",
       "  1,\n",
       "  331,\n",
       "  791,\n",
       "  37,\n",
       "  6,\n",
       "  88,\n",
       "  17,\n",
       "  66,\n",
       "  792,\n",
       "  2,\n",
       "  793],\n",
       " [53,\n",
       "  62,\n",
       "  113,\n",
       "  790,\n",
       "  6,\n",
       "  328,\n",
       "  146,\n",
       "  144,\n",
       "  1,\n",
       "  331,\n",
       "  791,\n",
       "  37,\n",
       "  6,\n",
       "  88,\n",
       "  17,\n",
       "  66,\n",
       "  792,\n",
       "  2,\n",
       "  793,\n",
       "  68],\n",
       " [53,\n",
       "  62,\n",
       "  113,\n",
       "  790,\n",
       "  6,\n",
       "  328,\n",
       "  146,\n",
       "  144,\n",
       "  1,\n",
       "  331,\n",
       "  791,\n",
       "  37,\n",
       "  6,\n",
       "  88,\n",
       "  17,\n",
       "  66,\n",
       "  792,\n",
       "  2,\n",
       "  793,\n",
       "  68,\n",
       "  69],\n",
       " [53,\n",
       "  62,\n",
       "  113,\n",
       "  790,\n",
       "  6,\n",
       "  328,\n",
       "  146,\n",
       "  144,\n",
       "  1,\n",
       "  331,\n",
       "  791,\n",
       "  37,\n",
       "  6,\n",
       "  88,\n",
       "  17,\n",
       "  66,\n",
       "  792,\n",
       "  2,\n",
       "  793,\n",
       "  68,\n",
       "  69,\n",
       "  167],\n",
       " [485, 148],\n",
       " [485, 148, 333],\n",
       " [485, 148, 333, 4],\n",
       " [485, 148, 333, 4, 794],\n",
       " [485, 148, 333, 4, 794, 71],\n",
       " [8, 117],\n",
       " [8, 117, 1],\n",
       " [8, 117, 1, 68],\n",
       " [8, 117, 1, 68, 795],\n",
       " [8, 117, 1, 68, 795, 486],\n",
       " [8, 117, 1, 68, 795, 486, 1],\n",
       " [8, 117, 1, 68, 795, 486, 1, 91],\n",
       " [8, 117, 1, 68, 795, 486, 1, 91, 90],\n",
       " [8, 117, 1, 68, 795, 486, 1, 91, 90, 70],\n",
       " [8, 117, 1, 68, 795, 486, 1, 91, 90, 70, 6],\n",
       " [8, 117, 1, 68, 795, 486, 1, 91, 90, 70, 6, 796],\n",
       " [8, 117, 1, 68, 795, 486, 1, 91, 90, 70, 6, 796, 797],\n",
       " [8, 117, 1, 68, 795, 486, 1, 91, 90, 70, 6, 796, 797, 5],\n",
       " [8, 117, 1, 68, 795, 486, 1, 91, 90, 70, 6, 796, 797, 5, 6],\n",
       " [8, 117, 1, 68, 795, 486, 1, 91, 90, 70, 6, 796, 797, 5, 6, 798],\n",
       " [8, 117, 1, 68, 795, 486, 1, 91, 90, 70, 6, 796, 797, 5, 6, 798, 799],\n",
       " [334, 119],\n",
       " [334, 119, 6],\n",
       " [334, 119, 6, 800],\n",
       " [334, 119, 6, 800, 74],\n",
       " [334, 119, 6, 800, 74, 801],\n",
       " [334, 119, 6, 800, 74, 801, 477],\n",
       " [334, 119, 6, 800, 74, 801, 477, 256],\n",
       " [334, 119, 6, 800, 74, 801, 477, 256, 802],\n",
       " [334, 119, 6, 800, 74, 801, 477, 256, 802, 33],\n",
       " [334, 119, 6, 800, 74, 801, 477, 256, 802, 33, 12],\n",
       " [334, 119, 6, 800, 74, 801, 477, 256, 802, 33, 12, 9],\n",
       " [334, 119, 6, 800, 74, 801, 477, 256, 802, 33, 12, 9, 335],\n",
       " [334, 119, 6, 800, 74, 801, 477, 256, 802, 33, 12, 9, 335, 35],\n",
       " [334, 119, 6, 800, 74, 801, 477, 256, 802, 33, 12, 9, 335, 35, 90],\n",
       " [334, 119, 6, 800, 74, 801, 477, 256, 802, 33, 12, 9, 335, 35, 90, 5],\n",
       " [334, 119, 6, 800, 74, 801, 477, 256, 802, 33, 12, 9, 335, 35, 90, 5, 67],\n",
       " [334,\n",
       "  119,\n",
       "  6,\n",
       "  800,\n",
       "  74,\n",
       "  801,\n",
       "  477,\n",
       "  256,\n",
       "  802,\n",
       "  33,\n",
       "  12,\n",
       "  9,\n",
       "  335,\n",
       "  35,\n",
       "  90,\n",
       "  5,\n",
       "  67,\n",
       "  197],\n",
       " [334,\n",
       "  119,\n",
       "  6,\n",
       "  800,\n",
       "  74,\n",
       "  801,\n",
       "  477,\n",
       "  256,\n",
       "  802,\n",
       "  33,\n",
       "  12,\n",
       "  9,\n",
       "  335,\n",
       "  35,\n",
       "  90,\n",
       "  5,\n",
       "  67,\n",
       "  197,\n",
       "  8],\n",
       " [334,\n",
       "  119,\n",
       "  6,\n",
       "  800,\n",
       "  74,\n",
       "  801,\n",
       "  477,\n",
       "  256,\n",
       "  802,\n",
       "  33,\n",
       "  12,\n",
       "  9,\n",
       "  335,\n",
       "  35,\n",
       "  90,\n",
       "  5,\n",
       "  67,\n",
       "  197,\n",
       "  8,\n",
       "  3],\n",
       " [334,\n",
       "  119,\n",
       "  6,\n",
       "  800,\n",
       "  74,\n",
       "  801,\n",
       "  477,\n",
       "  256,\n",
       "  802,\n",
       "  33,\n",
       "  12,\n",
       "  9,\n",
       "  335,\n",
       "  35,\n",
       "  90,\n",
       "  5,\n",
       "  67,\n",
       "  197,\n",
       "  8,\n",
       "  3,\n",
       "  487],\n",
       " [334,\n",
       "  119,\n",
       "  6,\n",
       "  800,\n",
       "  74,\n",
       "  801,\n",
       "  477,\n",
       "  256,\n",
       "  802,\n",
       "  33,\n",
       "  12,\n",
       "  9,\n",
       "  335,\n",
       "  35,\n",
       "  90,\n",
       "  5,\n",
       "  67,\n",
       "  197,\n",
       "  8,\n",
       "  3,\n",
       "  487,\n",
       "  4],\n",
       " [334,\n",
       "  119,\n",
       "  6,\n",
       "  800,\n",
       "  74,\n",
       "  801,\n",
       "  477,\n",
       "  256,\n",
       "  802,\n",
       "  33,\n",
       "  12,\n",
       "  9,\n",
       "  335,\n",
       "  35,\n",
       "  90,\n",
       "  5,\n",
       "  67,\n",
       "  197,\n",
       "  8,\n",
       "  3,\n",
       "  487,\n",
       "  4,\n",
       "  88],\n",
       " [334,\n",
       "  119,\n",
       "  6,\n",
       "  800,\n",
       "  74,\n",
       "  801,\n",
       "  477,\n",
       "  256,\n",
       "  802,\n",
       "  33,\n",
       "  12,\n",
       "  9,\n",
       "  335,\n",
       "  35,\n",
       "  90,\n",
       "  5,\n",
       "  67,\n",
       "  197,\n",
       "  8,\n",
       "  3,\n",
       "  487,\n",
       "  4,\n",
       "  88,\n",
       "  40],\n",
       " [334,\n",
       "  119,\n",
       "  6,\n",
       "  800,\n",
       "  74,\n",
       "  801,\n",
       "  477,\n",
       "  256,\n",
       "  802,\n",
       "  33,\n",
       "  12,\n",
       "  9,\n",
       "  335,\n",
       "  35,\n",
       "  90,\n",
       "  5,\n",
       "  67,\n",
       "  197,\n",
       "  8,\n",
       "  3,\n",
       "  487,\n",
       "  4,\n",
       "  88,\n",
       "  40,\n",
       "  10],\n",
       " [334,\n",
       "  119,\n",
       "  6,\n",
       "  800,\n",
       "  74,\n",
       "  801,\n",
       "  477,\n",
       "  256,\n",
       "  802,\n",
       "  33,\n",
       "  12,\n",
       "  9,\n",
       "  335,\n",
       "  35,\n",
       "  90,\n",
       "  5,\n",
       "  67,\n",
       "  197,\n",
       "  8,\n",
       "  3,\n",
       "  487,\n",
       "  4,\n",
       "  88,\n",
       "  40,\n",
       "  10,\n",
       "  803],\n",
       " [334,\n",
       "  119,\n",
       "  6,\n",
       "  800,\n",
       "  74,\n",
       "  801,\n",
       "  477,\n",
       "  256,\n",
       "  802,\n",
       "  33,\n",
       "  12,\n",
       "  9,\n",
       "  335,\n",
       "  35,\n",
       "  90,\n",
       "  5,\n",
       "  67,\n",
       "  197,\n",
       "  8,\n",
       "  3,\n",
       "  487,\n",
       "  4,\n",
       "  88,\n",
       "  40,\n",
       "  10,\n",
       "  803,\n",
       "  804],\n",
       " [334,\n",
       "  119,\n",
       "  6,\n",
       "  800,\n",
       "  74,\n",
       "  801,\n",
       "  477,\n",
       "  256,\n",
       "  802,\n",
       "  33,\n",
       "  12,\n",
       "  9,\n",
       "  335,\n",
       "  35,\n",
       "  90,\n",
       "  5,\n",
       "  67,\n",
       "  197,\n",
       "  8,\n",
       "  3,\n",
       "  487,\n",
       "  4,\n",
       "  88,\n",
       "  40,\n",
       "  10,\n",
       "  803,\n",
       "  804,\n",
       "  1],\n",
       " [334,\n",
       "  119,\n",
       "  6,\n",
       "  800,\n",
       "  74,\n",
       "  801,\n",
       "  477,\n",
       "  256,\n",
       "  802,\n",
       "  33,\n",
       "  12,\n",
       "  9,\n",
       "  335,\n",
       "  35,\n",
       "  90,\n",
       "  5,\n",
       "  67,\n",
       "  197,\n",
       "  8,\n",
       "  3,\n",
       "  487,\n",
       "  4,\n",
       "  88,\n",
       "  40,\n",
       "  10,\n",
       "  803,\n",
       "  804,\n",
       "  1,\n",
       "  488],\n",
       " [325, 13],\n",
       " [325, 13, 805],\n",
       " [325, 13, 805, 43],\n",
       " [325, 13, 805, 43, 171],\n",
       " [325, 13, 805, 43, 171, 489],\n",
       " [325, 13, 805, 43, 171, 489, 24],\n",
       " [325, 13, 805, 43, 171, 489, 24, 76],\n",
       " [325, 13, 805, 43, 171, 489, 24, 76, 113],\n",
       " [325, 13, 805, 43, 171, 489, 24, 76, 113, 3],\n",
       " [325, 13, 805, 43, 171, 489, 24, 76, 113, 3, 488],\n",
       " [325, 13, 805, 43, 171, 489, 24, 76, 113, 3, 488, 806],\n",
       " [6, 1],\n",
       " [6, 1, 331],\n",
       " [6, 1, 331, 807],\n",
       " [6, 1, 331, 807, 90],\n",
       " [6, 1, 331, 807, 90, 12],\n",
       " [6, 1, 331, 807, 90, 12, 127],\n",
       " [6, 1, 331, 807, 90, 12, 127, 130],\n",
       " [6, 1, 331, 807, 90, 12, 127, 130, 808],\n",
       " [6, 1, 331, 807, 90, 12, 127, 130, 808, 809],\n",
       " [6, 1, 331, 807, 90, 12, 127, 130, 808, 809, 61],\n",
       " [6, 1, 331, 807, 90, 12, 127, 130, 808, 809, 61, 17],\n",
       " [6, 1, 331, 807, 90, 12, 127, 130, 808, 809, 61, 17, 490],\n",
       " [6, 1, 331, 807, 90, 12, 127, 130, 808, 809, 61, 17, 490, 336],\n",
       " [6, 1, 331, 807, 90, 12, 127, 130, 808, 809, 61, 17, 490, 336, 2],\n",
       " [6, 1, 331, 807, 90, 12, 127, 130, 808, 809, 61, 17, 490, 336, 2, 185],\n",
       " [6, 1, 331, 807, 90, 12, 127, 130, 808, 809, 61, 17, 490, 336, 2, 185, 198],\n",
       " [6,\n",
       "  1,\n",
       "  331,\n",
       "  807,\n",
       "  90,\n",
       "  12,\n",
       "  127,\n",
       "  130,\n",
       "  808,\n",
       "  809,\n",
       "  61,\n",
       "  17,\n",
       "  490,\n",
       "  336,\n",
       "  2,\n",
       "  185,\n",
       "  198,\n",
       "  18],\n",
       " [6,\n",
       "  1,\n",
       "  331,\n",
       "  807,\n",
       "  90,\n",
       "  12,\n",
       "  127,\n",
       "  130,\n",
       "  808,\n",
       "  809,\n",
       "  61,\n",
       "  17,\n",
       "  490,\n",
       "  336,\n",
       "  2,\n",
       "  185,\n",
       "  198,\n",
       "  18,\n",
       "  810],\n",
       " [811, 812],\n",
       " [811, 812, 491],\n",
       " [811, 812, 491, 9],\n",
       " [811, 812, 491, 9, 337],\n",
       " [811, 812, 491, 9, 337, 813],\n",
       " [811, 812, 491, 9, 337, 813, 814],\n",
       " [811, 812, 491, 9, 337, 813, 814, 48],\n",
       " [811, 812, 491, 9, 337, 813, 814, 48, 185],\n",
       " [811, 812, 491, 9, 337, 813, 814, 48, 185, 338],\n",
       " [811, 812, 491, 9, 337, 813, 814, 48, 185, 338, 12],\n",
       " [811, 812, 491, 9, 337, 813, 814, 48, 185, 338, 12, 5],\n",
       " [811, 812, 491, 9, 337, 813, 814, 48, 185, 338, 12, 5, 815],\n",
       " [811, 812, 491, 9, 337, 813, 814, 48, 185, 338, 12, 5, 815, 816],\n",
       " [5, 121],\n",
       " [5, 121, 18],\n",
       " [5, 121, 18, 817],\n",
       " [5, 121, 18, 817, 185],\n",
       " [5, 121, 18, 817, 185, 3],\n",
       " [5, 121, 18, 817, 185, 3, 13],\n",
       " [5, 121, 18, 817, 185, 3, 13, 818],\n",
       " [5, 121, 18, 817, 185, 3, 13, 818, 192],\n",
       " [5, 121, 18, 817, 185, 3, 13, 818, 192, 75],\n",
       " [5, 121, 18, 817, 185, 3, 13, 818, 192, 75, 2],\n",
       " [5, 121, 18, 817, 185, 3, 13, 818, 192, 75, 2, 67],\n",
       " [5, 121, 18, 817, 185, 3, 13, 818, 192, 75, 2, 67, 120],\n",
       " [5, 121, 18, 817, 185, 3, 13, 818, 192, 75, 2, 67, 120, 17],\n",
       " [5, 121, 18, 817, 185, 3, 13, 818, 192, 75, 2, 67, 120, 17, 490],\n",
       " [5, 121, 18, 817, 185, 3, 13, 818, 192, 75, 2, 67, 120, 17, 490, 492],\n",
       " [5, 121, 18, 817, 185, 3, 13, 818, 192, 75, 2, 67, 120, 17, 490, 492, 336],\n",
       " [10, 7],\n",
       " [10, 7, 92],\n",
       " [10, 7, 92, 171],\n",
       " [10, 7, 92, 171, 489],\n",
       " [10, 7, 92, 171, 489, 49],\n",
       " [10, 7, 92, 171, 489, 49, 199],\n",
       " [10, 7, 92, 171, 489, 49, 199, 493],\n",
       " [10, 7, 92, 171, 489, 49, 199, 493, 6],\n",
       " [10, 7, 92, 171, 489, 49, 199, 493, 6, 145],\n",
       " [10, 7, 92, 171, 489, 49, 199, 493, 6, 145, 339],\n",
       " [10, 7, 92, 171, 489, 49, 199, 493, 6, 145, 339, 340],\n",
       " [10, 7, 92, 171, 489, 49, 199, 493, 6, 145, 339, 340, 17],\n",
       " [10, 7, 92, 171, 489, 49, 199, 493, 6, 145, 339, 340, 17, 819],\n",
       " [10, 7, 92, 171, 489, 49, 199, 493, 6, 145, 339, 340, 17, 819, 172],\n",
       " [10, 7, 92, 171, 489, 49, 199, 493, 6, 145, 339, 340, 17, 819, 172, 338],\n",
       " [10, 7, 92, 171, 489, 49, 199, 493, 6, 145, 339, 340, 17, 819, 172, 338, 5],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4,\n",
       "  1],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  90],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  90,\n",
       "  13],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  90,\n",
       "  13,\n",
       "  22],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  90,\n",
       "  13,\n",
       "  22,\n",
       "  1],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  90,\n",
       "  13,\n",
       "  22,\n",
       "  1,\n",
       "  171],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  90,\n",
       "  13,\n",
       "  22,\n",
       "  1,\n",
       "  171,\n",
       "  341],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  90,\n",
       "  13,\n",
       "  22,\n",
       "  1,\n",
       "  171,\n",
       "  341,\n",
       "  201],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  90,\n",
       "  13,\n",
       "  22,\n",
       "  1,\n",
       "  171,\n",
       "  341,\n",
       "  201,\n",
       "  820],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  90,\n",
       "  13,\n",
       "  22,\n",
       "  1,\n",
       "  171,\n",
       "  341,\n",
       "  201,\n",
       "  820,\n",
       "  17],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  90,\n",
       "  13,\n",
       "  22,\n",
       "  1,\n",
       "  171,\n",
       "  341,\n",
       "  201,\n",
       "  820,\n",
       "  17,\n",
       "  8],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  90,\n",
       "  13,\n",
       "  22,\n",
       "  1,\n",
       "  171,\n",
       "  341,\n",
       "  201,\n",
       "  820,\n",
       "  17,\n",
       "  8,\n",
       "  36],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  90,\n",
       "  13,\n",
       "  22,\n",
       "  1,\n",
       "  171,\n",
       "  341,\n",
       "  201,\n",
       "  820,\n",
       "  17,\n",
       "  8,\n",
       "  36,\n",
       "  103],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  90,\n",
       "  13,\n",
       "  22,\n",
       "  1,\n",
       "  171,\n",
       "  341,\n",
       "  201,\n",
       "  820,\n",
       "  17,\n",
       "  8,\n",
       "  36,\n",
       "  103,\n",
       "  67],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  90,\n",
       "  13,\n",
       "  22,\n",
       "  1,\n",
       "  171,\n",
       "  341,\n",
       "  201,\n",
       "  820,\n",
       "  17,\n",
       "  8,\n",
       "  36,\n",
       "  103,\n",
       "  67,\n",
       "  120],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  90,\n",
       "  13,\n",
       "  22,\n",
       "  1,\n",
       "  171,\n",
       "  341,\n",
       "  201,\n",
       "  820,\n",
       "  17,\n",
       "  8,\n",
       "  36,\n",
       "  103,\n",
       "  67,\n",
       "  120,\n",
       "  17],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  90,\n",
       "  13,\n",
       "  22,\n",
       "  1,\n",
       "  171,\n",
       "  341,\n",
       "  201,\n",
       "  820,\n",
       "  17,\n",
       "  8,\n",
       "  36,\n",
       "  103,\n",
       "  67,\n",
       "  120,\n",
       "  17,\n",
       "  16],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  90,\n",
       "  13,\n",
       "  22,\n",
       "  1,\n",
       "  171,\n",
       "  341,\n",
       "  201,\n",
       "  820,\n",
       "  17,\n",
       "  8,\n",
       "  36,\n",
       "  103,\n",
       "  67,\n",
       "  120,\n",
       "  17,\n",
       "  16,\n",
       "  36],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  90,\n",
       "  13,\n",
       "  22,\n",
       "  1,\n",
       "  171,\n",
       "  341,\n",
       "  201,\n",
       "  820,\n",
       "  17,\n",
       "  8,\n",
       "  36,\n",
       "  103,\n",
       "  67,\n",
       "  120,\n",
       "  17,\n",
       "  16,\n",
       "  36,\n",
       "  118],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  90,\n",
       "  13,\n",
       "  22,\n",
       "  1,\n",
       "  171,\n",
       "  341,\n",
       "  201,\n",
       "  820,\n",
       "  17,\n",
       "  8,\n",
       "  36,\n",
       "  103,\n",
       "  67,\n",
       "  120,\n",
       "  17,\n",
       "  16,\n",
       "  36,\n",
       "  118,\n",
       "  5],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  90,\n",
       "  13,\n",
       "  22,\n",
       "  1,\n",
       "  171,\n",
       "  341,\n",
       "  201,\n",
       "  820,\n",
       "  17,\n",
       "  8,\n",
       "  36,\n",
       "  103,\n",
       "  67,\n",
       "  120,\n",
       "  17,\n",
       "  16,\n",
       "  36,\n",
       "  118,\n",
       "  5,\n",
       "  150],\n",
       " [10,\n",
       "  7,\n",
       "  92,\n",
       "  171,\n",
       "  489,\n",
       "  49,\n",
       "  199,\n",
       "  493,\n",
       "  6,\n",
       "  145,\n",
       "  339,\n",
       "  340,\n",
       "  17,\n",
       "  819,\n",
       "  172,\n",
       "  338,\n",
       "  5,\n",
       "  337,\n",
       "  5,\n",
       "  17,\n",
       "  149,\n",
       "  17,\n",
       "  336,\n",
       "  2,\n",
       "  330,\n",
       "  3,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  90,\n",
       "  13,\n",
       "  22,\n",
       "  1,\n",
       "  171,\n",
       "  341,\n",
       "  201,\n",
       "  820,\n",
       "  17,\n",
       "  8,\n",
       "  36,\n",
       "  103,\n",
       "  67,\n",
       "  120,\n",
       "  17,\n",
       "  16,\n",
       "  36,\n",
       "  118,\n",
       "  5,\n",
       "  150,\n",
       "  17],\n",
       " [65, 4],\n",
       " [65, 4, 1],\n",
       " [65, 4, 1, 199],\n",
       " [65, 4, 1, 199, 7],\n",
       " [65, 4, 1, 199, 7, 494],\n",
       " [65, 4, 1, 199, 7, 494, 495],\n",
       " [65, 4, 1, 199, 7, 494, 495, 6],\n",
       " [65, 4, 1, 199, 7, 494, 495, 6, 821],\n",
       " [65, 4, 1, 199, 7, 494, 495, 6, 821, 446],\n",
       " [65, 4, 1, 199, 7, 494, 495, 6, 821, 446, 17],\n",
       " [65, 4, 1, 199, 7, 494, 495, 6, 821, 446, 17, 66],\n",
       " [65, 4, 1, 199, 7, 494, 495, 6, 821, 446, 17, 66, 54],\n",
       " [65, 4, 1, 199, 7, 494, 495, 6, 821, 446, 17, 66, 54, 21],\n",
       " [65, 4, 1, 199, 7, 494, 495, 6, 821, 446, 17, 66, 54, 21, 162],\n",
       " [65, 4, 1, 199, 7, 494, 495, 6, 821, 446, 17, 66, 54, 21, 162, 822],\n",
       " [65, 4, 1, 199, 7, 494, 495, 6, 821, 446, 17, 66, 54, 21, 162, 822, 823],\n",
       " [65, 4, 1, 199, 7, 494, 495, 6, 821, 446, 17, 66, 54, 21, 162, 822, 823, 496],\n",
       " [65,\n",
       "  4,\n",
       "  1,\n",
       "  199,\n",
       "  7,\n",
       "  494,\n",
       "  495,\n",
       "  6,\n",
       "  821,\n",
       "  446,\n",
       "  17,\n",
       "  66,\n",
       "  54,\n",
       "  21,\n",
       "  162,\n",
       "  822,\n",
       "  823,\n",
       "  496,\n",
       "  5],\n",
       " [65,\n",
       "  4,\n",
       "  1,\n",
       "  199,\n",
       "  7,\n",
       "  494,\n",
       "  495,\n",
       "  6,\n",
       "  821,\n",
       "  446,\n",
       "  17,\n",
       "  66,\n",
       "  54,\n",
       "  21,\n",
       "  162,\n",
       "  822,\n",
       "  823,\n",
       "  496,\n",
       "  5,\n",
       "  66],\n",
       " [65,\n",
       "  4,\n",
       "  1,\n",
       "  199,\n",
       "  7,\n",
       "  494,\n",
       "  495,\n",
       "  6,\n",
       "  821,\n",
       "  446,\n",
       "  17,\n",
       "  66,\n",
       "  54,\n",
       "  21,\n",
       "  162,\n",
       "  822,\n",
       "  823,\n",
       "  496,\n",
       "  5,\n",
       "  66,\n",
       "  824],\n",
       " [65,\n",
       "  4,\n",
       "  1,\n",
       "  199,\n",
       "  7,\n",
       "  494,\n",
       "  495,\n",
       "  6,\n",
       "  821,\n",
       "  446,\n",
       "  17,\n",
       "  66,\n",
       "  54,\n",
       "  21,\n",
       "  162,\n",
       "  822,\n",
       "  823,\n",
       "  496,\n",
       "  5,\n",
       "  66,\n",
       "  824,\n",
       "  77],\n",
       " [65,\n",
       "  4,\n",
       "  1,\n",
       "  199,\n",
       "  7,\n",
       "  494,\n",
       "  495,\n",
       "  6,\n",
       "  821,\n",
       "  446,\n",
       "  17,\n",
       "  66,\n",
       "  54,\n",
       "  21,\n",
       "  162,\n",
       "  822,\n",
       "  823,\n",
       "  496,\n",
       "  5,\n",
       "  66,\n",
       "  824,\n",
       "  77,\n",
       "  825],\n",
       " [65,\n",
       "  4,\n",
       "  1,\n",
       "  199,\n",
       "  7,\n",
       "  494,\n",
       "  495,\n",
       "  6,\n",
       "  821,\n",
       "  446,\n",
       "  17,\n",
       "  66,\n",
       "  54,\n",
       "  21,\n",
       "  162,\n",
       "  822,\n",
       "  823,\n",
       "  496,\n",
       "  5,\n",
       "  66,\n",
       "  824,\n",
       "  77,\n",
       "  825,\n",
       "  162],\n",
       " [65,\n",
       "  4,\n",
       "  1,\n",
       "  199,\n",
       "  7,\n",
       "  494,\n",
       "  495,\n",
       "  6,\n",
       "  821,\n",
       "  446,\n",
       "  17,\n",
       "  66,\n",
       "  54,\n",
       "  21,\n",
       "  162,\n",
       "  822,\n",
       "  823,\n",
       "  496,\n",
       "  5,\n",
       "  66,\n",
       "  824,\n",
       "  77,\n",
       "  825,\n",
       "  162,\n",
       "  497],\n",
       " [65,\n",
       "  4,\n",
       "  1,\n",
       "  199,\n",
       "  7,\n",
       "  494,\n",
       "  495,\n",
       "  6,\n",
       "  821,\n",
       "  446,\n",
       "  17,\n",
       "  66,\n",
       "  54,\n",
       "  21,\n",
       "  162,\n",
       "  822,\n",
       "  823,\n",
       "  496,\n",
       "  5,\n",
       "  66,\n",
       "  824,\n",
       "  77,\n",
       "  825,\n",
       "  162,\n",
       "  497,\n",
       "  826],\n",
       " [65,\n",
       "  4,\n",
       "  1,\n",
       "  199,\n",
       "  7,\n",
       "  494,\n",
       "  495,\n",
       "  6,\n",
       "  821,\n",
       "  446,\n",
       "  17,\n",
       "  66,\n",
       "  54,\n",
       "  21,\n",
       "  162,\n",
       "  822,\n",
       "  823,\n",
       "  496,\n",
       "  5,\n",
       "  66,\n",
       "  824,\n",
       "  77,\n",
       "  825,\n",
       "  162,\n",
       "  497,\n",
       "  826,\n",
       "  342],\n",
       " [65,\n",
       "  4,\n",
       "  1,\n",
       "  199,\n",
       "  7,\n",
       "  494,\n",
       "  495,\n",
       "  6,\n",
       "  821,\n",
       "  446,\n",
       "  17,\n",
       "  66,\n",
       "  54,\n",
       "  21,\n",
       "  162,\n",
       "  822,\n",
       "  823,\n",
       "  496,\n",
       "  5,\n",
       "  66,\n",
       "  824,\n",
       "  77,\n",
       "  825,\n",
       "  162,\n",
       "  497,\n",
       "  826,\n",
       "  342,\n",
       "  202],\n",
       " [65,\n",
       "  4,\n",
       "  1,\n",
       "  199,\n",
       "  7,\n",
       "  494,\n",
       "  495,\n",
       "  6,\n",
       "  821,\n",
       "  446,\n",
       "  17,\n",
       "  66,\n",
       "  54,\n",
       "  21,\n",
       "  162,\n",
       "  822,\n",
       "  823,\n",
       "  496,\n",
       "  5,\n",
       "  66,\n",
       "  824,\n",
       "  77,\n",
       "  825,\n",
       "  162,\n",
       "  497,\n",
       "  826,\n",
       "  342,\n",
       "  202,\n",
       "  827],\n",
       " [828, 1],\n",
       " [828, 1, 199],\n",
       " [828, 1, 199, 5],\n",
       " [828, 1, 199, 5, 100],\n",
       " [828, 1, 199, 5, 100, 498],\n",
       " [828, 1, 199, 5, 100, 498, 15],\n",
       " [828, 1, 199, 5, 100, 498, 15, 829],\n",
       " [828, 1, 199, 5, 100, 498, 15, 829, 317],\n",
       " [828, 1, 199, 5, 100, 498, 15, 829, 317, 88],\n",
       " [828, 1, 199, 5, 100, 498, 15, 829, 317, 88, 328],\n",
       " [828, 1, 199, 5, 100, 498, 15, 829, 317, 88, 328, 146],\n",
       " [828, 1, 199, 5, 100, 498, 15, 829, 317, 88, 328, 146, 12],\n",
       " [828, 1, 199, 5, 100, 498, 15, 829, 317, 88, 328, 146, 12, 15],\n",
       " [828, 1, 199, 5, 100, 498, 15, 829, 317, 88, 328, 146, 12, 15, 343],\n",
       " [828, 1, 199, 5, 100, 498, 15, 829, 317, 88, 328, 146, 12, 15, 343, 244],\n",
       " [18, 20],\n",
       " [18, 20, 76],\n",
       " [18, 20, 76, 330],\n",
       " [18, 20, 76, 330, 830],\n",
       " [18, 20, 76, 330, 830, 257],\n",
       " [18, 20, 76, 330, 830, 257, 22],\n",
       " [18, 20, 76, 330, 830, 257, 22, 1],\n",
       " [18, 20, 76, 330, 830, 257, 22, 1, 201],\n",
       " [18, 20, 76, 330, 830, 257, 22, 1, 201, 2],\n",
       " [18, 20, 76, 330, 830, 257, 22, 1, 201, 2, 170],\n",
       " [18, 20, 76, 330, 830, 257, 22, 1, 201, 2, 170, 34],\n",
       " [18, 20, 76, 330, 830, 257, 22, 1, 201, 2, 170, 34, 150],\n",
       " [18, 20, 76, 330, 830, 257, 22, 1, 201, 2, 170, 34, 150, 36],\n",
       " [18, 20, 76, 330, 830, 257, 22, 1, 201, 2, 170, 34, 150, 36, 12],\n",
       " [61, 198],\n",
       " [61, 198, 89],\n",
       " [61, 198, 89, 20],\n",
       " [61, 198, 89, 20, 18],\n",
       " [61, 198, 89, 20, 18, 469],\n",
       " [61, 198, 89, 20, 18, 469, 8],\n",
       " [61, 198, 89, 20, 18, 469, 8, 101],\n",
       " [61, 198, 89, 20, 18, 469, 8, 101, 10],\n",
       " [61, 198, 89, 20, 18, 469, 8, 101, 10, 186],\n",
       " [61, 198, 89, 20, 18, 469, 8, 101, 10, 186, 17],\n",
       " [61, 198, 89, 20, 18, 469, 8, 101, 10, 186, 17, 66],\n",
       " [61, 198, 89, 20, 18, 469, 8, 101, 10, 186, 17, 66, 314],\n",
       " [61, 198, 89, 20, 18, 469, 8, 101, 10, 186, 17, 66, 314, 21],\n",
       " [61, 198, 89, 20, 18, 469, 8, 101, 10, 186, 17, 66, 314, 21, 339],\n",
       " [61, 198, 89, 20, 18, 469, 8, 101, 10, 186, 17, 66, 314, 21, 339, 340],\n",
       " [61, 198, 89, 20, 18, 469, 8, 101, 10, 186, 17, 66, 314, 21, 339, 340, 499],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1,\n",
       "  832],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1,\n",
       "  832,\n",
       "  4],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1,\n",
       "  832,\n",
       "  4,\n",
       "  1],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1,\n",
       "  832,\n",
       "  4,\n",
       "  1,\n",
       "  49],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1,\n",
       "  832,\n",
       "  4,\n",
       "  1,\n",
       "  49,\n",
       "  199],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1,\n",
       "  832,\n",
       "  4,\n",
       "  1,\n",
       "  49,\n",
       "  199,\n",
       "  833],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1,\n",
       "  832,\n",
       "  4,\n",
       "  1,\n",
       "  49,\n",
       "  199,\n",
       "  833,\n",
       "  17],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1,\n",
       "  832,\n",
       "  4,\n",
       "  1,\n",
       "  49,\n",
       "  199,\n",
       "  833,\n",
       "  17,\n",
       "  834],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1,\n",
       "  832,\n",
       "  4,\n",
       "  1,\n",
       "  49,\n",
       "  199,\n",
       "  833,\n",
       "  17,\n",
       "  834,\n",
       "  1],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1,\n",
       "  832,\n",
       "  4,\n",
       "  1,\n",
       "  49,\n",
       "  199,\n",
       "  833,\n",
       "  17,\n",
       "  834,\n",
       "  1,\n",
       "  835],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1,\n",
       "  832,\n",
       "  4,\n",
       "  1,\n",
       "  49,\n",
       "  199,\n",
       "  833,\n",
       "  17,\n",
       "  834,\n",
       "  1,\n",
       "  835,\n",
       "  5],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1,\n",
       "  832,\n",
       "  4,\n",
       "  1,\n",
       "  49,\n",
       "  199,\n",
       "  833,\n",
       "  17,\n",
       "  834,\n",
       "  1,\n",
       "  835,\n",
       "  5,\n",
       "  836],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1,\n",
       "  832,\n",
       "  4,\n",
       "  1,\n",
       "  49,\n",
       "  199,\n",
       "  833,\n",
       "  17,\n",
       "  834,\n",
       "  1,\n",
       "  835,\n",
       "  5,\n",
       "  836,\n",
       "  4],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1,\n",
       "  832,\n",
       "  4,\n",
       "  1,\n",
       "  49,\n",
       "  199,\n",
       "  833,\n",
       "  17,\n",
       "  834,\n",
       "  1,\n",
       "  835,\n",
       "  5,\n",
       "  836,\n",
       "  4,\n",
       "  1],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1,\n",
       "  832,\n",
       "  4,\n",
       "  1,\n",
       "  49,\n",
       "  199,\n",
       "  833,\n",
       "  17,\n",
       "  834,\n",
       "  1,\n",
       "  835,\n",
       "  5,\n",
       "  836,\n",
       "  4,\n",
       "  1,\n",
       "  203],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1,\n",
       "  832,\n",
       "  4,\n",
       "  1,\n",
       "  49,\n",
       "  199,\n",
       "  833,\n",
       "  17,\n",
       "  834,\n",
       "  1,\n",
       "  835,\n",
       "  5,\n",
       "  836,\n",
       "  4,\n",
       "  1,\n",
       "  203,\n",
       "  18],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1,\n",
       "  832,\n",
       "  4,\n",
       "  1,\n",
       "  49,\n",
       "  199,\n",
       "  833,\n",
       "  17,\n",
       "  834,\n",
       "  1,\n",
       "  835,\n",
       "  5,\n",
       "  836,\n",
       "  4,\n",
       "  1,\n",
       "  203,\n",
       "  18,\n",
       "  46],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1,\n",
       "  832,\n",
       "  4,\n",
       "  1,\n",
       "  49,\n",
       "  199,\n",
       "  833,\n",
       "  17,\n",
       "  834,\n",
       "  1,\n",
       "  835,\n",
       "  5,\n",
       "  836,\n",
       "  4,\n",
       "  1,\n",
       "  203,\n",
       "  18,\n",
       "  46,\n",
       "  185],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1,\n",
       "  832,\n",
       "  4,\n",
       "  1,\n",
       "  49,\n",
       "  199,\n",
       "  833,\n",
       "  17,\n",
       "  834,\n",
       "  1,\n",
       "  835,\n",
       "  5,\n",
       "  836,\n",
       "  4,\n",
       "  1,\n",
       "  203,\n",
       "  18,\n",
       "  46,\n",
       "  185,\n",
       "  6],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1,\n",
       "  832,\n",
       "  4,\n",
       "  1,\n",
       "  49,\n",
       "  199,\n",
       "  833,\n",
       "  17,\n",
       "  834,\n",
       "  1,\n",
       "  835,\n",
       "  5,\n",
       "  836,\n",
       "  4,\n",
       "  1,\n",
       "  203,\n",
       "  18,\n",
       "  46,\n",
       "  185,\n",
       "  6,\n",
       "  26],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1,\n",
       "  832,\n",
       "  4,\n",
       "  1,\n",
       "  49,\n",
       "  199,\n",
       "  833,\n",
       "  17,\n",
       "  834,\n",
       "  1,\n",
       "  835,\n",
       "  5,\n",
       "  836,\n",
       "  4,\n",
       "  1,\n",
       "  203,\n",
       "  18,\n",
       "  46,\n",
       "  185,\n",
       "  6,\n",
       "  26,\n",
       "  837],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1,\n",
       "  832,\n",
       "  4,\n",
       "  1,\n",
       "  49,\n",
       "  199,\n",
       "  833,\n",
       "  17,\n",
       "  834,\n",
       "  1,\n",
       "  835,\n",
       "  5,\n",
       "  836,\n",
       "  4,\n",
       "  1,\n",
       "  203,\n",
       "  18,\n",
       "  46,\n",
       "  185,\n",
       "  6,\n",
       "  26,\n",
       "  837,\n",
       "  7],\n",
       " [61,\n",
       "  198,\n",
       "  89,\n",
       "  20,\n",
       "  18,\n",
       "  469,\n",
       "  8,\n",
       "  101,\n",
       "  10,\n",
       "  186,\n",
       "  17,\n",
       "  66,\n",
       "  314,\n",
       "  21,\n",
       "  339,\n",
       "  340,\n",
       "  499,\n",
       "  47,\n",
       "  171,\n",
       "  831,\n",
       "  1,\n",
       "  832,\n",
       "  4,\n",
       "  1,\n",
       "  49,\n",
       "  199,\n",
       "  833,\n",
       "  17,\n",
       "  834,\n",
       "  1,\n",
       "  835,\n",
       "  5,\n",
       "  836,\n",
       "  4,\n",
       "  1,\n",
       "  203,\n",
       "  18,\n",
       "  46,\n",
       "  185,\n",
       "  6,\n",
       "  26,\n",
       "  837,\n",
       "  7,\n",
       "  838],\n",
       " [17, 839],\n",
       " [17, 839, 500],\n",
       " [17, 839, 500, 22],\n",
       " [17, 839, 500, 22, 1],\n",
       " [17, 839, 500, 22, 1, 27],\n",
       " [17, 839, 500, 22, 1, 27, 39],\n",
       " [17, 839, 500, 22, 1, 27, 39, 344],\n",
       " [17, 839, 500, 22, 1, 27, 39, 344, 2],\n",
       " [17, 839, 500, 22, 1, 27, 39, 344, 2, 1],\n",
       " [17, 839, 500, 22, 1, 27, 39, 344, 2, 1, 49],\n",
       " [17, 839, 500, 22, 1, 27, 39, 344, 2, 1, 49, 199],\n",
       " [17, 839, 500, 22, 1, 27, 39, 344, 2, 1, 49, 199, 5],\n",
       " [17, 839, 500, 22, 1, 27, 39, 344, 2, 1, 49, 199, 5, 1],\n",
       " [17, 839, 500, 22, 1, 27, 39, 344, 2, 1, 49, 199, 5, 1, 840],\n",
       " [17, 839, 500, 22, 1, 27, 39, 344, 2, 1, 49, 199, 5, 1, 840, 498],\n",
       " [17, 839, 500, 22, 1, 27, 39, 344, 2, 1, 49, 199, 5, 1, 840, 498, 841],\n",
       " [17, 839, 500, 22, 1, 27, 39, 344, 2, 1, 49, 199, 5, 1, 840, 498, 841, 17],\n",
       " [167, 842],\n",
       " [167, 842, 843],\n",
       " [167, 842, 843, 1],\n",
       " [167, 842, 843, 1, 345],\n",
       " [167, 842, 843, 1, 345, 16],\n",
       " [167, 842, 843, 1, 345, 16, 104],\n",
       " [167, 842, 843, 1, 345, 16, 104, 18],\n",
       " [167, 842, 843, 1, 345, 16, 104, 18, 20],\n",
       " [167, 842, 843, 1, 345, 16, 104, 18, 20, 844],\n",
       " [167, 842, 843, 1, 345, 16, 104, 18, 20, 844, 845],\n",
       " [167, 842, 843, 1, 345, 16, 104, 18, 20, 844, 845, 6],\n",
       " [167, 842, 843, 1, 345, 16, 104, 18, 20, 844, 845, 6, 846],\n",
       " [167, 842, 843, 1, 345, 16, 104, 18, 20, 844, 845, 6, 846, 847],\n",
       " [167, 842, 843, 1, 345, 16, 104, 18, 20, 844, 845, 6, 846, 847, 5],\n",
       " [167, 842, 843, 1, 345, 16, 104, 18, 20, 844, 845, 6, 846, 847, 5, 44],\n",
       " [167, 842, 843, 1, 345, 16, 104, 18, 20, 844, 845, 6, 846, 847, 5, 44, 1],\n",
       " [167,\n",
       "  842,\n",
       "  843,\n",
       "  1,\n",
       "  345,\n",
       "  16,\n",
       "  104,\n",
       "  18,\n",
       "  20,\n",
       "  844,\n",
       "  845,\n",
       "  6,\n",
       "  846,\n",
       "  847,\n",
       "  5,\n",
       "  44,\n",
       "  1,\n",
       "  204],\n",
       " [167,\n",
       "  842,\n",
       "  843,\n",
       "  1,\n",
       "  345,\n",
       "  16,\n",
       "  104,\n",
       "  18,\n",
       "  20,\n",
       "  844,\n",
       "  845,\n",
       "  6,\n",
       "  846,\n",
       "  847,\n",
       "  5,\n",
       "  44,\n",
       "  1,\n",
       "  204,\n",
       "  346],\n",
       " [167,\n",
       "  842,\n",
       "  843,\n",
       "  1,\n",
       "  345,\n",
       "  16,\n",
       "  104,\n",
       "  18,\n",
       "  20,\n",
       "  844,\n",
       "  845,\n",
       "  6,\n",
       "  846,\n",
       "  847,\n",
       "  5,\n",
       "  44,\n",
       "  1,\n",
       "  204,\n",
       "  346,\n",
       "  15],\n",
       " [167,\n",
       "  842,\n",
       "  843,\n",
       "  1,\n",
       "  345,\n",
       "  16,\n",
       "  104,\n",
       "  18,\n",
       "  20,\n",
       "  844,\n",
       "  845,\n",
       "  6,\n",
       "  846,\n",
       "  847,\n",
       "  5,\n",
       "  44,\n",
       "  1,\n",
       "  204,\n",
       "  346,\n",
       "  15,\n",
       "  848],\n",
       " [167,\n",
       "  842,\n",
       "  843,\n",
       "  1,\n",
       "  345,\n",
       "  16,\n",
       "  104,\n",
       "  18,\n",
       "  20,\n",
       "  844,\n",
       "  845,\n",
       "  6,\n",
       "  846,\n",
       "  847,\n",
       "  5,\n",
       "  44,\n",
       "  1,\n",
       "  204,\n",
       "  346,\n",
       "  15,\n",
       "  848,\n",
       "  2],\n",
       " [167,\n",
       "  842,\n",
       "  843,\n",
       "  1,\n",
       "  345,\n",
       "  16,\n",
       "  104,\n",
       "  18,\n",
       "  20,\n",
       "  844,\n",
       "  845,\n",
       "  6,\n",
       "  846,\n",
       "  847,\n",
       "  5,\n",
       "  44,\n",
       "  1,\n",
       "  204,\n",
       "  346,\n",
       "  15,\n",
       "  848,\n",
       "  2,\n",
       "  1],\n",
       " [167,\n",
       "  842,\n",
       "  843,\n",
       "  1,\n",
       "  345,\n",
       "  16,\n",
       "  104,\n",
       "  18,\n",
       "  20,\n",
       "  844,\n",
       "  845,\n",
       "  6,\n",
       "  846,\n",
       "  847,\n",
       "  5,\n",
       "  44,\n",
       "  1,\n",
       "  204,\n",
       "  346,\n",
       "  15,\n",
       "  848,\n",
       "  2,\n",
       "  1,\n",
       "  849],\n",
       " [167,\n",
       "  842,\n",
       "  843,\n",
       "  1,\n",
       "  345,\n",
       "  16,\n",
       "  104,\n",
       "  18,\n",
       "  20,\n",
       "  844,\n",
       "  845,\n",
       "  6,\n",
       "  846,\n",
       "  847,\n",
       "  5,\n",
       "  44,\n",
       "  1,\n",
       "  204,\n",
       "  346,\n",
       "  15,\n",
       "  848,\n",
       "  2,\n",
       "  1,\n",
       "  849,\n",
       "  5],\n",
       " [167,\n",
       "  842,\n",
       "  843,\n",
       "  1,\n",
       "  345,\n",
       "  16,\n",
       "  104,\n",
       "  18,\n",
       "  20,\n",
       "  844,\n",
       "  845,\n",
       "  6,\n",
       "  846,\n",
       "  847,\n",
       "  5,\n",
       "  44,\n",
       "  1,\n",
       "  204,\n",
       "  346,\n",
       "  15,\n",
       "  848,\n",
       "  2,\n",
       "  1,\n",
       "  849,\n",
       "  5,\n",
       "  93],\n",
       " [167,\n",
       "  842,\n",
       "  843,\n",
       "  1,\n",
       "  345,\n",
       "  16,\n",
       "  104,\n",
       "  18,\n",
       "  20,\n",
       "  844,\n",
       "  845,\n",
       "  6,\n",
       "  846,\n",
       "  847,\n",
       "  5,\n",
       "  44,\n",
       "  1,\n",
       "  204,\n",
       "  346,\n",
       "  15,\n",
       "  848,\n",
       "  2,\n",
       "  1,\n",
       "  849,\n",
       "  5,\n",
       "  93,\n",
       "  2],\n",
       " [167,\n",
       "  842,\n",
       "  843,\n",
       "  1,\n",
       "  345,\n",
       "  16,\n",
       "  104,\n",
       "  18,\n",
       "  20,\n",
       "  844,\n",
       "  845,\n",
       "  6,\n",
       "  846,\n",
       "  847,\n",
       "  5,\n",
       "  44,\n",
       "  1,\n",
       "  204,\n",
       "  346,\n",
       "  15,\n",
       "  848,\n",
       "  2,\n",
       "  1,\n",
       "  849,\n",
       "  5,\n",
       "  93,\n",
       "  2,\n",
       "  248],\n",
       " [1, 850],\n",
       " [1, 850, 24],\n",
       " [1, 850, 24, 851],\n",
       " [1, 850, 24, 851, 852],\n",
       " [1, 850, 24, 851, 852, 6],\n",
       " [1, 850, 24, 851, 852, 6, 58],\n",
       " [1, 850, 24, 851, 852, 6, 58, 258],\n",
       " [1, 850, 24, 851, 852, 6, 58, 258, 132],\n",
       " [1, 850, 24, 851, 852, 6, 58, 258, 132, 27],\n",
       " [1, 850, 24, 851, 852, 6, 58, 258, 132, 27, 12],\n",
       " [1, 850, 24, 851, 852, 6, 58, 258, 132, 27, 12, 5],\n",
       " [1, 850, 24, 851, 852, 6, 58, 258, 132, 27, 12, 5, 60],\n",
       " [1, 850, 24, 851, 852, 6, 58, 258, 132, 27, 12, 5, 60, 501],\n",
       " [1, 850, 24, 851, 852, 6, 58, 258, 132, 27, 12, 5, 60, 501, 3],\n",
       " [1, 850, 24, 851, 852, 6, 58, 258, 132, 27, 12, 5, 60, 501, 3, 853],\n",
       " [1, 850, 24, 851, 852, 6, 58, 258, 132, 27, 12, 5, 60, 501, 3, 853, 4],\n",
       " [1, 850, 24, 851, 852, 6, 58, 258, 132, 27, 12, 5, 60, 501, 3, 853, 4, 854],\n",
       " [1,\n",
       "  850,\n",
       "  24,\n",
       "  851,\n",
       "  852,\n",
       "  6,\n",
       "  58,\n",
       "  258,\n",
       "  132,\n",
       "  27,\n",
       "  12,\n",
       "  5,\n",
       "  60,\n",
       "  501,\n",
       "  3,\n",
       "  853,\n",
       "  4,\n",
       "  854,\n",
       "  5],\n",
       " [1,\n",
       "  850,\n",
       "  24,\n",
       "  851,\n",
       "  852,\n",
       "  6,\n",
       "  58,\n",
       "  258,\n",
       "  132,\n",
       "  27,\n",
       "  12,\n",
       "  5,\n",
       "  60,\n",
       "  501,\n",
       "  3,\n",
       "  853,\n",
       "  4,\n",
       "  854,\n",
       "  5,\n",
       "  855],\n",
       " [1,\n",
       "  850,\n",
       "  24,\n",
       "  851,\n",
       "  852,\n",
       "  6,\n",
       "  58,\n",
       "  258,\n",
       "  132,\n",
       "  27,\n",
       "  12,\n",
       "  5,\n",
       "  60,\n",
       "  501,\n",
       "  3,\n",
       "  853,\n",
       "  4,\n",
       "  854,\n",
       "  5,\n",
       "  855,\n",
       "  8],\n",
       " [1,\n",
       "  850,\n",
       "  24,\n",
       "  851,\n",
       "  852,\n",
       "  6,\n",
       "  58,\n",
       "  258,\n",
       "  132,\n",
       "  27,\n",
       "  12,\n",
       "  5,\n",
       "  60,\n",
       "  501,\n",
       "  3,\n",
       "  853,\n",
       "  4,\n",
       "  854,\n",
       "  5,\n",
       "  855,\n",
       "  8,\n",
       "  502],\n",
       " [1,\n",
       "  850,\n",
       "  24,\n",
       "  851,\n",
       "  852,\n",
       "  6,\n",
       "  58,\n",
       "  258,\n",
       "  132,\n",
       "  27,\n",
       "  12,\n",
       "  5,\n",
       "  60,\n",
       "  501,\n",
       "  3,\n",
       "  853,\n",
       "  4,\n",
       "  854,\n",
       "  5,\n",
       "  855,\n",
       "  8,\n",
       "  502,\n",
       "  500],\n",
       " [1,\n",
       "  850,\n",
       "  24,\n",
       "  851,\n",
       "  852,\n",
       "  6,\n",
       "  58,\n",
       "  258,\n",
       "  132,\n",
       "  27,\n",
       "  12,\n",
       "  5,\n",
       "  60,\n",
       "  501,\n",
       "  3,\n",
       "  853,\n",
       "  4,\n",
       "  854,\n",
       "  5,\n",
       "  855,\n",
       "  8,\n",
       "  502,\n",
       "  500,\n",
       "  2],\n",
       " [1,\n",
       "  850,\n",
       "  24,\n",
       "  851,\n",
       "  852,\n",
       "  6,\n",
       "  58,\n",
       "  258,\n",
       "  132,\n",
       "  27,\n",
       "  12,\n",
       "  5,\n",
       "  60,\n",
       "  501,\n",
       "  3,\n",
       "  853,\n",
       "  4,\n",
       "  854,\n",
       "  5,\n",
       "  855,\n",
       "  8,\n",
       "  502,\n",
       "  500,\n",
       "  2,\n",
       "  250],\n",
       " [1,\n",
       "  850,\n",
       "  24,\n",
       "  851,\n",
       "  852,\n",
       "  6,\n",
       "  58,\n",
       "  258,\n",
       "  132,\n",
       "  27,\n",
       "  12,\n",
       "  5,\n",
       "  60,\n",
       "  501,\n",
       "  3,\n",
       "  853,\n",
       "  4,\n",
       "  854,\n",
       "  5,\n",
       "  855,\n",
       "  8,\n",
       "  502,\n",
       "  500,\n",
       "  2,\n",
       "  250,\n",
       "  347],\n",
       " [458, 37],\n",
       " [458, 37, 57],\n",
       " [458, 37, 57, 856],\n",
       " [458, 37, 57, 856, 96],\n",
       " [458, 37, 57, 856, 96, 857],\n",
       " [458, 37, 57, 856, 96, 857, 348],\n",
       " [458, 37, 57, 856, 96, 857, 348, 7],\n",
       " [458, 37, 57, 856, 96, 857, 348, 7, 858],\n",
       " [458, 37, 57, 856, 96, 857, 348, 7, 858, 5],\n",
       " [458, 37, 57, 856, 96, 857, 348, 7, 858, 5, 859],\n",
       " [458, 37, 57, 856, 96, 857, 348, 7, 858, 5, 859, 17],\n",
       " [458, 37, 57, 856, 96, 857, 348, 7, 858, 5, 859, 17, 860],\n",
       " [458, 37, 57, 856, 96, 857, 348, 7, 858, 5, 859, 17, 860, 43],\n",
       " [458, 37, 57, 856, 96, 857, 348, 7, 858, 5, 859, 17, 860, 43, 3],\n",
       " [458, 37, 57, 856, 96, 857, 348, 7, 858, 5, 859, 17, 860, 43, 3, 259],\n",
       " [6, 310],\n",
       " [6, 310, 167],\n",
       " [6, 310, 167, 861],\n",
       " [6, 310, 167, 861, 862],\n",
       " [6, 310, 167, 861, 862, 10],\n",
       " [6, 310, 167, 861, 862, 10, 186],\n",
       " [6, 310, 167, 861, 862, 10, 186, 5],\n",
       " [6, 310, 167, 861, 862, 10, 186, 5, 863],\n",
       " [6, 310, 167, 861, 862, 10, 186, 5, 863, 864],\n",
       " [6, 310, 167, 861, 862, 10, 186, 5, 863, 864, 18],\n",
       " [6, 310, 167, 861, 862, 10, 186, 5, 863, 864, 18, 46],\n",
       " [6, 310, 167, 861, 862, 10, 186, 5, 863, 864, 18, 46, 244],\n",
       " [865, 866],\n",
       " [865, 866, 6],\n",
       " [865, 866, 6, 205],\n",
       " [865, 866, 6, 205, 349],\n",
       " [865, 866, 6, 205, 349, 19],\n",
       " [865, 866, 6, 205, 349, 19, 102],\n",
       " [865, 866, 6, 205, 349, 19, 102, 4],\n",
       " [865, 866, 6, 205, 349, 19, 102, 4, 1],\n",
       " [865, 866, 6, 205, 349, 19, 102, 4, 1, 200],\n",
       " [865, 866, 6, 205, 349, 19, 102, 4, 1, 200, 33],\n",
       " [865, 866, 6, 205, 349, 19, 102, 4, 1, 200, 33, 146],\n",
       " [865, 866, 6, 205, 349, 19, 102, 4, 1, 200, 33, 146, 503],\n",
       " [865, 866, 6, 205, 349, 19, 102, 4, 1, 200, 33, 146, 503, 350],\n",
       " [865, 866, 6, 205, 349, 19, 102, 4, 1, 200, 33, 146, 503, 350, 2],\n",
       " [865, 866, 6, 205, 349, 19, 102, 4, 1, 200, 33, 146, 503, 350, 2, 444],\n",
       " [865, 866, 6, 205, 349, 19, 102, 4, 1, 200, 33, 146, 503, 350, 2, 444, 17],\n",
       " [145, 867],\n",
       " [145, 867, 2],\n",
       " [145, 867, 2, 1],\n",
       " [145, 867, 2, 1, 868],\n",
       " [145, 867, 2, 1, 868, 19],\n",
       " [145, 867, 2, 1, 868, 19, 44],\n",
       " [145, 867, 2, 1, 868, 19, 44, 1],\n",
       " [145, 867, 2, 1, 868, 19, 44, 1, 351],\n",
       " [145, 867, 2, 1, 868, 19, 44, 1, 351, 159],\n",
       " [145, 867, 2, 1, 868, 19, 44, 1, 351, 159, 239],\n",
       " [869, 21],\n",
       " [869, 21, 110],\n",
       " [869, 21, 110, 18],\n",
       " [869, 21, 110, 18, 15],\n",
       " [869, 21, 110, 18, 15, 870],\n",
       " [869, 21, 110, 18, 15, 870, 2],\n",
       " [869, 21, 110, 18, 15, 870, 2, 352],\n",
       " [869, 21, 110, 18, 15, 870, 2, 352, 75],\n",
       " [869, 21, 110, 18, 15, 870, 2, 352, 75, 2],\n",
       " [869, 21, 110, 18, 15, 870, 2, 352, 75, 2, 151],\n",
       " [869, 21, 110, 18, 15, 870, 2, 352, 75, 2, 151, 58],\n",
       " [869, 21, 110, 18, 15, 870, 2, 352, 75, 2, 151, 58, 5],\n",
       " [869, 21, 110, 18, 15, 870, 2, 352, 75, 2, 151, 58, 5, 871],\n",
       " [869, 21, 110, 18, 15, 870, 2, 352, 75, 2, 151, 58, 5, 871, 26],\n",
       " [869, 21, 110, 18, 15, 870, 2, 352, 75, 2, 151, 58, 5, 871, 26, 872],\n",
       " [869, 21, 110, 18, 15, 870, 2, 352, 75, 2, 151, 58, 5, 871, 26, 872, 12],\n",
       " [869, 21, 110, 18, 15, 870, 2, 352, 75, 2, 151, 58, 5, 871, 26, 872, 12, 59],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4,\n",
       "  1],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  12],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  12,\n",
       "  8],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  12,\n",
       "  8,\n",
       "  83],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  12,\n",
       "  8,\n",
       "  83,\n",
       "  260],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  12,\n",
       "  8,\n",
       "  83,\n",
       "  260,\n",
       "  261],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  12,\n",
       "  8,\n",
       "  83,\n",
       "  260,\n",
       "  261,\n",
       "  353],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  12,\n",
       "  8,\n",
       "  83,\n",
       "  260,\n",
       "  261,\n",
       "  353,\n",
       "  319],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  12,\n",
       "  8,\n",
       "  83,\n",
       "  260,\n",
       "  261,\n",
       "  353,\n",
       "  319,\n",
       "  320],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  12,\n",
       "  8,\n",
       "  83,\n",
       "  260,\n",
       "  261,\n",
       "  353,\n",
       "  319,\n",
       "  320,\n",
       "  116],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  12,\n",
       "  8,\n",
       "  83,\n",
       "  260,\n",
       "  261,\n",
       "  353,\n",
       "  319,\n",
       "  320,\n",
       "  116,\n",
       "  5],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  12,\n",
       "  8,\n",
       "  83,\n",
       "  260,\n",
       "  261,\n",
       "  353,\n",
       "  319,\n",
       "  320,\n",
       "  116,\n",
       "  5,\n",
       "  190],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  12,\n",
       "  8,\n",
       "  83,\n",
       "  260,\n",
       "  261,\n",
       "  353,\n",
       "  319,\n",
       "  320,\n",
       "  116,\n",
       "  5,\n",
       "  190,\n",
       "  37],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  12,\n",
       "  8,\n",
       "  83,\n",
       "  260,\n",
       "  261,\n",
       "  353,\n",
       "  319,\n",
       "  320,\n",
       "  116,\n",
       "  5,\n",
       "  190,\n",
       "  37,\n",
       "  10],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  12,\n",
       "  8,\n",
       "  83,\n",
       "  260,\n",
       "  261,\n",
       "  353,\n",
       "  319,\n",
       "  320,\n",
       "  116,\n",
       "  5,\n",
       "  190,\n",
       "  37,\n",
       "  10,\n",
       "  7],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  12,\n",
       "  8,\n",
       "  83,\n",
       "  260,\n",
       "  261,\n",
       "  353,\n",
       "  319,\n",
       "  320,\n",
       "  116,\n",
       "  5,\n",
       "  190,\n",
       "  37,\n",
       "  10,\n",
       "  7,\n",
       "  465],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  12,\n",
       "  8,\n",
       "  83,\n",
       "  260,\n",
       "  261,\n",
       "  353,\n",
       "  319,\n",
       "  320,\n",
       "  116,\n",
       "  5,\n",
       "  190,\n",
       "  37,\n",
       "  10,\n",
       "  7,\n",
       "  465,\n",
       "  130],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  12,\n",
       "  8,\n",
       "  83,\n",
       "  260,\n",
       "  261,\n",
       "  353,\n",
       "  319,\n",
       "  320,\n",
       "  116,\n",
       "  5,\n",
       "  190,\n",
       "  37,\n",
       "  10,\n",
       "  7,\n",
       "  465,\n",
       "  130,\n",
       "  1],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  12,\n",
       "  8,\n",
       "  83,\n",
       "  260,\n",
       "  261,\n",
       "  353,\n",
       "  319,\n",
       "  320,\n",
       "  116,\n",
       "  5,\n",
       "  190,\n",
       "  37,\n",
       "  10,\n",
       "  7,\n",
       "  465,\n",
       "  130,\n",
       "  1,\n",
       "  873],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  12,\n",
       "  8,\n",
       "  83,\n",
       "  260,\n",
       "  261,\n",
       "  353,\n",
       "  319,\n",
       "  320,\n",
       "  116,\n",
       "  5,\n",
       "  190,\n",
       "  37,\n",
       "  10,\n",
       "  7,\n",
       "  465,\n",
       "  130,\n",
       "  1,\n",
       "  873,\n",
       "  4],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  12,\n",
       "  8,\n",
       "  83,\n",
       "  260,\n",
       "  261,\n",
       "  353,\n",
       "  319,\n",
       "  320,\n",
       "  116,\n",
       "  5,\n",
       "  190,\n",
       "  37,\n",
       "  10,\n",
       "  7,\n",
       "  465,\n",
       "  130,\n",
       "  1,\n",
       "  873,\n",
       "  4,\n",
       "  1],\n",
       " [869,\n",
       "  21,\n",
       "  110,\n",
       "  18,\n",
       "  15,\n",
       "  870,\n",
       "  2,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  151,\n",
       "  58,\n",
       "  5,\n",
       "  871,\n",
       "  26,\n",
       "  872,\n",
       "  12,\n",
       "  59,\n",
       "  36,\n",
       "  255,\n",
       "  206,\n",
       "  504,\n",
       "  187,\n",
       "  6,\n",
       "  10,\n",
       "  41,\n",
       "  9,\n",
       "  19,\n",
       "  130,\n",
       "  3,\n",
       "  160,\n",
       "  139,\n",
       "  4,\n",
       "  204,\n",
       "  18,\n",
       "  20,\n",
       "  65,\n",
       "  102,\n",
       "  4,\n",
       "  1,\n",
       "  200,\n",
       "  12,\n",
       "  8,\n",
       "  83,\n",
       "  260,\n",
       "  261,\n",
       "  353,\n",
       "  319,\n",
       "  320,\n",
       "  116,\n",
       "  5,\n",
       "  190,\n",
       "  37,\n",
       "  10,\n",
       "  7,\n",
       "  465,\n",
       "  130,\n",
       "  1,\n",
       "  873,\n",
       "  4,\n",
       "  1,\n",
       "  874],\n",
       " [334, 6],\n",
       " [334, 6, 1],\n",
       " [334, 6, 1, 207],\n",
       " [334, 6, 1, 207, 875],\n",
       " [334, 6, 1, 207, 875, 18],\n",
       " [334, 6, 1, 207, 875, 18, 46],\n",
       " [334, 6, 1, 207, 875, 18, 46, 352],\n",
       " [334, 6, 1, 207, 875, 18, 46, 352, 75],\n",
       " [334, 6, 1, 207, 875, 18, 46, 352, 75, 2],\n",
       " [334, 6, 1, 207, 875, 18, 46, 352, 75, 2, 354],\n",
       " [334, 6, 1, 207, 875, 18, 46, 352, 75, 2, 354, 49],\n",
       " [334, 6, 1, 207, 875, 18, 46, 352, 75, 2, 354, 49, 2],\n",
       " [334, 6, 1, 207, 875, 18, 46, 352, 75, 2, 354, 49, 2, 3],\n",
       " [334, 6, 1, 207, 875, 18, 46, 352, 75, 2, 354, 49, 2, 3, 876],\n",
       " [334, 6, 1, 207, 875, 18, 46, 352, 75, 2, 354, 49, 2, 3, 876, 877],\n",
       " [334, 6, 1, 207, 875, 18, 46, 352, 75, 2, 354, 49, 2, 3, 876, 877, 4],\n",
       " [334, 6, 1, 207, 875, 18, 46, 352, 75, 2, 354, 49, 2, 3, 876, 877, 4, 65],\n",
       " [334,\n",
       "  6,\n",
       "  1,\n",
       "  207,\n",
       "  875,\n",
       "  18,\n",
       "  46,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  354,\n",
       "  49,\n",
       "  2,\n",
       "  3,\n",
       "  876,\n",
       "  877,\n",
       "  4,\n",
       "  65,\n",
       "  355],\n",
       " [334,\n",
       "  6,\n",
       "  1,\n",
       "  207,\n",
       "  875,\n",
       "  18,\n",
       "  46,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  354,\n",
       "  49,\n",
       "  2,\n",
       "  3,\n",
       "  876,\n",
       "  877,\n",
       "  4,\n",
       "  65,\n",
       "  355,\n",
       "  99],\n",
       " [334,\n",
       "  6,\n",
       "  1,\n",
       "  207,\n",
       "  875,\n",
       "  18,\n",
       "  46,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  354,\n",
       "  49,\n",
       "  2,\n",
       "  3,\n",
       "  876,\n",
       "  877,\n",
       "  4,\n",
       "  65,\n",
       "  355,\n",
       "  99,\n",
       "  11],\n",
       " [334,\n",
       "  6,\n",
       "  1,\n",
       "  207,\n",
       "  875,\n",
       "  18,\n",
       "  46,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  354,\n",
       "  49,\n",
       "  2,\n",
       "  3,\n",
       "  876,\n",
       "  877,\n",
       "  4,\n",
       "  65,\n",
       "  355,\n",
       "  99,\n",
       "  11,\n",
       "  205],\n",
       " [334,\n",
       "  6,\n",
       "  1,\n",
       "  207,\n",
       "  875,\n",
       "  18,\n",
       "  46,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  354,\n",
       "  49,\n",
       "  2,\n",
       "  3,\n",
       "  876,\n",
       "  877,\n",
       "  4,\n",
       "  65,\n",
       "  355,\n",
       "  99,\n",
       "  11,\n",
       "  205,\n",
       "  3],\n",
       " [334,\n",
       "  6,\n",
       "  1,\n",
       "  207,\n",
       "  875,\n",
       "  18,\n",
       "  46,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  354,\n",
       "  49,\n",
       "  2,\n",
       "  3,\n",
       "  876,\n",
       "  877,\n",
       "  4,\n",
       "  65,\n",
       "  355,\n",
       "  99,\n",
       "  11,\n",
       "  205,\n",
       "  3,\n",
       "  29],\n",
       " [334,\n",
       "  6,\n",
       "  1,\n",
       "  207,\n",
       "  875,\n",
       "  18,\n",
       "  46,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  354,\n",
       "  49,\n",
       "  2,\n",
       "  3,\n",
       "  876,\n",
       "  877,\n",
       "  4,\n",
       "  65,\n",
       "  355,\n",
       "  99,\n",
       "  11,\n",
       "  205,\n",
       "  3,\n",
       "  29,\n",
       "  356],\n",
       " [334,\n",
       "  6,\n",
       "  1,\n",
       "  207,\n",
       "  875,\n",
       "  18,\n",
       "  46,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  354,\n",
       "  49,\n",
       "  2,\n",
       "  3,\n",
       "  876,\n",
       "  877,\n",
       "  4,\n",
       "  65,\n",
       "  355,\n",
       "  99,\n",
       "  11,\n",
       "  205,\n",
       "  3,\n",
       "  29,\n",
       "  356,\n",
       "  34],\n",
       " [334,\n",
       "  6,\n",
       "  1,\n",
       "  207,\n",
       "  875,\n",
       "  18,\n",
       "  46,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  354,\n",
       "  49,\n",
       "  2,\n",
       "  3,\n",
       "  876,\n",
       "  877,\n",
       "  4,\n",
       "  65,\n",
       "  355,\n",
       "  99,\n",
       "  11,\n",
       "  205,\n",
       "  3,\n",
       "  29,\n",
       "  356,\n",
       "  34,\n",
       "  3],\n",
       " [334,\n",
       "  6,\n",
       "  1,\n",
       "  207,\n",
       "  875,\n",
       "  18,\n",
       "  46,\n",
       "  352,\n",
       "  75,\n",
       "  2,\n",
       "  354,\n",
       "  49,\n",
       "  2,\n",
       "  3,\n",
       "  876,\n",
       "  877,\n",
       "  4,\n",
       "  65,\n",
       "  355,\n",
       "  99,\n",
       "  11,\n",
       "  205,\n",
       "  3,\n",
       "  29,\n",
       "  356,\n",
       "  34,\n",
       "  3,\n",
       "  878],\n",
       " ...]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = max([len(x) for x in input_sequences])\n",
    "max_len \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_input_sequences = pad_sequences(input_sequences , maxlen  = max_len , padding=  'pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0,   3, 443],\n",
       "       [  0,   0,   0, ...,   3, 443,   7],\n",
       "       [  0,   0,   0, ..., 443,   7, 304],\n",
       "       ...,\n",
       "       [  0,   0,   0, ..., 347, 679, 168],\n",
       "       [  0,   0,   0, ..., 679, 168, 142],\n",
       "       [  0,   0,   0, ..., 168, 142,  33]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= padded_input_sequences[: , :-1]\n",
    "\n",
    "y=padded_input_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y = to_categorical(y,num_classes=1491)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6011, 1491)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 61, 100)           149100    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 61, 150)           150600    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 61, 150)           180600    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 150)               180600    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1491)              225141    \n",
      "=================================================================\n",
      "Total params: 886,041\n",
      "Trainable params: 886,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(1491, 100, input_length=61))\n",
    "model.add(LSTM(150, return_sequences=True))\n",
    "model.add(LSTM(150,return_sequences=True))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(1491, activation='softmax'))\n",
    "model.build(input_shape=(None, 62))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "151/151 [==============================] - 7s 44ms/step - loss: 0.4475 - accuracy: 0.9272 - val_loss: 14.0883 - val_accuracy: 0.0333\n",
      "Epoch 2/50\n",
      "151/151 [==============================] - 5s 36ms/step - loss: 0.4290 - accuracy: 0.9303 - val_loss: 14.1367 - val_accuracy: 0.0324\n",
      "Epoch 3/50\n",
      "151/151 [==============================] - 14s 94ms/step - loss: 0.4132 - accuracy: 0.9343 - val_loss: 14.2413 - val_accuracy: 0.0324\n",
      "Epoch 4/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.3949 - accuracy: 0.9378 - val_loss: 14.2693 - val_accuracy: 0.0341\n",
      "Epoch 5/50\n",
      "151/151 [==============================] - 7s 48ms/step - loss: 0.3894 - accuracy: 0.9380 - val_loss: 14.3311 - val_accuracy: 0.0316\n",
      "Epoch 6/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.3767 - accuracy: 0.9403 - val_loss: 14.4174 - val_accuracy: 0.0316\n",
      "Epoch 7/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.3622 - accuracy: 0.9409 - val_loss: 14.4912 - val_accuracy: 0.0341\n",
      "Epoch 8/50\n",
      "151/151 [==============================] - 7s 48ms/step - loss: 0.3476 - accuracy: 0.9468 - val_loss: 14.5171 - val_accuracy: 0.0324\n",
      "Epoch 9/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.3392 - accuracy: 0.9459 - val_loss: 14.5807 - val_accuracy: 0.0349\n",
      "Epoch 10/50\n",
      "151/151 [==============================] - 7s 48ms/step - loss: 0.3290 - accuracy: 0.9451 - val_loss: 14.6043 - val_accuracy: 0.0316\n",
      "Epoch 11/50\n",
      "151/151 [==============================] - 7s 48ms/step - loss: 0.3173 - accuracy: 0.9528 - val_loss: 14.7253 - val_accuracy: 0.0316\n",
      "Epoch 12/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.3059 - accuracy: 0.9507 - val_loss: 14.7787 - val_accuracy: 0.0357\n",
      "Epoch 13/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.3037 - accuracy: 0.9503 - val_loss: 14.8115 - val_accuracy: 0.0333\n",
      "Epoch 14/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.2928 - accuracy: 0.9511 - val_loss: 14.8693 - val_accuracy: 0.0357\n",
      "Epoch 15/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.2761 - accuracy: 0.9536 - val_loss: 14.9248 - val_accuracy: 0.0341\n",
      "Epoch 16/50\n",
      "151/151 [==============================] - 7s 48ms/step - loss: 0.2687 - accuracy: 0.9559 - val_loss: 15.0060 - val_accuracy: 0.0374\n",
      "Epoch 17/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.2646 - accuracy: 0.9563 - val_loss: 15.0897 - val_accuracy: 0.0349\n",
      "Epoch 18/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.2651 - accuracy: 0.9561 - val_loss: 15.1440 - val_accuracy: 0.0357\n",
      "Epoch 19/50\n",
      "151/151 [==============================] - 7s 48ms/step - loss: 0.2677 - accuracy: 0.9532 - val_loss: 15.0282 - val_accuracy: 0.0324\n",
      "Epoch 20/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.2794 - accuracy: 0.9534 - val_loss: 15.1264 - val_accuracy: 0.0316\n",
      "Epoch 21/50\n",
      "151/151 [==============================] - 7s 48ms/step - loss: 0.2705 - accuracy: 0.9549 - val_loss: 15.2206 - val_accuracy: 0.0291\n",
      "Epoch 22/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.2391 - accuracy: 0.9586 - val_loss: 15.2389 - val_accuracy: 0.0333\n",
      "Epoch 23/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.2186 - accuracy: 0.9599 - val_loss: 15.3151 - val_accuracy: 0.0349\n",
      "Epoch 24/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.2065 - accuracy: 0.9615 - val_loss: 15.4062 - val_accuracy: 0.0333\n",
      "Epoch 25/50\n",
      "151/151 [==============================] - 7s 48ms/step - loss: 0.1993 - accuracy: 0.9615 - val_loss: 15.4265 - val_accuracy: 0.0341\n",
      "Epoch 26/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.1971 - accuracy: 0.9613 - val_loss: 15.5370 - val_accuracy: 0.0349\n",
      "Epoch 27/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.1911 - accuracy: 0.9630 - val_loss: 15.5349 - val_accuracy: 0.0324\n",
      "Epoch 28/50\n",
      "151/151 [==============================] - 7s 48ms/step - loss: 0.1888 - accuracy: 0.9624 - val_loss: 15.6232 - val_accuracy: 0.0316\n",
      "Epoch 29/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.1816 - accuracy: 0.9621 - val_loss: 15.6207 - val_accuracy: 0.0299\n",
      "Epoch 30/50\n",
      "151/151 [==============================] - 7s 48ms/step - loss: 0.1848 - accuracy: 0.9632 - val_loss: 15.6743 - val_accuracy: 0.0349\n",
      "Epoch 31/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.2047 - accuracy: 0.9615 - val_loss: 15.7221 - val_accuracy: 0.0308\n",
      "Epoch 32/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.2725 - accuracy: 0.9453 - val_loss: 15.7684 - val_accuracy: 0.0391\n",
      "Epoch 33/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.2888 - accuracy: 0.9420 - val_loss: 15.8005 - val_accuracy: 0.0349\n",
      "Epoch 34/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.2097 - accuracy: 0.9574 - val_loss: 15.8249 - val_accuracy: 0.0308\n",
      "Epoch 35/50\n",
      "151/151 [==============================] - 7s 48ms/step - loss: 0.1642 - accuracy: 0.9646 - val_loss: 15.8913 - val_accuracy: 0.0366\n",
      "Epoch 36/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.1543 - accuracy: 0.9642 - val_loss: 15.9418 - val_accuracy: 0.0333\n",
      "Epoch 37/50\n",
      "151/151 [==============================] - 7s 49ms/step - loss: 0.1490 - accuracy: 0.9653 - val_loss: 15.9835 - val_accuracy: 0.0357\n",
      "Epoch 38/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.1448 - accuracy: 0.9655 - val_loss: 16.0172 - val_accuracy: 0.0366\n",
      "Epoch 39/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.1423 - accuracy: 0.9657 - val_loss: 16.0837 - val_accuracy: 0.0391\n",
      "Epoch 40/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.1403 - accuracy: 0.9642 - val_loss: 16.0784 - val_accuracy: 0.0366\n",
      "Epoch 41/50\n",
      "151/151 [==============================] - 7s 48ms/step - loss: 0.1375 - accuracy: 0.9659 - val_loss: 16.1208 - val_accuracy: 0.0341\n",
      "Epoch 42/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.1355 - accuracy: 0.9667 - val_loss: 16.1905 - val_accuracy: 0.0391\n",
      "Epoch 43/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.1344 - accuracy: 0.9661 - val_loss: 16.2278 - val_accuracy: 0.0341\n",
      "Epoch 44/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.1309 - accuracy: 0.9663 - val_loss: 16.3005 - val_accuracy: 0.0399\n",
      "Epoch 45/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.1294 - accuracy: 0.9653 - val_loss: 16.3261 - val_accuracy: 0.0357\n",
      "Epoch 46/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.1296 - accuracy: 0.9653 - val_loss: 16.3848 - val_accuracy: 0.0374\n",
      "Epoch 47/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.1290 - accuracy: 0.9659 - val_loss: 16.4021 - val_accuracy: 0.0366\n",
      "Epoch 48/50\n",
      "151/151 [==============================] - 7s 48ms/step - loss: 0.1308 - accuracy: 0.9653 - val_loss: 16.4308 - val_accuracy: 0.0374\n",
      "Epoch 49/50\n",
      "151/151 [==============================] - 7s 47ms/step - loss: 0.1349 - accuracy: 0.9642 - val_loss: 16.5064 - val_accuracy: 0.0291\n",
      "Epoch 50/50\n",
      "151/151 [==============================] - 7s 48ms/step - loss: 0.2637 - accuracy: 0.9332 - val_loss: 16.4406 - val_accuracy: 0.0333\n"
     ]
    }
   ],
   "source": [
    "history  = model.fit(x,y , epochs =50 , validation_split=0.2 , verbose =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 61) for input KerasTensor(type_spec=TensorSpec(shape=(None, 61), dtype=tf.float32, name='embedding_1_input'), name='embedding_1_input', description=\"created by layer 'embedding_1_input'\"), but it was called on an input with incompatible shape (None, 62).\n",
      "A miracle is taking place\n",
      "A miracle is taking place as\n",
      "A miracle is taking place as you\n",
      "A miracle is taking place as you read\n",
      "A miracle is taking place as you read these\n",
      "A miracle is taking place as you read these thoughts\n",
      "A miracle is taking place as you read these thoughts with\n",
      "A miracle is taking place as you read these thoughts with others\n",
      "A miracle is taking place as you read these thoughts with others what\n",
      "A miracle is taking place as you read these thoughts with others what you\n"
     ]
    }
   ],
   "source": [
    "text = \"A miracle is taking\"\n",
    "\n",
    "for i in range(10):\n",
    "  # tokenize\n",
    "  token_text = tokenizer.texts_to_sequences([text])[0]\n",
    "  # padding\n",
    "  padded_token_text = pad_sequences([token_text], maxlen=62, padding='pre')\n",
    "  # predict\n",
    "  pos = np.argmax(model.predict(padded_token_text))\n",
    "\n",
    "  for word,index in tokenizer.word_index.items():\n",
    "    if index == pos:\n",
    "      text = text + \" \" + word\n",
    "      print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "# Initialize the Tokenizer\n",
    "tokenizer = Tokenizer()  # You can adjust the num_words based on your need\n",
    "# Save the tokenizer to a file\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"Tokenizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn while saving (showing 5 of 15). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: numpy\n",
      "Version: 1.20.3\n",
      "Summary: NumPy is the fundamental package for array computing with Python.\n",
      "Home-page: https://www.numpy.org\n",
      "Author: Travis E. Oliphant et al.\n",
      "Author-email: \n",
      "License: BSD\n",
      "Location: c:\\users\\gupta\\anaconda3\\envs\\env3\\lib\\site-packages\n",
      "Requires: \n",
      "Required-by: h5py, Keras-Preprocessing, mkl-fft, mkl-random, opt-einsum, scipy, tensorboard, tensorflow\n",
      "---\n",
      "Name: tensorflow\n",
      "Version: 2.6.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: c:\\users\\gupta\\anaconda3\\envs\\env3\\lib\\site-packages\n",
      "Requires: absl-py, astunparse, clang, flatbuffers, gast, google-pasta, grpcio, h5py, keras, keras-preprocessing, numpy, opt-einsum, protobuf, six, tensorboard, tensorflow-estimator, termcolor, typing-extensions, wheel, wrapt\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show numpy tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
